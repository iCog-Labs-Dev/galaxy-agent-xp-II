[
    {
        "name": "OmicsDI",
        "description": "Sync dataset's files from OmicsDI",
        "category": "Get Data",
        "version": "1.0.0",
        "help": null
    },
    {
        "name": "SRA",
        "description": "server",
        "category": "Get Data",
        "version": "1.0.1",
        "help": null
    },
    {
        "name": "Download and Extract Reads in BAM",
        "description": "format from NCBI SRA",
        "category": "Get Data",
        "version": "3.0.8+galaxy0",
        "help": "**What it does?**\n\nThis tool extracts data (in BAM_ format) from the Short Read Archive (SRA) at the National Center for Biotechnology Information (NCBI). It is based on the sam-dump_ utility of the SRA Toolkit and returns a collection of NGS data containing one file for each accession number provided.\n\n\n\n    **How to use it?**\n\nThere are three ways in which you can download data:\n\n 1. Plain text input of accession number(s)\n 2. Providing a list of accessions from file\n 3. Extracting data from an already uploaded SRA dataset\n\nBelow we discuss each in detail.\n\n------\n\n**Plain text input of accession number(s)**\n\nWhen you type an accession number (e.g., `SRR1582967`) into **Accession** box and click **Execute** the tool will fetch the data for you. You can also provide a list of multiple accession numbers (e.g. `SRR3141592, SRR271828, SRR112358`).\n\n-----\n\n**Providing a list of accessions from file**\n\nA more realistic scenario is when you want to upload a number of datasets at once. To do this you need a list of accession, where there is only one accession per line (see below for information on how to generate such a file). Once you have this file:\n\n 1. Upload it into your history using Galaxy's upload tool\n 2. Once the list of accessions is uploaded choose *List of SRA accessions, one per line* from **select input type** dropdown\n 3. Choose uploaded file within the **sra accession list** field\n 4. Click **Execute**\n\n-----\n\n**Extract data from an already uploaded SRA dataset**\n\nIf an SRA dataset is already present in the history, the sequencing data can be extracted in a human-readable data format (fastq, sam, bam) by setting **select input type** drop-down to *SRA archive in current history*.\n    \n\n\n-----\n\n**How to generate accession lists**\n\n 1. Go to **SRA Run Selector** by clicking this link_\n 2. Find the study you are interested in by typing a search term within the **Search** box. This can be a word (e.g., *mitochondria*) or an accession you have gotten from a paper (e.g., *SRR1582967*).\n 3. Once you click on the study of interest you will see the number of datasets in this study within the **Related SRA data** box\n 4. Click on the Runs number\n 5. On the page that would open you will see **Accession List** button\n 6. Clicking of this button will produce a file that you will need to upload into Galaxy and use as the input to this tool.\n    \n\n-----\n\n.. _sam-dump: https://github.com/ncbi/sra-tools\n.. _BAM: https://samtools.github.io/hts-specs/SAMv1.pdf\n.. _collection: https://galaxyproject.org/tutorials/collections/\n.. _link: https://trace.ncbi.nlm.nih.gov/Traces/index.html?view=run_browser&display=reads\n\n\nFor credits, information, support and bug reports, please refer ato https://github.com/galaxyproject/tools-iuc."
    },
    {
        "name": "Copernicus Marine Data Store",
        "description": "retrieve marine data",
        "category": "Get Data",
        "version": "1.3.3+galaxy0",
        "help": "============================\nCopernicus Marine Data Store\n============================\n\n** Context **\nThis tool is a wrapper to retrieve data from the Copernicus Marine Environment Monitoring Service (CMEMS).\n\n- It allows to retrieve data from the Copernicus Marine Service.\n- Any user willing to use this tool needs to `create a new account <https://data.marine.copernicus.eu/login>`_.\n- Set your Copernicus CMEMS API Key via: User > Preferences > Manage Information\n- Enter your username and password for Copernicus CMEMS\n- Compose your request directly on Copernicus Marine Data Store \n    - Choose there which data interest you click on the download button\n    - Then on the top right click again on the big download butto\n    - Log in\n    - Click on \"Automate\"\n    - You should have a pop-up window called \"Automate download\"\n    - Copy the \">_Command-Line Interface\" proposed there\n- Back on Galaxy paste it in the input field \"Paste API Request\".\n\nFor more information on the Command-Line Interface (CLI) go on `Copernicus Marine Toolbox CLI - Subset <https://help.marine.copernicus.eu/en/articles/7972861-copernicus-marine-toolbox-cli-subset>`\n\n** Input **\nCommand line from the Copernicus marine services copy paste as a text.\n\n** Output **\nA netcdf file containing the the data chose by the user from the Copernicus Marine Data Store."
    },
    {
        "name": "Protein Database Downloader",
        "description": "",
        "category": "Get Data",
        "version": "0.3.2",
        "help": "**Output**\n\nCreates a FASTA file of specified protein sequences for comparison with experimental MS/MS data in search algorithm. \n\n**External Links**\n\n  - Galaxy-P_101_ shows usage Protein Database Downloader tool in the creation of a workflow\n  - UniProtKB_ provides additional information about the UniProt Knowledgebase\n\n\n.. _Galaxy-P_101: http://msi-galaxy-p.readthedocs.org/en/latest/sections/galaxyp_101.html\n.. _UniProtKB: http://www.uniprot.org/help/uniprotkb\n\n\n**Additional Protein Fasta URLs**\n\n  *HUMAN GUT METAPROTEOME:*\n\n    *  61MB gzip http://www.bork.embl.de/~arumugam/Qin_et_al_2010/frequent_microbe_proteins.fasta.gz\n\n\n  *MOUSE GUT MICROBIOTA:*\n\n    * See: http://gigadb.org/dataset/view/id/100114/token/mZlMYJIF04LshpgP"
    },
    {
        "name": "NCBI EGQuery",
        "description": "Provides the number of records retrieved in all Entrez databases by a single text query.",
        "category": "Get Data",
        "version": "1.1",
        "help": "NCBI Entrez EGQuery\n===================\n\nProvides the number of records retrieved in all Entrez databases by a single\ntext query.\n\nExample Queries\n---------------\n\n+----------------------+-------------+\n| Parameter            | Value       |\n+======================+=============+\n| Term                 | Cancer      |\n+----------------------+-------------+\n\n\n  \n\n\nUsage Guidelines and Requirements\n=================================\n\nFrequency, Timing, and Registration of E-utility URL Requests\n-------------------------------------------------------------\n\nIn order not to overload the E-utility servers, NCBI recommends that users\nlimit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time\nduring weekdays. Failure to comply with this policy may result in an IP address\nbeing blocked from accessing NCBI.\n\nMinimizing the Number of Requests\n---------------------------------\n\nIf a task requires searching for and/or downloading a large number of\nrecords, it is much more efficient to use the Entrez History to upload\nand/or retrieve these records in batches rather than using separate\nrequests for each record. Please refer to Application 3 in Chapter 3\nfor an example. Many thousands of IDs can be uploaded using a single\nEPost request, and several hundred records can be downloaded using one\nEFetch request.\n\n\nDisclaimer and Copyright Issues\n-------------------------------\n\nIn accordance with requirements of NCBI's E-Utilities, we must provide\nthe following disclaimer:\n\nPlease note that abstracts in PubMed may incorporate material that may\nbe protected by U.S. and foreign copyright laws. All persons\nreproducing, redistributing, or making commercial use of this\ninformation are expected to adhere to the terms and conditions asserted\nby the copyright holder. Transmission or reproduction of protected\nitems beyond that allowed by fair use (PDF) as defined in the copyright\nlaws requires the written permission of the copyright owners. NLM\nprovides no legal advice concerning distribution of copyrighted\nmaterials. Please consult your legal counsel. If you wish to do a large\ndata mining project on PubMed data, you can enter into a licensing\nagreement and lease the data for free from NLM. For more information on\nthis please see `http://www.nlm.nih.gov/databases/leased.html <http://www.nlm.nih.gov/databases/leased.html>`__\n\nThe `full disclaimer <http://www.ncbi.nlm.nih.gov/About/disclaimer.html>`__ is available on\ntheir website\n\nLiability\n~~~~~~~~~\n\nFor documents and software available from this server, the\nU.S. Government does not warrant or assume any legal liability or\nresponsibility for the accuracy, completeness, or usefulness of any\ninformation, apparatus, product, or process disclosed.\n\nEndorsement\n~~~~~~~~~~~\n\nNCBI does not endorse or recommend any commercial\nproducts, processes, or services. The views and opinions of authors\nexpressed on NCBI's Web sites do not necessarily state or reflect those\nof the U.S. Government, and they may not be used for advertising or\nproduct endorsement purposes.\n\nExternal Links\n~~~~~~~~~~~~~~\n\nSome NCBI Web pages may provide links to other Internet\nsites for the convenience of users. NCBI is not responsible for the\navailability or content of these external sites, nor does NCBI endorse,\nwarrant, or guarantee the products, services, or information described\nor offered at these other Internet sites. Users cannot assume that the\nexternal sites will abide by the same Privacy Policy to which NCBI\nadheres. It is the responsibility of the user to examine the copyright\nand licensing restrictions of linked pages and to secure all necessary\npermissions."
    },
    {
        "name": "BioMart",
        "description": "Ensembl server",
        "category": "Get Data",
        "version": "1.0.1",
        "help": null
    },
    {
        "name": "Download and Extract Reads in FASTQ",
        "description": "format from NCBI SRA",
        "category": "Get Data",
        "version": "3.0.8+galaxy0",
        "help": "**What it does?**\n\nThis tool extracts data (in fastq_ format) from the Short Read Archive (SRA) at the National Center for Biotechnology Information (NCBI). It is based on the fasterq-dump_ utility of the SRA Toolkit.  The following applies:\n\n - if data is paired-ended (or mate-pair) the tool will generate a collection of file pairs, in which each element will be a pair of fastq_ files containing forward and reverse mates.\n - if data is single ended, each element of the collection will be a single fastq_ dataset.\n\n\n\n    **How to use it?**\n\nThere are three ways in which you can download data:\n\n 1. Plain text input of accession number(s)\n 2. Providing a list of accessions from file\n 3. Extracting data from an already uploaded SRA dataset\n\nBelow we discuss each in detail.\n\n------\n\n**Plain text input of accession number(s)**\n\nWhen you type an accession number (e.g., `SRR1582967`) into **Accession** box and click **Execute** the tool will fetch the data for you. You can also provide a list of multiple accession numbers (e.g. `SRR3141592, SRR271828, SRR112358`).\n\n-----\n\n**Providing a list of accessions from file**\n\nA more realistic scenario is when you want to upload a number of datasets at once. To do this you need a list of accession, where there is only one accession per line (see below for information on how to generate such a file). Once you have this file:\n\n 1. Upload it into your history using Galaxy's upload tool\n 2. Once the list of accessions is uploaded choose *List of SRA accessions, one per line* from **select input type** dropdown\n 3. Choose uploaded file within the **sra accession list** field\n 4. Click **Execute**\n\n-----\n\n**Extract data from an already uploaded SRA dataset**\n\nIf an SRA dataset is already present in the history, the sequencing data can be extracted in a human-readable data format (fastq, sam, bam) by setting **select input type** drop-down to *SRA archive in current history*.\n    \n\n-----\n\n**Output**\n\nIn every case, fastq datasets produced will be saved in Galaxy's history as a collection_ - a single history element containing multiple datasets. In fact, regardless of the experimental design, three collections will be produced: one containing paired-end data, another containing single-end data, and a third one which contains reads which could not be classified.\nSome collections may be empty if the accessions provided in the list do not contain one of the type of data.\n\n.. class:: warningmark\n\nWhen you decide to dump technical reads (in Advanced Options Dump only biological reads is set to No), you will probably find your PAIRED data in the other data collection as it is impossible to determine if it was 2 biological reads or one biological and one technical.\n\n.. class:: warningmark\n\nBy default, only biological reads are dumped and in case of PAIRED dataset only the spots which have both reads will be in the paired-end collection. The remaining single reads will be in the other colletion.\nTo keep all reads, and potentially not have the same number of reads in forward and reverse use the --split-files option in Advanced Options, Select how to split the spots.\n\n\n-----\n\n**How to generate accession lists**\n\n 1. Go to **SRA Run Selector** by clicking this link_\n 2. Find the study you are interested in by typing a search term within the **Search** box. This can be a word (e.g., *mitochondria*) or an accession you have gotten from a paper (e.g., *SRR1582967*).\n 3. Once you click on the study of interest you will see the number of datasets in this study within the **Related SRA data** box\n 4. Click on the Runs number\n 5. On the page that would open you will see **Accession List** button\n 6. Clicking of this button will produce a file that you will need to upload into Galaxy and use as the input to this tool.\n    \n\n-----\n\n\n.. _fastq: https://en.wikipedia.org/wiki/FASTQ_format\n.. _fasterq-dump: https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump\n.. _collection: https://galaxyproject.org/tutorials/collections/\n.. _link: https://trace.ncbi.nlm.nih.gov/Traces/index.html?view=run_browser&display=reads\n\n\nFor credits, information, support and bug reports, please refer ato https://github.com/galaxyproject/tools-iuc."
    },
    {
        "name": "AquaINFRA Importer",
        "description": "downloads content via the AquaINFRA interaction platform",
        "category": "Get Data",
        "version": "1.0",
        "help": null
    },
    {
        "name": "UniProtXML Download",
        "description": "proteome",
        "category": "Get Data",
        "version": "1.0.1",
        "help": "**UniProtXML Downloader**\n\nDownloads a UniProtXML file from UniProtKB\n\nThe Morpheus proteomics search algorithm can use this format as a search database.\n\nAvailable proteomes: http://www.uniprot.org/proteomes/\n\nUniProtKB help: http://www.uniprot.org/help/uniprotkb"
    },
    {
        "name": "Faster Download and Extract Reads in FASTQ",
        "description": "format from NCBI SRA",
        "category": "Get Data",
        "version": "2.10.4+galaxy2",
        "help": "**What it does?**\n\nThis tool extracts data (in fastq_ format) from the Short Read Archive (SRA) at the National Center for Biotechnology Information (NCBI). It is based on the fasterq-dump_ utility of the SRA Toolkit.\n\n**How to use it?**\n\nThere are three ways in which you can download data:\n\n 1. Data for single accession\n 2. Multiple datasets using a list of accessions\n 3. Extract data from already uploaded SRA dataset\n\nBelow we discuss each in detail.\n\n------\n\n**Uploading data for a single accession**\n\nWhen you type a single accession number (e.g., `SRR1582967`) into **Accession** box and click **Execute** the tool will fetch data for you.\n\n-----\n\n**Uploading multiple datasets using a list of accessions**\n\nA more realistic scenario is when you want to upload a number of datasets at once. To do this you need a list of accession, where there is only one accession per line (see below for information on how to generate such a file). Once you have this file:\n\n 1. Upload it into your history using Galaxy's upload tool\n 2. Once the list of accessions is uploaded choose *List of SRA accessions, one per line* from **select input type** dropdown\n 3. Choose uploaded file within the **sra accession list** field\n 4. Click **Execute**\n\n-----\n\n**Extract data from already uploaded SRA dataset**\n\nIf a SRA dataset is present in the history, it can be converted into fastq dataset by setting **select input type** drop-down to *SRA archive in current history*. Just like in the case of extracting data for single accession number the following applies:\n\n - if data is paired-ended (or mate-pair) the tool will generate a single *interleaved* dataset, in which forward and reverse mates are alternating (see example below).\n - if data is single ended, a standard fastq dataset will be produced\n\n-----\n\n**Output**\n\nIn every case, fastq datasets produced will be saved in Galaxy's history as a collection_ - a single history element containing multiple datasets.\nIn fact, three collections will be produced: one containing paired-end data, another containing single-end data, and a third one which contains reads which could not be classified.\nSome collections may be empty if the accessions provided in the list does not contain one of the type of data.\n\n.. class:: warningmark\n\nWhen you decide to dump technical reads (in Advanced Options Dump only biological reads is set to No), you will probably find your PAIRED data in the other data collection as it is impossible to determine if it was 2 biological reads or one biological and one technical.\n\n.. class:: warningmark\n\nBy default, only biological reads are dumped and in case of PAIRED dataset only the spots which have both reads will be in the paired-end collection. The remaining single reads will be in the other colletion.\nTo keep all reads, and maybe do not have the same number of reads in forward and reverse use the --split-files option in Advanced Options, Select how to split the spots.\n\n\n-----\n\n**How to generate accession lists**\n\n 1. Go to **SRA Run Selector** by clicking this link_\n 2. Find the study you are interested in by typing a search term within the **Search** box. This can be a word (e.g., *mitochondria*) or an accession you have gotten from a paper (e.g., *SRR1582967*).\n 3. Once you click on the study of interest you will see the number of datasets in this study within the **Related SRA data** box\n 4. Click on the Runs number\n 5. On the page that would open you will see **Accession List** button\n 6. Clicking of this button will produce a file that you will need to upload into Galaxy and use as the input to this tool.\n    \n\n-----\n\n\n.. _fastq: https://en.wikipedia.org/wiki/FASTQ_format\n.. _fastq-dump: https://ncbi.github.io/sra-tools/fastq-dump.html\n.. _fasterq-dump: https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump\n.. _collection: https://galaxyproject.org/tutorials/collections/\n.. _link: http://www.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=studies\n\n\nGalaxy tool wrapper originally written by Matt Shirley (mdshw5 at gmail.com).\nWrapper modified by Philip Mabon ( philip.mabon at phac-aspc.gc.ca ).\nTool dependencies, clean-up and bug-fixes by Marius van den Beek (m.vandenbeek at gmail.com).\nFor support and bug reports contact Matt Shirley or Marius van den Beek or go to https://github.com/galaxyproject/tools-iuc."
    },
    {
        "name": "Download and Extract Reads in FASTQ",
        "description": "format from NCBI SRA",
        "category": "Get Data",
        "version": "3.0.10+galaxy0",
        "help": "**What it does?**\n\nThis tool extracts data (in fastq_ format) from the Short Read Archive (SRA) at the National Center for Biotechnology Information (NCBI). It is based on the fasterq-dump_ utility of the SRA Toolkit.  The following applies:\n\n - if data is paired-ended (or mate-pair) the tool will generate a collection of file pairs, in which each element will be a pair of fastq_ files containing forward and reverse mates.\n - if data is single ended, each element of the collection will be a single fastq_ dataset.\n\n\n\n    **How to use it?**\n\nThere are three ways in which you can download data:\n\n 1. Plain text input of accession number(s)\n 2. Providing a list of accessions from file\n 3. Extracting data from an already uploaded SRA dataset\n\nBelow we discuss each in detail.\n\n------\n\n**Plain text input of accession number(s)**\n\nWhen you type an accession number (e.g., `SRR1582967`) into **Accession** box and click **Execute** the tool will fetch the data for you. You can also provide a list of multiple accession numbers (e.g. `SRR3141592, SRR271828, SRR112358`).\n\n-----\n\n**Providing a list of accessions from file**\n\nA more realistic scenario is when you want to upload a number of datasets at once. To do this you need a list of accession, where there is only one accession per line (see below for information on how to generate such a file). Once you have this file:\n\n 1. Upload it into your history using Galaxy's upload tool\n 2. Once the list of accessions is uploaded choose *List of SRA accessions, one per line* from **select input type** dropdown\n 3. Choose uploaded file within the **sra accession list** field\n 4. Click **Execute**\n\n-----\n\n**Extract data from an already uploaded SRA dataset**\n\nIf an SRA dataset is already present in the history, the sequencing data can be extracted in a human-readable data format (fastq, sam, bam) by setting **select input type** drop-down to *SRA archive in current history*.\n    \n\n-----\n\n**Output**\n\nIn every case, fastq datasets produced will be saved in Galaxy's history as a collection_ - a single history element containing multiple datasets. In fact, regardless of the experimental design, three collections will be produced: one containing paired-end data, another containing single-end data, and a third one which contains reads which could not be classified.\nSome collections may be empty if the accessions provided in the list do not contain one of the type of data.\n\n.. class:: warningmark\n\nWhen you decide to dump technical reads (in Advanced Options Dump only biological reads is set to No), you will probably find your PAIRED data in the other data collection as it is impossible to determine if it was 2 biological reads or one biological and one technical.\n\n.. class:: warningmark\n\nBy default, only biological reads are dumped and in case of PAIRED dataset only the spots which have both reads will be in the paired-end collection. The remaining single reads will be in the other colletion.\nTo keep all reads, and potentially not have the same number of reads in forward and reverse use the --split-files option in Advanced Options, Select how to split the spots.\n\n\n-----\n\n**How to generate accession lists**\n\n 1. Go to **SRA Run Selector** by clicking this link_\n 2. Find the study you are interested in by typing a search term within the **Search** box. This can be a word (e.g., *mitochondria*) or an accession you have gotten from a paper (e.g., *SRR1582967*).\n 3. Once you click on the study of interest you will see the number of datasets in this study within the **Related SRA data** box\n 4. Click on the Runs number\n 5. On the page that would open you will see **Accession List** button\n 6. Clicking of this button will produce a file that you will need to upload into Galaxy and use as the input to this tool.\n    \n\n-----\n\n\n.. _fastq: https://en.wikipedia.org/wiki/FASTQ_format\n.. _fasterq-dump: https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump\n.. _collection: https://galaxyproject.org/tutorials/collections/\n.. _link: https://trace.ncbi.nlm.nih.gov/Traces/index.html?view=run_browser&display=reads\n\n\nFor credits, information, support and bug reports, please refer ato https://github.com/galaxyproject/tools-iuc."
    },
    {
        "name": "pysradb search",
        "description": "sequence metadata from SRA/ENA",
        "category": "Get Data",
        "version": "1.4.2+galaxy1",
        "help": ".. class:: infomark\n\n**Purpose**\n\npysradb allows to retrieve metadata, such as run accession numbers, from SRA and ENA based on multiple criteria:\n\n- Database:\tSRA or ENA\n- Query keywords\n- Accession number:\ta relevant study/experiment/sample/run accession number\n- Organism: scientific name of the sample organism\n- Library layout: paired or single-end reads\n- Sample size: rounded to the nearest megabase\n- Publication date\n- Sequencing platform: Illumina, Nanopore or PacBio\n- Library selection: method used to select and/or enrich the material being sequenced\n- Library source: Type of source material that is being sequenced\n- Library preparation strategy: sequencing technique intended for the library\n\n------\n\n.. class:: infomark\n\n**Outputs**\n\npysradb generates three different output types:\n\n- Raw metadata file\n- Statistics for the search query\n- Graphs to illustrate the search results\n\n------\n\n.. class:: infomark\n\n**Sequencing instruments**\n\n**Comparisons between HiSeq instruments**\n\nHiSeq 3000/4000 provides some improvements with respect the previous model HiSeq 2500:\n\n- HiSeq 3000/4000 genere up to 1.5 Tb and 5 Tb reads per run.\n- HiSeq 3000/4000 use patterned flow cell technology originally developed for HiSeq X platforms.\n- HiSeq 3000/4000 run 3 times faster and yield 65% more reads per lane. \n- HiSeq 3000/4000 patterned flow cells contain billions of nanowells at fixed, known positions on the flow cell. The structured organization enables clustering at higher densities compared to non-pattern HiSeq designs.\n\nHowever, the HiSeq 3000/4000 also have some also some limitations with respect to HiSeq 2500:\n\n- HiSeq 3000/4000 are not recommended for low complexity sequencing. Applications such as non-unique amplicons, 16S, are currently not recommended. \n- Libraries with low complexity within the first 25 bases of a read are not expected to produce high quality data.\n- Library size restrictions. Libraries that are too long can result in polyclonal clusters that span more than 1 well, these will not pass filter. Smaller libraries will preferentially amplify with Illumina's new kinetic exclusion amplification so tight library distributions ranging from 300-500 bp are recommended.\n- Very low tolerance for adapter dimers. Even as little as 1% adapter dimer can take up ~6% of sequencing reads, 10% contamination will take up 84% of reads. Illumina recommends you keep adapter contamination below 0.5% of your entire library.\n- Higher duplication rates as compared to HiSeq 2500.\n- Low quality read 2 (entire HiSeq 3000 install base is affected).\n\nHiSeq 3000/4000 support DNA-seq, RNA-seq , ChIP-Seq, mate-pair, small RNA and exome library preparation. Any library preparation where there is enough sequence diversity is currently supported. Amplicon, 16S and applications with low sequencing diversity are currently not supported on the HiSeq 3000 / 4000.\n\nHiSeq 2500 is considered the most reliable model according to different sources.\n\n**What type of read quality is expected from the HiSeq 3000/4000 ?**\n\n- 2 x 50bp  \u226585% bases > Q30\n- 2 x 75bp  \u226580% bases > Q30\n- 2 x 150bp \u226575% of bases >Q30\n\n**What is the difference between MiSeq and HiSeq?**\n\nHiSeq and MiSeq platforms are among the most widely used platform to study microbial communities. But the two platforms differ in the length and amount of reads. \nMiSeq can run 600 cycles to produce 200 million 300 bp reads, on the other hand, HiSeq 2500 can run 500 cycles to produce 120 million 250 bp.\n\n**What are the differences between HiSeq and NovaSeq?**\n\nThe Illumina NovaSeq provides a massive upgrade in sequencing throughput compared to the HiSeq 4000. There are more stringent library requirements and requires a \nlarger sample size. Due to the vast amount of data produced by the NovaSeq and the known issue of index swapping, unique dual-indexed libraries are required.\n\n**What are the characteristics of HiSeq X instruments?**\n\n- HiSeq X is recommended for whole genome sequencing only (including whole bisulfite sequencing). This means that it is not adequate for RNA-seq, exome, ChIP-seq or small RNA-seq applications. \n- Plant and animal samples can be sequenced on the HiSeq X.\n- Expect coverate is over 30x  or approximately 375 million reads per lane by loading one sample per lane.\n- Hiseq X Ten generates utilize 2x150 base pair read configurations and has slightly better GC coverage than the HiSeq 2500.\n\n**What are the differences between MiSeq and Nextseq?**\n\nThe NextSeq Series of systems delivers the power of high-throughput sequencing with the simplicity of a desktop sequencer. NextSeq instruments represent an improvement when compared with Miseq, despite generating sorter reads (150bp, compared to MiSeq 250bp). NextSeq is recommended in \nthe following applications & methods:\n\n- Exome & large panel sequencing (enrichment-based)\n- Single-cell profiling (scRNA-Seq, scDNA-Seq, oligo tagging assays)\n- Transcriptome sequencing (total RNA-Seq, mRNA-Seq, gene expression profiling)\n- Methylation sequencing\t\n- Metagenomic profiling (shotgun metagenomics, metatranscriptomics)\n- Cell-free sequencing & liquid biopsy analysis\n\nRegarding the maximum number of reads per ran, MiSeq can generate 25 million, vs 400 million generated by the Nextseq 550 instrument. MiSeq recommended for sequencing samples of low diversity.\n\n**What are the differences between HiSeq and NextSeq?**\n\nThe main technical difference between HiSeq and NextSeq will be the number of dyes each machines use. HiSeq uses traditional color coding with four different dyes, while NextSeq uses two dyes. This does not give any practical differences in terms of the data quality, but the trend in illumina sequencers are more into the direction of reducing the number of dyes.\n\n**What is the difference between Nextseq and NovaSeq?**\n\nThe NovaSeq 6000 system offers deep and broad coverage and is recommended for large whole-genome sequencing (human, plant, animal)\tprojects. It generates 250 bp reads, \nwith 20 billion maximum reads per run. NovaSeq 6000 instruments have not application based restrictions.\n\n**Illumina maximum read-length  summary**\n\n- MiSeq: between 300 and 600 bp\n- NextSeq: 300 bp\n- HiSeq 2500: between 250 and 500 bp (depending of the sofware)\n- HiSeq 4000: 150 bp\n- HiSeq X: 150 bp\n\n**Nanopore models - single-molecule ultra-long-read sequencing**\n\nNanopore sequencing provides the longest read lengths, from 500 bp to the current record of 2.3 Mb, with 10-30-kb genomic libraries being common. Even after error correction, sequencing error rates of corrected nanopore reads (1.5-9%) are still higher than those of corrected PacBio reads (<1%). \n\n**PacBio SMRT instruments - single-molecule long-read low-error rate sequencing**\n\nPacBio Sequel II CLR sequencing represents a major advancement in sequencing throughput over previous PacBio platforms with the production of more sequencing data and longer reads versus RS II and the Sequel I.\nThe PacBio HiFi sequencing method yields highly accurate long-read sequencing datasets with read lengths averaging 10-25 kb and accuracies greater than 99.5%."
    },
    {
        "name": "Faster Download and Extract Reads in FASTQ",
        "description": "format from NCBI SRA",
        "category": "Get Data",
        "version": "3.1.1+galaxy0",
        "help": "**What it does?**\n\nThis tool extracts data (in fastq_ format) from the Short Read Archive (SRA) at the National Center for Biotechnology Information (NCBI). It is based on the fasterq-dump_ utility of the SRA Toolkit.  The following applies:\n\n - if data is paired-ended (or mate-pair) the tool will generate a collection of file pairs, in which each element will be a pair of fastq_ files containing forward and reverse mates.\n - if data is single ended, each element of the collection will be a single fastq_ dataset.\n\n\n\n    **How to use it?**\n\nThere are three ways in which you can download data:\n\n 1. Plain text input of accession number(s)\n 2. Providing a list of accessions from file\n 3. Extracting data from an already uploaded SRA dataset\n\nBelow we discuss each in detail.\n\n------\n\n**Plain text input of accession number(s)**\n\nWhen you type an accession number (e.g., `SRR1582967`) into **Accession** box and click **Execute** the tool will fetch the data for you. You can also provide a list of multiple accession numbers (e.g. `SRR3141592, SRR271828, SRR112358`).\n\n-----\n\n**Providing a list of accessions from file**\n\nA more realistic scenario is when you want to upload a number of datasets at once. To do this you need a list of accession, where there is only one accession per line (see below for information on how to generate such a file). Once you have this file:\n\n 1. Upload it into your history using Galaxy's upload tool\n 2. Once the list of accessions is uploaded choose *List of SRA accessions, one per line* from **select input type** dropdown\n 3. Choose uploaded file within the **sra accession list** field\n 4. Click **Execute**\n\n-----\n\n**Extract data from an already uploaded SRA dataset**\n\nIf an SRA dataset is already present in the history, the sequencing data can be extracted in a human-readable data format (fastq, sam, bam) by setting **select input type** drop-down to *SRA archive in current history*.\n    \n\n-----\n\n**Output**\n\nIn every case, fastq datasets produced will be saved in Galaxy's history as a collection_ - a single history element containing multiple datasets. In fact, regardless of the experimental design, three collections will be produced: one containing paired-end data, another containing single-end data, and a third one which contains reads which could not be classified.\nSome collections may be empty if the accessions provided in the list do not contain one of the type of data.\n\n.. class:: warningmark\n\nWhen you decide to dump technical reads (in Advanced Options Dump only biological reads is set to No), you will probably find your PAIRED data in the other data collection as it is impossible to determine if it was 2 biological reads or one biological and one technical.\n\n.. class:: warningmark\n\nBy default, only biological reads are dumped and in case of PAIRED dataset only the spots which have both reads will be in the paired-end collection. The remaining single reads will be in the other colletion.\nTo keep all reads, and potentially not have the same number of reads in forward and reverse use the --split-files option in Advanced Options, Select how to split the spots.\n\n\n-----\n\n**How to generate accession lists**\n\n 1. Go to **SRA Run Selector** by clicking this link_\n 2. Find the study you are interested in by typing a search term within the **Search** box. This can be a word (e.g., *mitochondria*) or an accession you have gotten from a paper (e.g., *SRR1582967*).\n 3. Once you click on the study of interest you will see the number of datasets in this study within the **Related SRA data** box\n 4. Click on the Runs number\n 5. On the page that would open you will see **Accession List** button\n 6. Clicking of this button will produce a file that you will need to upload into Galaxy and use as the input to this tool.\n    \n\n-----\n\n\n.. _fastq: https://en.wikipedia.org/wiki/FASTQ_format\n.. _fasterq-dump: https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump\n.. _collection: https://galaxyproject.org/tutorials/collections/\n.. _link: https://trace.ncbi.nlm.nih.gov/Traces/index.html?view=run_browser&display=reads\n\n\nFor credits, information, support and bug reports, please refer ato https://github.com/galaxyproject/tools-iuc."
    },
    {
        "name": "Download and Extract Reads in FASTQ",
        "description": "format from NCBI SRA",
        "category": "Get Data",
        "version": "3.1.1+galaxy0",
        "help": "**What it does?**\n\nThis tool extracts data (in fastq_ format) from the Short Read Archive (SRA) at the National Center for Biotechnology Information (NCBI). It is based on the fasterq-dump_ utility of the SRA Toolkit.  The following applies:\n\n - if data is paired-ended (or mate-pair) the tool will generate a collection of file pairs, in which each element will be a pair of fastq_ files containing forward and reverse mates.\n - if data is single ended, each element of the collection will be a single fastq_ dataset.\n\n\n\n    **How to use it?**\n\nThere are three ways in which you can download data:\n\n 1. Plain text input of accession number(s)\n 2. Providing a list of accessions from file\n 3. Extracting data from an already uploaded SRA dataset\n\nBelow we discuss each in detail.\n\n------\n\n**Plain text input of accession number(s)**\n\nWhen you type an accession number (e.g., `SRR1582967`) into **Accession** box and click **Execute** the tool will fetch the data for you. You can also provide a list of multiple accession numbers (e.g. `SRR3141592, SRR271828, SRR112358`).\n\n-----\n\n**Providing a list of accessions from file**\n\nA more realistic scenario is when you want to upload a number of datasets at once. To do this you need a list of accession, where there is only one accession per line (see below for information on how to generate such a file). Once you have this file:\n\n 1. Upload it into your history using Galaxy's upload tool\n 2. Once the list of accessions is uploaded choose *List of SRA accessions, one per line* from **select input type** dropdown\n 3. Choose uploaded file within the **sra accession list** field\n 4. Click **Execute**\n\n-----\n\n**Extract data from an already uploaded SRA dataset**\n\nIf an SRA dataset is already present in the history, the sequencing data can be extracted in a human-readable data format (fastq, sam, bam) by setting **select input type** drop-down to *SRA archive in current history*.\n    \n\n-----\n\n**Output**\n\nIn every case, fastq datasets produced will be saved in Galaxy's history as a collection_ - a single history element containing multiple datasets. In fact, regardless of the experimental design, three collections will be produced: one containing paired-end data, another containing single-end data, and a third one which contains reads which could not be classified.\nSome collections may be empty if the accessions provided in the list do not contain one of the type of data.\n\n.. class:: warningmark\n\nWhen you decide to dump technical reads (in Advanced Options Dump only biological reads is set to No), you will probably find your PAIRED data in the other data collection as it is impossible to determine if it was 2 biological reads or one biological and one technical.\n\n.. class:: warningmark\n\nBy default, only biological reads are dumped and in case of PAIRED dataset only the spots which have both reads will be in the paired-end collection. The remaining single reads will be in the other colletion.\nTo keep all reads, and potentially not have the same number of reads in forward and reverse use the --split-files option in Advanced Options, Select how to split the spots.\n\n\n-----\n\n**How to generate accession lists**\n\n 1. Go to **SRA Run Selector** by clicking this link_\n 2. Find the study you are interested in by typing a search term within the **Search** box. This can be a word (e.g., *mitochondria*) or an accession you have gotten from a paper (e.g., *SRR1582967*).\n 3. Once you click on the study of interest you will see the number of datasets in this study within the **Related SRA data** box\n 4. Click on the Runs number\n 5. On the page that would open you will see **Accession List** button\n 6. Clicking of this button will produce a file that you will need to upload into Galaxy and use as the input to this tool.\n    \n\n-----\n\n\n.. _fastq: https://en.wikipedia.org/wiki/FASTQ_format\n.. _fasterq-dump: https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump\n.. _collection: https://galaxyproject.org/tutorials/collections/\n.. _link: https://trace.ncbi.nlm.nih.gov/Traces/index.html?view=run_browser&display=reads\n\n\nFor credits, information, support and bug reports, please refer ato https://github.com/galaxyproject/tools-iuc."
    },
    {
        "name": "NCBI Datasets Genomes",
        "description": "import data from the NCBI Datasets Genomes page",
        "category": "Get Data",
        "version": "13.14.0",
        "help": null
    },
    {
        "name": "NCBI ELink",
        "description": "link UIDs from one database to another",
        "category": "Get Data",
        "version": "1.1",
        "help": "NCBI Entrez ELink\n=================\n\nResponds to a list of UIDs in a given database with either a list of related\nUIDs (and relevancy scores) in the same database or a list of linked UIDs in\nanother Entrez database; checks for the existence of a specified link from a\nlist of one or more UIDs; creates a hyperlink to the primary LinkOut provider\nfor a specific UID and database, or lists LinkOut URLs and attributes for\nmultiple UIDs.\n\nCommands\n--------\n\nExample Queries\n---------------\n\nLink from protein to gene\n\n+----------------------+--------------------------------------+\n| Parameter            | Value                                |\n+======================+======================================+\n| From NCBI Database   | Protein                              |\n+----------------------+--------------------------------------+\n| Elink Command        | Neighbor                             |\n+----------------------+--------------------------------------+\n| To NCBI Database     | Gene                                 |\n+----------------------+--------------------------------------+\n| ID List              | 15718680 157427902                   |\n+----------------------+--------------------------------------+\n\nFind related articles to PMID 20210808 with scores\n\n+----------------------+--------------------------------------+\n| Parameter            | Value                                |\n+======================+======================================+\n| From NCBI Database   | PubMed                               |\n+----------------------+--------------------------------------+\n| Elink Command        | Scored Neighbors                     |\n+----------------------+--------------------------------------+\n| To NCBI Database     | PubMed                               |\n+----------------------+--------------------------------------+\n| ID List              | 20210808                             |\n+----------------------+--------------------------------------+\n\nList all possible links from two protein GIs\n\n+----------------------+--------------------------------------+\n| Parameter            | Value                                |\n+======================+======================================+\n| From NCBI Database   | Protein                              |\n+----------------------+--------------------------------------+\n| Elink Command        | ACheck                               |\n+----------------------+--------------------------------------+\n| ID List              | 15718680 157427902                   |\n+----------------------+--------------------------------------+\n\nList all possible links from two protein GIs to PubMed\n\n+----------------------+--------------------------------------+\n| Parameter            | Value                                |\n+======================+======================================+\n| From NCBI Database   | Protein                              |\n+----------------------+--------------------------------------+\n| Elink Command        | ACheck                               |\n+----------------------+--------------------------------------+\n| To NCBI Database     | PubMed                               |\n+----------------------+--------------------------------------+\n| ID List              | 15718680 157427902                   |\n+----------------------+--------------------------------------+\n\nCheck whether two nuccore sequences have \"related sequences\" links.\n\n+----------------------+--------------------------------------+\n| Parameter            | Value                                |\n+======================+======================================+\n| From NCBI Database   | Nuccore                              |\n+----------------------+--------------------------------------+\n| Elink Command        | NCheck                               |\n+----------------------+--------------------------------------+\n| ID List              | 21614549 219152114                   |\n+----------------------+--------------------------------------+\n\nList the LinkOut URLs for non-library providers for two pubmed abstracts.\n\n+----------------------+--------------------------------------+\n| Parameter            | Value                                |\n+======================+======================================+\n| From NCBI Database   | Pubmed                               |\n+----------------------+--------------------------------------+\n| Elink Command        | Links                                |\n+----------------------+--------------------------------------+\n| ID List              | 19880848 19822630                    |\n+----------------------+--------------------------------------+\n\nFind links to full text providers for two PubMed abstracts.\n\n+----------------------+--------------------------------------+\n| Parameter            | Value                                |\n+======================+======================================+\n| From NCBI Database   | Pubmed                               |\n+----------------------+--------------------------------------+\n| Elink Command        | Provider Links                       |\n+----------------------+--------------------------------------+\n| ID List              | 19880848 19822630                    |\n+----------------------+--------------------------------------+\n\n\n  \n\n\nUsage Guidelines and Requirements\n=================================\n\nFrequency, Timing, and Registration of E-utility URL Requests\n-------------------------------------------------------------\n\nIn order not to overload the E-utility servers, NCBI recommends that users\nlimit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time\nduring weekdays. Failure to comply with this policy may result in an IP address\nbeing blocked from accessing NCBI.\n\nMinimizing the Number of Requests\n---------------------------------\n\nIf a task requires searching for and/or downloading a large number of\nrecords, it is much more efficient to use the Entrez History to upload\nand/or retrieve these records in batches rather than using separate\nrequests for each record. Please refer to Application 3 in Chapter 3\nfor an example. Many thousands of IDs can be uploaded using a single\nEPost request, and several hundred records can be downloaded using one\nEFetch request.\n\n\nDisclaimer and Copyright Issues\n-------------------------------\n\nIn accordance with requirements of NCBI's E-Utilities, we must provide\nthe following disclaimer:\n\nPlease note that abstracts in PubMed may incorporate material that may\nbe protected by U.S. and foreign copyright laws. All persons\nreproducing, redistributing, or making commercial use of this\ninformation are expected to adhere to the terms and conditions asserted\nby the copyright holder. Transmission or reproduction of protected\nitems beyond that allowed by fair use (PDF) as defined in the copyright\nlaws requires the written permission of the copyright owners. NLM\nprovides no legal advice concerning distribution of copyrighted\nmaterials. Please consult your legal counsel. If you wish to do a large\ndata mining project on PubMed data, you can enter into a licensing\nagreement and lease the data for free from NLM. For more information on\nthis please see `http://www.nlm.nih.gov/databases/leased.html <http://www.nlm.nih.gov/databases/leased.html>`__\n\nThe `full disclaimer <http://www.ncbi.nlm.nih.gov/About/disclaimer.html>`__ is available on\ntheir website\n\nLiability\n~~~~~~~~~\n\nFor documents and software available from this server, the\nU.S. Government does not warrant or assume any legal liability or\nresponsibility for the accuracy, completeness, or usefulness of any\ninformation, apparatus, product, or process disclosed.\n\nEndorsement\n~~~~~~~~~~~\n\nNCBI does not endorse or recommend any commercial\nproducts, processes, or services. The views and opinions of authors\nexpressed on NCBI's Web sites do not necessarily state or reflect those\nof the U.S. Government, and they may not be used for advertising or\nproduct endorsement purposes.\n\nExternal Links\n~~~~~~~~~~~~~~\n\nSome NCBI Web pages may provide links to other Internet\nsites for the convenience of users. NCBI is not responsible for the\navailability or content of these external sites, nor does NCBI endorse,\nwarrant, or guarantee the products, services, or information described\nor offered at these other Internet sites. Users cannot assume that the\nexternal sites will abide by the same Privacy Policy to which NCBI\nadheres. It is the responsibility of the user to examine the copyright\nand licensing restrictions of linked pages and to secure all necessary\npermissions."
    },
    {
        "name": "NCBI EInfo",
        "description": "fetch NCBI database metadata",
        "category": "Get Data",
        "version": "1.1",
        "help": "NCBI Entrez EInfo\n=================\n\nProvides the number of records indexed in each field of a given database, the\ndate of the last update of the database, and the available links from the\ndatabase to other Entrez databases.\n\n\n  \n\n\nUsage Guidelines and Requirements\n=================================\n\nFrequency, Timing, and Registration of E-utility URL Requests\n-------------------------------------------------------------\n\nIn order not to overload the E-utility servers, NCBI recommends that users\nlimit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time\nduring weekdays. Failure to comply with this policy may result in an IP address\nbeing blocked from accessing NCBI.\n\nMinimizing the Number of Requests\n---------------------------------\n\nIf a task requires searching for and/or downloading a large number of\nrecords, it is much more efficient to use the Entrez History to upload\nand/or retrieve these records in batches rather than using separate\nrequests for each record. Please refer to Application 3 in Chapter 3\nfor an example. Many thousands of IDs can be uploaded using a single\nEPost request, and several hundred records can be downloaded using one\nEFetch request.\n\n\nDisclaimer and Copyright Issues\n-------------------------------\n\nIn accordance with requirements of NCBI's E-Utilities, we must provide\nthe following disclaimer:\n\nPlease note that abstracts in PubMed may incorporate material that may\nbe protected by U.S. and foreign copyright laws. All persons\nreproducing, redistributing, or making commercial use of this\ninformation are expected to adhere to the terms and conditions asserted\nby the copyright holder. Transmission or reproduction of protected\nitems beyond that allowed by fair use (PDF) as defined in the copyright\nlaws requires the written permission of the copyright owners. NLM\nprovides no legal advice concerning distribution of copyrighted\nmaterials. Please consult your legal counsel. If you wish to do a large\ndata mining project on PubMed data, you can enter into a licensing\nagreement and lease the data for free from NLM. For more information on\nthis please see `http://www.nlm.nih.gov/databases/leased.html <http://www.nlm.nih.gov/databases/leased.html>`__\n\nThe `full disclaimer <http://www.ncbi.nlm.nih.gov/About/disclaimer.html>`__ is available on\ntheir website\n\nLiability\n~~~~~~~~~\n\nFor documents and software available from this server, the\nU.S. Government does not warrant or assume any legal liability or\nresponsibility for the accuracy, completeness, or usefulness of any\ninformation, apparatus, product, or process disclosed.\n\nEndorsement\n~~~~~~~~~~~\n\nNCBI does not endorse or recommend any commercial\nproducts, processes, or services. The views and opinions of authors\nexpressed on NCBI's Web sites do not necessarily state or reflect those\nof the U.S. Government, and they may not be used for advertising or\nproduct endorsement purposes.\n\nExternal Links\n~~~~~~~~~~~~~~\n\nSome NCBI Web pages may provide links to other Internet\nsites for the convenience of users. NCBI is not responsible for the\navailability or content of these external sites, nor does NCBI endorse,\nwarrant, or guarantee the products, services, or information described\nor offered at these other Internet sites. Users cannot assume that the\nexternal sites will abide by the same Privacy Policy to which NCBI\nadheres. It is the responsibility of the user to examine the copyright\nand licensing restrictions of linked pages and to secure all necessary\npermissions."
    },
    {
        "name": "NCBI Datasets Genomes",
        "description": "download genome sequence, annotation and metadata",
        "category": "Get Data",
        "version": "17.0.0+galaxy0",
        "help": "**Download Genome Datasets from NCBI**\n\nDownload a genome dataset including genome, transcript and protein sequence, annotation and a detailed data report.\nGenome datasets can be specified by NCBI Assembly or BioProject accession(s) or by taxon.\n\nThe download is a three step process:\n\n1. A \"dehydrated\" zip file is downloaded which includes the metadata and the download URL)\n2. The metadata is transformed into a tabular (TSV) file\n3. The data is hydrated (the actual data is downloaded)\n\nThe 3rd step can be skipped by unselecting all output types in the `Include` parameter.\nThereby its possible to inspect the metadata prior to the actual data download. Also this\nallows to use the tool for querying data sets (and their accessions) of interest which\ncan then be downloaded in a second call using the accessions."
    },
    {
        "name": "EBI Search",
        "description": "to obtain search results on resources and services hosted at the EBI",
        "category": "Get Data",
        "version": "0.1.1",
        "help": "**What it does**\n\nThe European Bioinformatics Institute (EMBL-EBI) maintains the world\u2019s most comprehensive range of freely available and up-to-date molecular databases.\n\nEBI Search, also named as 'EB-eye', is a scalable search engine that:\n\n- provides text search functionality and uniform access to resources and services hosted at the European Bioinformatics Institute (EMBL-EBI)\n- is based on the consolidated  Apache Lucene  technology\n- exposes both a Web and  RESTful Web Services interfaces\n- provides inter-domain navigation via a network of cross-references\n\nHere, sample clients provided by EBI is used"
    },
    {
        "name": "UniProt",
        "description": "download proteome as XML or fasta",
        "category": "Get Data",
        "version": "2.3.0",
        "help": "**UniProt Downloader**\n\nDownloads either a UniProtXML file or a fasta file from UniProtKB\n\nThe Morpheus proteomics search algorithm can use the UniProtXML format as a search database.\n\nAvailable proteomes: http://www.uniprot.org/proteomes/\n\nAvailable taxon names: http://www.uniprot.org/taxonomy/\n\nExample taxon: http://www.uniprot.org/taxonomy/512562\n\nTaxon IDs or names can be entered as text or read from a column in a tabular dataset from your history.\n\nExample IDs and names releated to the Bacteria Helicobacter pylori (strain Shi470) ::\n\n\n - 512562\n - Shi470\n - Helicobacter pylori\n - Helicobacter\n - Helicobacteraceae\n\n\nUniProtKB help: http://www.uniprot.org/help/uniprotkb"
    },
    {
        "name": "Extract reads",
        "description": "in SAM or BAM format from NCBI SRA.",
        "category": "Get Data",
        "version": "1.2.5",
        "help": "This tool extracts reads from sra archives using sam-dump.\n        The sam-dump program is developed at NCBI, and is available at\n        http://www.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software.\n        Browse the NCBI SRA for SRR accessions at http://www.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=studies.\n        Galaxy tool wrapper originally written by Matt Shirley (mdshw5 at gmail.com).\n        Tool dependencies, clean-up and bug-fixes by Marius van den Beek (m.vandenbeek at gmail.com).\n        For support and bug reports contact Matt Shirley or Marius van den Beek or go to https://github.com/galaxyproject/tools-iuc."
    },
    {
        "name": "UniProt",
        "description": "download proteome as XML or fasta",
        "category": "Get Data",
        "version": "2.4.0",
        "help": "**UniProt Downloader**\n\nDownloads either a UniProtXML file or a fasta file from UniProtKB\n\nThe Morpheus proteomics search algorithm can use the UniProtXML format as a search database.\n\nAvailable proteomes: http://www.uniprot.org/proteomes/\n\nAvailable taxon names: http://www.uniprot.org/taxonomy/\n\nExample taxon: http://www.uniprot.org/taxonomy/512562\n\nExample protein: https://www.uniprot.org/uniprotkb/E1Q2I0/entry\n\nDescription of query fields: https://www.uniprot.org/help/query-fields\n\nIDs can be entered as text or read from a column in a tabular dataset from your history.\n\nExample IDs and names releated to the Bacteria Helicobacter pylori (strain Shi470) ::\n\n\n - 512562\n - Shi470\n - Helicobacter pylori\n - Helicobacter\n - Helicobacteraceae\n\n Example protein accession numbers from Helicobacter pylori:\n\n - E1Q2I0\n - E1Q3C4\n\n\nUniProtKB help: http://www.uniprot.org/help/uniprotkb"
    },
    {
        "name": "IEDB",
        "description": "MHC Binding prediction",
        "category": "Get Data",
        "version": "2.15.1",
        "help": "The IEDB is a free resource, funded by a contract from the National Institute of Allergy and Infectious Diseases. It offers easy searching of experimental data characterizing antibody and T cell epitopes studied in humans, non-human primates, and other animal species. \n\nThis tool retrieves epitope binding information about input peptide sequences by using the RESTful web services provided by IEDB.  \nThe webservices are described at:  http://tools.immuneepitope.org/main/tools-api/\nThat page also describes how to retrieve the available HLA alleles for class of epitope binding.\n\n**INPUTS**\n\n  peptide sequences from a fasta file or a column in a tabular file\n\n  HLA alleles either entered as text or one per line in a text file\n\n\n**OUTPUTS**\n  \n  A tabular file containing the results returned from the IEDB web service\n\n**Typical Workflow for Human MHC I Binding Prediction** \n\nThe RNAseq data for the subject would be used for:\n\n  - HLA prediction by seq2HLA\n  - Novel Antigen Prediction by a variety of workflows to generate a Antigen peptide fasta \n\n\n.. image:: $PATH_TO_IMAGES/IEDB_Workflow_QueryTabular.png\n   :width: 584\n   :height: 430\n\n.. note:: The seq2HLA ClassI.HLAgenotype4digits output needs to be converted for IEDB alleles.\n\nThe seq2HLA ClassI.HLAgenotype4digits output:\n\n.. image:: $PATH_TO_IMAGES/seq2HLA_ClassI.HLAgenotype4digits.png\n   :width: 285\n   :height: 77\n\nNeeds to be converted into IEDB formatted alleles:\n\n.. image:: $PATH_TO_IMAGES/IEDB_formatted_alleles.png\n   :width: 74\n   :height: 81\n\nIn the workflow above QueryTabular tool converts the alleles:\n\n  - Filter Dataset Input\n\n    * skip leading lines - *skip lines:* 1\n    * select columns - *columns:* 2,4\n    * regex replace value in column - *column:* 1  *regex pattern:* ^(\\\\w+[*]\\\\d\\\\d:\\\\d\\\\d\\\\d?).*$  *replacement expression:* HLA-\\\\1\n    * regex replace value in column - *column:* 2  *regex pattern:* ^(\\\\w+[*]\\\\d\\\\d:\\\\d\\\\d\\\\d?).*$  *replacement expression:* HLA-\\\\1\n\n  - SQL Query to generate tabular output\n\n    * SELECT c1 FROM t1 UNION SELECT c2 FROM t1\n\n\nThe IEDB formatting can also be performed by TextProcessing tools:\n\n.. image:: $PATH_TO_IMAGES/TextProcessingConversion.png\n   :width: 608\n   :height: 87\n\nThe TextProcessing steps to convert the alleles:\n\n  - Remove beginning -  removes the header line\n  - Replace Text - picks Allele 1 and Allele 2 from each line and reformats each on a separate line \n\n    * *Find pattern:* ^.*\\\\t([a-zA-Z]+[*][0-9]{2}:[0-9]{2,3}).*\\\\t.*\\\\t([a-zA-Z]+[*][0-9]{2}:[0-9]{2,3}).*\\\\t.*$\n    * *Replace with:* HLA-\\\\1\\\\nHLA-\\\\2\n\n  - Unique - remove duplicates"
    },
    {
        "name": "Copernicus Marine Data Store",
        "description": "retrieve marine data",
        "category": "Get Data",
        "version": "1.3.3+galaxy1",
        "help": "============================\nCopernicus Marine Data Store\n============================\n\n** Context **\n        \nThis tool is a wrapper to retrieve data from the Copernicus Marine Environment Monitoring Service (CMEMS).\n\n- It allows to retrieve data from the Copernicus Marine Service.\n- Any user willing to use this tool needs to `create a new account <https://data.marine.copernicus.eu/login>`_.\n- Set your Copernicus CMEMS API Key via: User > Preferences > Manage Information\n- Enter your username and password for Copernicus CMEMS\n- Compose your request directly on Copernicus Marine Data Store \n    - Choose there which data interest you click on the download button\n    - Then on the top right click again on the big download butto\n    - Log in\n    - Click on \"Automate\"\n    - You should have a pop-up window called \"Automate download\"\n    - Copy the \">_Command-Line Interface\" proposed there\n- Back on Galaxy paste it in the input field \"Paste API Request\".\n\nFor more information on the Command-Line Interface (CLI) go on `Copernicus Marine Toolbox CLI - Subset <https://help.marine.copernicus.eu/en/articles/7972861-copernicus-marine-toolbox-cli-subset>`\n\n** Input **\n        \nCommand line from the Copernicus marine services copy paste as a text.\n\n** Output **\n        \nA netcdf file containing the the data chose by the user from the Copernicus Marine Data Store."
    },
    {
        "name": "Generate pileup format",
        "description": "from NCBI sra.",
        "category": "Get Data",
        "version": "1.2.5",
        "help": "This tool produces pileup format from sra archives using sra-pileup.\n        The sra-pileup program is developed at NCBI, and is available at\n        http://www.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software.\n        Browse the NCBI SRA for SRR accessions at http://www.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=studies.\n        Galaxy tool wrapper originally written by Matt Shirley (mdshw5 at gmail.com).\n        Tool dependencies, clean-up and bug-fixes by Marius van den Beek (m.vandenbeek at gmail.com).\n        For support and bug reports contact Matt Shirley or Marius van den Beek or go to https://github.com/galaxyproject/tools-iuc."
    },
    {
        "name": "NCBI Datasets Genomes",
        "description": "download genome sequence, annotation and metadata",
        "category": "Get Data",
        "version": "14.3+galaxy0",
        "help": "**Download Genome Datasets from NCBI**\n\nDownload a genome dataset including genome, transcript and protein sequence, annotation and a detailed data report.\nGenome datasets can be specified by NCBI Assembly or BioProject accession or taxon. Datasets are downloaded as a zip file.\n\nTthe default genome dataset includes the following files (if available):\n * data_report.jsonl (genome assembly and annotation metadata, not always available)\n * genomic.fna (genomic sequences)\n * rna.fna (transcript sequences)\n * protein.faa (protein sequences)\n * genomic.gff (genome annotation in gff3 format)\n * dataset_catalog.json (a list of files and file types included in the dataset)"
    },
    {
        "name": "MouseMine",
        "description": "server",
        "category": "Get Data",
        "version": "1.0.0",
        "help": null
    },
    {
        "name": "EGA Download Client",
        "description": "",
        "category": "Get Data",
        "version": "4.0.0+galaxy1",
        "help": "The pyEGA3 download client is a python-based tool for viewing and downloading files from authorized EGA datasets.\n\nIf you have an EGA account, you can set your EGA credentials in the user preferences menu of Galaxy. Otherwise, default EGA credentials with access to an example dataset will be used.\n\npyEGA3 uses the EGA Data API and has several key features:\n\n- Files are transferred over secure https connections and received unencrypted, so no need for decryption after download.\n- Downloads resume from where they left off in the event that the connection is interrupted.\n- pyEGA3 supports file segmenting and parallelized download of segments, improving overall performance.\n- After download completes, file integrity is verified using checksums.\n- pyEGA3 implements the GA4GH-compliant htsget protocol for download of genomic ranges for data files with accompanying index files."
    },
    {
        "name": "Faster Download and Extract Reads in FASTQ",
        "description": "format from NCBI SRA",
        "category": "Get Data",
        "version": "2.10.4+galaxy1",
        "help": "**What it does?**\n\nThis tool extracts data (in fastq_ format) from the Short Read Archive (SRA) at the National Center for Biotechnology Information (NCBI). It is based on the fasterq-dump_ utility of the SRA Toolkit.\n\n**How to use it?**\n\nThere are three ways in which you can download data:\n\n 1. Data for single accession\n 2. Multiple datasets using a list of accessions\n 3. Extract data from already uploaded SRA dataset\n\nBelow we discuss each in detail.\n\n------\n\n**Uploading data for a single accession**\n\nWhen you type a single accession number (e.g., `SRR1582967`) into **Accession** box and click **Execute** the tool will fetch data for you. \n\n-----\n\n**Uploading multiple datasets using a list of accessions**\n\nA more realistic scenario is when you want to upload a number of datasets at once. To do this you need a list of accession, where there is only one accession per line (see below for information on how to generate such a file). Once you have this file:\n\n 1. Upload it into your history using Galaxy's upload tool\n 2. Once the list of accessions is uploaded choose *List of SRA accessions, one per line* from **select input type** dropdown\n 3. Choose uploaded file within the **sra accession list** field\n 4. Click **Execute**\n\n-----\n\n**Extract data from already uploaded SRA dataset**\n\nIf a SRA dataset is present in the history, it can be converted into fastq dataset by setting **select input type** drop-down to *SRA archive in current history*. Just like in the case of extracting data for single accession number the following applies:\n\n - if data is paired-ended (or mate-pair) the tool will generate a single *interleaved* dataset, in which forward and reverse mates are alternating (see example below).\n - if data is single ended, a standard fastq dataset will be produced\n\n-----\n\n**Output**\n\nIn every case, fastq datasets produced will be saved in Galaxy's history as a collection_ - a single history element containing multiple datasets. \nIn fact, three collections will be produced: one containing paired-end data, another containing single-end data, and a third one which contains reads which could not be classified.\nSome collections may be empty if the accessions provided in the list does not contain one of the type of data.\n\n.. class:: warningmark\n\nWhen you decide to dump technical reads (in Advanced Options Dump only biological reads is set to No), you will probably find your PAIRED data in the other data collection as it is impossible to determine if it was 2 biological reads or one biological and one technical.\n\n.. class:: warningmark\n\nBy default, only biological reads are dumped and in case of PAIRED dataset only the spots which have both reads will be in the paired-end collection. The remaining single reads will be in the other colletion.\nTo keep all reads, and maybe do not have the same number of reads in forward and reverse use the --split-files option in Advanced Options, Select how to split the spots.\n\n\n-----\n\n**How to generate accession lists**\n\n 1. Go to **SRA Run Selector** by clicking this link_\n 2. Find the study you are interested in by typing a search term within the **Search** box. This can be a word (e.g., *mitochondria*) or an accession you have gotten from a paper (e.g., *SRR1582967*).\n 3. Once you click on the study of interest you will see the number of datasets in this study within the **Related SRA data** box\n 4. Click on the Runs number\n 5. On the page that would open you will see **Accession List** button\n 6. Clicking of this button will produce a file that you will need to upload into Galaxy and use as the input to this tool.\n    \n\n-----\n\n\n.. _fastq: https://en.wikipedia.org/wiki/FASTQ_format\n.. _fastq-dump: https://ncbi.github.io/sra-tools/fastq-dump.html\n.. _fasterq-dump: https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump\n.. _collection: https://galaxyproject.org/tutorials/collections/\n.. _link: http://www.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=studies\n\n\nGalaxy tool wrapper originally written by Matt Shirley (mdshw5 at gmail.com).\nWrapper modified by Philip Mabon ( philip.mabon at phac-aspc.gc.ca ).\nTool dependencies, clean-up and bug-fixes by Marius van den Beek (m.vandenbeek at gmail.com).\nFor support and bug reports contact Matt Shirley or Marius van den Beek or go to https://github.com/galaxyproject/tools-iuc."
    },
    {
        "name": "Unipept",
        "description": "retrieve taxonomy for peptides",
        "category": "Get Data",
        "version": "2.0.1",
        "help": "**Unipept** \n\n    Retrieve Uniprot and taxanomic information for trypic peptides.\n    \n    Unipept API documentation - http://unipept.ugent.be/apidocs \n\n    **Input**\n\n    Input peptides can be retrieved from tabular, fasta, mzid, or pepxml datasets.  \n \n    Processing deatils::\n\n        The input peptides are split into typtic peptide fragments in order to match the Unipept records.   \n        Only fragments that are complete tryptic peptides between 5 and 50 animo acid in length will be matched by Unipept.\n        The match to the most specific tryptic fragment is reported.\n\n\n    **Unipept APIs**\n\n    **pept2prot**  - http://unipept.ugent.be/apidocs/pept2prot\n\n    Returns the list of UniProt entries containing a given tryptic peptide. This is the same information as provided on the Protein matches tab when performing a search with the Tryptic Peptide Analysis in the web interface. \n\n    By default, each object contains the following information fields extracted from the UniProt record::\n\n        peptide: the peptide that matched this record\n        uniprot_id: the UniProt accession number of the matching record\n        taxon_id: the NCBI taxon id of the organism associated with the matching record\n\n    When the extra parameter is set to true, objects contain the following additional fields extracted from the UniProt record::\n\n        taxon_name: the name of the organism associated with the matching UniProt record\n        ec_references: a space separated list of associated EC numbers\n        go_references: a space separated list of associated GO terms\n        refseq_ids: a space separated list of associated RefSeq accession numbers\n        refseq_protein_ids: a space separated list of associated RefSeq protein accession numbers\n        insdc_ids: a space separated list of associated insdc accession numbers\n        insdc_protein_ids: a space separated list of associated insdc protein accession numbers\n\n\n    **pept2taxa**  - http://unipept.ugent.be/apidocs/pept2taxa\n\n    Returns the set of organisms associated with the UniProt entries containing a given tryptic peptide. This is the same information as provided on the Lineage table tab when performing a search with the Tryptic Peptide Analysis in the web interface.\n\n    By default, each object contains the following information fields extracted from the UniProt record and NCBI taxonomy::\n\n        peptide: the peptide that matched this record\n        taxon_id: the NCBI taxon id of the organism associated with the matching record\n        taxon_name: the name of the organism associated with the matching record\n        taxon_rank: the taxonomic rank of the organism associated with the matching record\n\n    When the extra parameter is set to true, objects contain additional information about the lineages of the organism extracted from the NCBI taxonomy. The taxon id of each rank in the lineage is specified using the following information fields::\n\n        superkingdom_id\n        kingdom_id\n        subkingdom_id\n        superphylum_id\n        phylum_id\n        subphylum_id\n        superclass_id\n        class_id\n        subclass_id\n        infraclass_id\n        superorder_id\n        order_id\n        suborder_id\n        infraorder_id\n        parvorder_id\n        superfamily_id\n        family_id\n        subfamily_id\n        tribe_id\n        subtribe_id\n        genus_id\n        subgenus_id\n        species_group_id\n        species_subgroup_id\n        species_id\n        subspecies_id\n        varietas_id\n        forma_id\n\n\n    **pept2lca**  - http://unipept.ugent.be/apidocs/pept2lca\n\n    Returns the taxonomic lowest common ancestor for a given tryptic peptide. This is the same information as provided when performing a search with the Tryptic Peptide Analysis in the web interface.\n\n    By default, each object contains the following information fields extracted from the UniProt record and NCBI taxonomy::\n\n        peptide: the peptide that matched this record\n        taxon_id: the NCBI taxon id of the organism associated with the matching record\n        taxon_name: the name of the organism associated with the matching record\n        taxon_rank: the taxonomic rank of the organism associated with the matching record\n\n    When the extra parameter is set to true, objects contain additional information about the lineage of the taxonomic lowest common ancestor extracted from the NCBI taxonomy. The taxon id of each rank in the lineage is specified using the following information fields::\n\n        superkingdom_id\n        kingdom_id\n        subkingdom_id\n        superphylum_id\n        phylum_id\n        subphylum_id\n        superclass_id\n        class_id\n        subclass_id\n        infraclass_id\n        superorder_id\n        order_id\n        suborder_id\n        infraorder_id\n        parvorder_id\n        superfamily_id\n        family_id\n        subfamily_id\n        tribe_id\n        subtribe_id\n        genus_id\n        subgenus_id\n        species_group_id\n        species_subgroup_id\n        species_id\n        subspecies_id\n        varietas_id\n        forma_id\n\n\n    **Attributions**\n\n    The Unipept metaproteomics analysis pipeline\n    Bart Mesuere1,*, Griet Debyser2, Maarten Aerts3, Bart Devreese2, Peter Vandamme3 andPeter Dawyndt1\n    Article first published online: 11 FEB 2015\n    DOI: 10.1002/pmic.201400361\n    http://onlinelibrary.wiley.com/doi/10.1002/pmic.201400361/abstract;jsessionid=BFF1994E4C14DA73D7C907EB208AD710.f04t04"
    },
    {
        "name": "Faster Download and Extract Reads in FASTQ",
        "description": "format from NCBI SRA",
        "category": "Get Data",
        "version": "3.0.10+galaxy0",
        "help": "**What it does?**\n\nThis tool extracts data (in fastq_ format) from the Short Read Archive (SRA) at the National Center for Biotechnology Information (NCBI). It is based on the fasterq-dump_ utility of the SRA Toolkit.  The following applies:\n\n - if data is paired-ended (or mate-pair) the tool will generate a collection of file pairs, in which each element will be a pair of fastq_ files containing forward and reverse mates.\n - if data is single ended, each element of the collection will be a single fastq_ dataset.\n\n\n\n    **How to use it?**\n\nThere are three ways in which you can download data:\n\n 1. Plain text input of accession number(s)\n 2. Providing a list of accessions from file\n 3. Extracting data from an already uploaded SRA dataset\n\nBelow we discuss each in detail.\n\n------\n\n**Plain text input of accession number(s)**\n\nWhen you type an accession number (e.g., `SRR1582967`) into **Accession** box and click **Execute** the tool will fetch the data for you. You can also provide a list of multiple accession numbers (e.g. `SRR3141592, SRR271828, SRR112358`).\n\n-----\n\n**Providing a list of accessions from file**\n\nA more realistic scenario is when you want to upload a number of datasets at once. To do this you need a list of accession, where there is only one accession per line (see below for information on how to generate such a file). Once you have this file:\n\n 1. Upload it into your history using Galaxy's upload tool\n 2. Once the list of accessions is uploaded choose *List of SRA accessions, one per line* from **select input type** dropdown\n 3. Choose uploaded file within the **sra accession list** field\n 4. Click **Execute**\n\n-----\n\n**Extract data from an already uploaded SRA dataset**\n\nIf an SRA dataset is already present in the history, the sequencing data can be extracted in a human-readable data format (fastq, sam, bam) by setting **select input type** drop-down to *SRA archive in current history*.\n    \n\n-----\n\n**Output**\n\nIn every case, fastq datasets produced will be saved in Galaxy's history as a collection_ - a single history element containing multiple datasets. In fact, regardless of the experimental design, three collections will be produced: one containing paired-end data, another containing single-end data, and a third one which contains reads which could not be classified.\nSome collections may be empty if the accessions provided in the list do not contain one of the type of data.\n\n.. class:: warningmark\n\nWhen you decide to dump technical reads (in Advanced Options Dump only biological reads is set to No), you will probably find your PAIRED data in the other data collection as it is impossible to determine if it was 2 biological reads or one biological and one technical.\n\n.. class:: warningmark\n\nBy default, only biological reads are dumped and in case of PAIRED dataset only the spots which have both reads will be in the paired-end collection. The remaining single reads will be in the other colletion.\nTo keep all reads, and potentially not have the same number of reads in forward and reverse use the --split-files option in Advanced Options, Select how to split the spots.\n\n\n-----\n\n**How to generate accession lists**\n\n 1. Go to **SRA Run Selector** by clicking this link_\n 2. Find the study you are interested in by typing a search term within the **Search** box. This can be a word (e.g., *mitochondria*) or an accession you have gotten from a paper (e.g., *SRR1582967*).\n 3. Once you click on the study of interest you will see the number of datasets in this study within the **Related SRA data** box\n 4. Click on the Runs number\n 5. On the page that would open you will see **Accession List** button\n 6. Clicking of this button will produce a file that you will need to upload into Galaxy and use as the input to this tool.\n    \n\n-----\n\n\n.. _fastq: https://en.wikipedia.org/wiki/FASTQ_format\n.. _fasterq-dump: https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump\n.. _collection: https://galaxyproject.org/tutorials/collections/\n.. _link: https://trace.ncbi.nlm.nih.gov/Traces/index.html?view=run_browser&display=reads\n\n\nFor credits, information, support and bug reports, please refer ato https://github.com/galaxyproject/tools-iuc."
    },
    {
        "name": "GrameneMart",
        "description": "Central server",
        "category": "Get Data",
        "version": "1.0.1",
        "help": null
    },
    {
        "name": "Download and Extract Reads in BAM",
        "description": "format from NCBI SRA",
        "category": "Get Data",
        "version": "2.9.1.4",
        "help": "**What it does?**\n\nThis tool extracts data (in BAM_ format) from the Short Read Archive (SRA) at the National Center for Biotechnology Information (NCBI). It is based on the sam-dump_ utility of the SRA Toolkit.\n\n**How to use it?**\n\nThere are three ways in which you can download data:\n\n 1. Data for single accession\n 2. Multiple datasets using a list of accessions\n 3. Extract data from already uploaded SRA dataset\n\nBelow we discuss each in detail.\n\n------\n\n**Uploading data for a single accession**\n\nWhen you type a single accession number (e.g., `SRR1582967`) into **Accession** box and click **Execute** the tool will fetch data for you. As a result you will get a single BAM (or SAM) dataset in the history.\n\n-----\n\n**Uploading multiple datasets using a list of accessions**\n\nA more realistic scenario is when you want to upload a number of datasets at once. To do this you need a list of accession, where there is only one accession per line (see below for information on how to generate such a file). Once you have this file:\n\n 1. Upload it into your history using Galaxy's upload tool\n 2. Once the list of accessions is uploaded choose *List of SRA accessions, one per line* from **select input type** dropdown\n 3. Choose uploaded file within the **sra accession list** field\n 4. Click **Execute**\n\n.. class:: warningmark\n\nBAM datasets produced by this option will be saved in Galaxy's history as a collection_ - a single history element containing multiple datasets.\n\n-----\n\n**Extract data from already uploaded SRA dataset**\n\nIf a SRA dataset is present in the history, it can be converted into BAM dataset by setting **select input type** drop-down to *SRA archive in current history*. Just like in the case of extracting data for single accession number a single BAM dataset will be generated in the history.\n\n\n-----\n\n**How to generate accession lists**\n\n 1. Go to **SRA Run Selector** by clicking this link_\n 2. Find the study you are interested in by typing a search term within the **Search** box. This can be a word (e.g., *mitochondria*) or an accession you have gotten from a paper (e.g., *SRR1582967*).\n 3. Once you click on the study of interest you will see the number of datasets in this study within the **Related SRA data** box\n 4. Click on the Runs number\n 5. On the page that would open you will see **Accession List** button\n 6. Clicking of this button will produce a file that you will need to upload into Galaxy and use as the input to this tool.\n    \n\n-----\n\n.. _BAM: https://samtools.github.io/hts-specs/SAMv1.pdf\n.. _sam-dump: http://ncbi.github.io/sra-tools/sam-dump.html\n.. _collection: https://galaxyproject.org/tutorials/collections/\n.. _link: http://www.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=studies\n\n\nGalaxy tool wrapper originally written by Matt Shirley (mdshw5 at gmail.com).\nWrapper modified by Philip Mabon ( philip.mabon at phac-aspc.gc.ca ).\nTool dependencies, clean-up and bug-fixes by Marius van den Beek (m.vandenbeek at gmail.com).\nFor support and bug reports contact Matt Shirley or Marius van den Beek or go to https://github.com/galaxyproject/tools-iuc."
    },
    {
        "name": "Get sequences by Ensembl ID",
        "description": "using REST API",
        "category": "Get Data",
        "version": "0.1.2",
        "help": "**What it does**\n\nRetrieves FASTA sequences from Ensembl using its REST API.\n\nUses the `\"POST sequence/id\"`_ API endpoint.\n\n.. _\"POST sequence/id\": https://rest.ensembl.org/documentation/info/sequence_id_post"
    },
    {
        "name": "Extract reads",
        "description": "in SAM or BAM format from NCBI SRA.",
        "category": "Get Data",
        "version": "2.8.0",
        "help": "This tool extracts reads from sra archives using sam-dump.\n        The sam-dump program is developed at NCBI, and is available at\n        http://www.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software.\n        \n        Browse the NCBI SRA for SRR accessions at http://www.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=studies.\n\n        Galaxy tool wrapper originally written by Matt Shirley (mdshw5 at gmail.com).\n\n        Wrapper modified by Philip Mabon ( philip.mabon at phac-aspc.gc.ca ).\n\n        Tool dependencies, clean-up and bug-fixes by Marius van den Beek (m.vandenbeek at gmail.com).\n\n        For support and bug reports contact Matt Shirley or Marius van den Beek or go to https://github.com/galaxyproject/tools-iuc."
    },
    {
        "name": "NCBI ESummary",
        "description": "fetch summary of history/ids",
        "category": "Get Data",
        "version": "1.1",
        "help": "NCBI Entrez ESummary\n====================\n\nResponds to a list of UIDs from a given database with the corresponding\ndocument summaries.\n\nExample Queries\n---------------\n\nSearch against protein:\n\n+----------------------+--------------------------------------+\n| Parameter            | Value                                |\n+======================+======================================+\n| NCBI Database to Use | Protein                              |\n+----------------------+--------------------------------------+\n| ID List              | 28800982 28628843                    |\n+----------------------+--------------------------------------+\n\n\n  \n\n\nUsage Guidelines and Requirements\n=================================\n\nFrequency, Timing, and Registration of E-utility URL Requests\n-------------------------------------------------------------\n\nIn order not to overload the E-utility servers, NCBI recommends that users\nlimit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time\nduring weekdays. Failure to comply with this policy may result in an IP address\nbeing blocked from accessing NCBI.\n\nMinimizing the Number of Requests\n---------------------------------\n\nIf a task requires searching for and/or downloading a large number of\nrecords, it is much more efficient to use the Entrez History to upload\nand/or retrieve these records in batches rather than using separate\nrequests for each record. Please refer to Application 3 in Chapter 3\nfor an example. Many thousands of IDs can be uploaded using a single\nEPost request, and several hundred records can be downloaded using one\nEFetch request.\n\n\nDisclaimer and Copyright Issues\n-------------------------------\n\nIn accordance with requirements of NCBI's E-Utilities, we must provide\nthe following disclaimer:\n\nPlease note that abstracts in PubMed may incorporate material that may\nbe protected by U.S. and foreign copyright laws. All persons\nreproducing, redistributing, or making commercial use of this\ninformation are expected to adhere to the terms and conditions asserted\nby the copyright holder. Transmission or reproduction of protected\nitems beyond that allowed by fair use (PDF) as defined in the copyright\nlaws requires the written permission of the copyright owners. NLM\nprovides no legal advice concerning distribution of copyrighted\nmaterials. Please consult your legal counsel. If you wish to do a large\ndata mining project on PubMed data, you can enter into a licensing\nagreement and lease the data for free from NLM. For more information on\nthis please see `http://www.nlm.nih.gov/databases/leased.html <http://www.nlm.nih.gov/databases/leased.html>`__\n\nThe `full disclaimer <http://www.ncbi.nlm.nih.gov/About/disclaimer.html>`__ is available on\ntheir website\n\nLiability\n~~~~~~~~~\n\nFor documents and software available from this server, the\nU.S. Government does not warrant or assume any legal liability or\nresponsibility for the accuracy, completeness, or usefulness of any\ninformation, apparatus, product, or process disclosed.\n\nEndorsement\n~~~~~~~~~~~\n\nNCBI does not endorse or recommend any commercial\nproducts, processes, or services. The views and opinions of authors\nexpressed on NCBI's Web sites do not necessarily state or reflect those\nof the U.S. Government, and they may not be used for advertising or\nproduct endorsement purposes.\n\nExternal Links\n~~~~~~~~~~~~~~\n\nSome NCBI Web pages may provide links to other Internet\nsites for the convenience of users. NCBI is not responsible for the\navailability or content of these external sites, nor does NCBI endorse,\nwarrant, or guarantee the products, services, or information described\nor offered at these other Internet sites. Users cannot assume that the\nexternal sites will abide by the same Privacy Policy to which NCBI\nadheres. It is the responsibility of the user to examine the copyright\nand licensing restrictions of linked pages and to secure all necessary\npermissions."
    },
    {
        "name": "Download and Extract Reads in BAM",
        "description": "format from NCBI SRA",
        "category": "Get Data",
        "version": "2.10.7+galaxy1",
        "help": "**What it does?**\n\nThis tool extracts data (in BAM_ format) from the Short Read Archive (SRA) at the National Center for Biotechnology Information (NCBI). It is based on the sam-dump_ utility of the SRA Toolkit.\n\n**How to use it?**\n\nThere are three ways in which you can download data:\n\n 1. Data for single accession\n 2. Multiple datasets using a list of accessions\n 3. Extract data from already uploaded SRA dataset\n\nBelow we discuss each in detail.\n\n------\n\n**Uploading data for a single accession**\n\nWhen you type a single accession number (e.g., `SRR1582967`) into **Accession** box and click **Execute** the tool will fetch data for you. As a result you will get a single BAM (or SAM) dataset in the history.\n\n-----\n\n**Uploading multiple datasets using a list of accessions**\n\nA more realistic scenario is when you want to upload a number of datasets at once. To do this you need a list of accession, where there is only one accession per line (see below for information on how to generate such a file). Once you have this file:\n\n 1. Upload it into your history using Galaxy's upload tool\n 2. Once the list of accessions is uploaded choose *List of SRA accessions, one per line* from **select input type** dropdown\n 3. Choose uploaded file within the **sra accession list** field\n 4. Click **Execute**\n\n.. class:: warningmark\n\nBAM datasets produced by this option will be saved in Galaxy's history as a collection_ - a single history element containing multiple datasets.\n\n-----\n\n**Extract data from already uploaded SRA dataset**\n\nIf a SRA dataset is present in the history, it can be converted into BAM dataset by setting **select input type** drop-down to *SRA archive in current history*. Just like in the case of extracting data for single accession number a single BAM dataset will be generated in the history.\n\n\n-----\n\n**How to generate accession lists**\n\n 1. Go to **SRA Run Selector** by clicking this link_\n 2. Find the study you are interested in by typing a search term within the **Search** box. This can be a word (e.g., *mitochondria*) or an accession you have gotten from a paper (e.g., *SRR1582967*).\n 3. Once you click on the study of interest you will see the number of datasets in this study within the **Related SRA data** box\n 4. Click on the Runs number\n 5. On the page that would open you will see **Accession List** button\n 6. Clicking of this button will produce a file that you will need to upload into Galaxy and use as the input to this tool.\n    \n\n-----\n\n.. _BAM: https://samtools.github.io/hts-specs/SAMv1.pdf\n.. _sam-dump: https://ncbi.github.io/sra-tools/sam-dump.html\n.. _collection: https://galaxyproject.org/tutorials/collections/\n.. _link: https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=studies\n\n\nGalaxy tool wrapper originally written by Matt Shirley (mdshw5 at gmail.com).\nWrapper modified by Philip Mabon ( philip.mabon at phac-aspc.gc.ca ).\nTool dependencies, clean-up and bug-fixes by Marius van den Beek (m.vandenbeek at gmail.com).\nFor support and bug reports contact Matt Shirley or Marius van den Beek or go to https://github.com/galaxyproject/tools-iuc."
    },
    {
        "name": "NCBI Datasets Genomes",
        "description": "download genome sequence, annotation and metadata",
        "category": "Get Data",
        "version": "13.3.0",
        "help": "**Download Genome Datasets from NCBI**\n\nDownload a genome dataset including genome, transcript and protein sequence, annotation and a detailed data report.\nGenome datasets can be specified by NCBI Assembly or BioProject accession or taxon. Datasets are downloaded as a zip file.\n\nTthe default genome dataset includes the following files (if available):\n * genomic.fna (genomic sequences)\n * rna.fna (transcript sequences)\n * protein.faa (protein sequences)\n * genomic.gff (genome annotation in gff3 format)\n * data_report.jsonl (data report with genome assembly and annotation metadata)\n * dataset_catalog.json (a list of files and file types included in the dataset)"
    },
    {
        "name": "Export datasets",
        "description": "to repositories",
        "category": "Send Data",
        "version": "0.1.0",
        "help": null
    },
    {
        "name": "cURL",
        "description": "send cURL POST requests",
        "category": "Send Data",
        "version": "0.0.2",
        "help": "**What it does**\n\n    Uses cURL to send a file via POST."
    },
    {
        "name": "ENA Upload tool",
        "description": "",
        "category": "Send Data",
        "version": "0.3.1",
        "help": "This is a wrapper for the ENA upload tool in https://github.com/usegalaxy-eu/ena-upload-cli\n        The input metadata can be submitted following the tabular format of the templates in https://github.com/usegalaxy-eu/ena-upload-cli/tree/master/example_tables\n        It is also possible to submit an excel file by following the template in https://drive.google.com/file/d/1ncC22--tW2v-EI-te_r86sAZujIPAjlX/view?usp=sharing\n        For viral submissions a larger set of metadata is required, you can find the template in https://drive.google.com/file/d/1U4VdcczsIecIXxseV8svE1zO_CBUadog/view?usp=sharing"
    },
    {
        "name": "ENA Upload tool",
        "description": "",
        "category": "Send Data",
        "version": "0.3.2",
        "help": "This is a wrapper for the ENA upload tool in https://github.com/usegalaxy-eu/ena-upload-cli\n        The input metadata can be submitted following the tabular format of the templates in https://github.com/usegalaxy-eu/ena-upload-cli/tree/master/example_tables\n        It is also possible to submit an excel file by following the template in https://drive.google.com/file/d/1ncC22--tW2v-EI-te_r86sAZujIPAjlX/view?usp=sharing\n        For viral submissions a larger set of metadata is required, you can find the template in https://drive.google.com/file/d/1Gx78GKh58PmRjdmJ05DBbpObAL-3oUFX/view?usp=sharing"
    },
    {
        "name": "ENA Upload tool",
        "description": "",
        "category": "Send Data",
        "version": "0.3.3",
        "help": "This is a wrapper for the ENA upload tool in https://github.com/usegalaxy-eu/ena-upload-cli\n        The input metadata can be submitted following the tabular format of the templates in https://github.com/usegalaxy-eu/ena-upload-cli/tree/master/example_tables\n        It is also possible to submit an excel file by following the template in https://drive.google.com/file/d/1ncC22--tW2v-EI-te_r86sAZujIPAjlX/view?usp=sharing\n        For viral submissions a larger set of metadata is required, you can find the template in https://drive.google.com/file/d/1Gx78GKh58PmRjdmJ05DBbpObAL-3oUFX/view?usp=sharing"
    },
    {
        "name": "ENA Upload tool",
        "description": "",
        "category": "Send Data",
        "version": "0.4.1",
        "help": "This is a wrapper for the ENA upload tool in https://github.com/usegalaxy-eu/ena-upload-cli\n        The input metadata can be submitted following the tabular format of the templates in https://github.com/usegalaxy-eu/ena-upload-cli/tree/master/example_tables\n        It is also possible to submit an excel file by following the template in https://drive.google.com/file/d/1ncC22--tW2v-EI-te_r86sAZujIPAjlX/view?usp=sharing\n        For viral submissions a larger set of metadata is required, you can find the template in https://drive.google.com/file/d/1Gx78GKh58PmRjdmJ05DBbpObAL-3oUFX/view?usp=sharing"
    },
    {
        "name": "ENA Upload tool",
        "description": "",
        "category": "Send Data",
        "version": "0.4.3",
        "help": "This is a wrapper for the ENA upload tool in https://github.com/usegalaxy-eu/ena-upload-cli\n        The input metadata can be submitted following the tabular format of the templates in https://github.com/usegalaxy-eu/ena-upload-cli/tree/master/example_tables\n        It is also possible to submit an excel file by following the template in https://drive.google.com/file/d/1ncC22--tW2v-EI-te_r86sAZujIPAjlX/view?usp=sharing\n        For viral submissions a larger set of metadata is required, you can find the template in https://drive.google.com/file/d/1Gx78GKh58PmRjdmJ05DBbpObAL-3oUFX/view?usp=sharing"
    },
    {
        "name": "ENA Upload tool",
        "description": "",
        "category": "Send Data",
        "version": "0.5.3",
        "help": "This is a wrapper for the ENA upload tool in https://github.com/usegalaxy-eu/ena-upload-cli\n        The input metadata can be submitted following the tabular format of the templates in https://github.com/usegalaxy-eu/ena-upload-cli/tree/master/example_tables\n        It is also possible to submit an excel file by following the template in https://github.com/ELIXIR-Belgium/ENA-metadata-templates"
    },
    {
        "name": "ENA Upload tool",
        "description": "",
        "category": "Send Data",
        "version": "0.6.1",
        "help": "This is a wrapper for the ENA upload tool in https://github.com/usegalaxy-eu/ena-upload-cli\n        The input metadata can be submitted following the tabular format of the templates in https://github.com/usegalaxy-eu/ena-upload-cli/tree/master/example_tables\n        It is also possible to submit an excel file by following the template in https://github.com/ELIXIR-Belgium/ENA-metadata-templates"
    },
    {
        "name": "ENA Upload tool",
        "description": "",
        "category": "Send Data",
        "version": "0.6.0",
        "help": "This is a wrapper for the ENA upload tool in https://github.com/usegalaxy-eu/ena-upload-cli\n        The input metadata can be submitted following the tabular format of the templates in https://github.com/usegalaxy-eu/ena-upload-cli/tree/master/example_tables\n        It is also possible to submit an excel file by following the template in https://github.com/ELIXIR-Belgium/ENA-metadata-templates"
    },
    {
        "name": "ENA Upload tool",
        "description": "",
        "category": "Send Data",
        "version": "0.6.3+galaxy0",
        "help": "This is a wrapper for the ENA upload tool in https://github.com/usegalaxy-eu/ena-upload-cli\n        The input metadata can be submitted following the tabular format of the templates or their excel spreadsheet equivalent in https://github.com/ELIXIR-Belgium/ENA-metadata-templates. This template repo provides ready to use sheets for every ENA sample checklist and is automatically updated."
    },
    {
        "name": "ENA Upload tool",
        "description": "Submission of (meta)data to the European Nucleotide Archive (ENA)",
        "category": "Send Data",
        "version": "0.7.1+galaxy1",
        "help": "This is a wrapper for the ENA upload tool in https://github.com/usegalaxy-eu/ena-upload-cli. The input metadata can be submitted following the tabular format of the templates or their excel spreadsheet equivalent in https://github.com/ELIXIR-Belgium/ENA-metadata-templates. This template repo provides ready to use sheets for every ENA sample checklist and is automatically updated.\n    \n        .. class:: warningmark\n    \n            The ENA upload tool won't work unless you have provided an ENA Webin ID in User > Preferences > Manage Information > ENA Webin account details."
    },
    {
        "name": "ENA Upload tool",
        "description": "Submission of (meta)data to the European Nucleotide Archive (ENA)",
        "category": "Send Data",
        "version": "0.7.3+galaxy1",
        "help": "This is a wrapper for the ENA upload tool in https://github.com/usegalaxy-eu/ena-upload-cli. The input metadata can be submitted following the tabular format of the templates or their excel spreadsheet equivalent in https://github.com/ELIXIR-Belgium/ENA-metadata-templates. This template repo provides ready to use sheets for every ENA sample checklist and is automatically updated.\n    \n        .. class:: warningmark\n    \n            The ENA upload tool won't work unless you have provided an ENA Webin ID in User > Preferences > Manage Information > ENA Webin account details."
    },
    {
        "name": "ENA Upload tool",
        "description": "Submission of (meta)data to the European Nucleotide Archive (ENA)",
        "category": "Send Data",
        "version": "0.7.5+galaxy1",
        "help": "This is a wrapper for the ENA upload tool in https://github.com/usegalaxy-eu/ena-upload-cli. The input metadata can be submitted following the tabular format of the templates or their excel spreadsheet equivalent in https://github.com/ELIXIR-Belgium/ENA-metadata-templates. This template repo provides ready to use sheets for every ENA sample checklist and is automatically updated.\n    \n        .. class:: warningmark\n    \n            The ENA upload tool won't work unless you have provided an ENA Webin ID in User > Preferences > Manage Information > ENA Webin account details."
    },
    {
        "name": "ENA Upload tool",
        "description": "Submission of (meta)data to the European Nucleotide Archive (ENA)",
        "category": "Send Data",
        "version": "0.8.0+galaxy0",
        "help": "This is a wrapper for the ENA upload tool in https://github.com/usegalaxy-eu/ena-upload-cli. The input metadata can be submitted following the tabular format of the templates or their excel spreadsheet equivalent in https://github.com/ELIXIR-Belgium/ENA-metadata-templates. This template repo provides ready to use sheets for every ENA sample checklist and is automatically updated.\n    \n        .. class:: warningmark\n    \n            The ENA upload tool won't work unless you have provided an ENA Webin ID in User > Preferences > Manage Information > ENA Webin account details."
    },
    {
        "name": "ENA Upload tool",
        "description": "",
        "category": "Send Data",
        "version": "0.3",
        "help": "This is a wrapper for the ENA upload tool in https://github.com/usegalaxy-eu/ena-upload-cli\n        The input metadata can be submitted following the tabular format of the templates in https://github.com/usegalaxy-eu/ena-upload-cli/tree/master/example_tables\n        It is also possible to submit an excel file by following the template in https://drive.google.com/file/d/1ncC22--tW2v-EI-te_r86sAZujIPAjlX/view?usp=sharing\n        For viral submissions a larger set of metadata is required, you can find the template in https://drive.google.com/file/d/1U4VdcczsIecIXxseV8svE1zO_CBUadog/view?usp=sharing"
    },
    {
        "name": "Unzip collection",
        "description": "",
        "category": "Collection Operations",
        "version": "1.0.0",
        "help": "========\nSynopsis\n========\n\nTakes a paired collection and \"unzips\" it into two simple dataset collections (lists of datasets). \n\n===========\nDescription\n===========\n\nGiven a paired collection of forward and reverse reads this tool will \"unzip\" it into two collections containing forward and reverse reads, respectively:\n\n.. image:: ${static_path}/images/tools/collection_ops/unzip.svg\n  :width: 500\n  :alt: Unzipping operation\n\n-----\n\n.. class:: infomark\n\nThis tool will create new history datasets from your collection but your quota usage will not increase."
    },
    {
        "name": "Zip collections",
        "description": "",
        "category": "Collection Operations",
        "version": "1.0.0",
        "help": "========\nSynopsis\n========\n\nTakes two collections and creates a paired collection from them. \n\n===========\nDescription\n===========\n\nIf you have one collection containing only forward reads and one containing only reverse, this tools will \"zip\" them together into a simple paired collection. For example, given two collections with `forward` and `reverse` reads they can be \"zipped\" into a single paired collection:\n\n.. image:: ${static_path}/images/tools/collection_ops/zip.svg\n  :width: 500\n  :alt: Zipping operation\n\n\n-----\n    \n.. class:: infomark\n\nThis tool will create new history datasets for your collection but your quota usage will not increase."
    },
    {
        "name": "Filter failed datasets",
        "description": "",
        "category": "Collection Operations",
        "version": "1.0.0",
        "help": "========\nSynopsis\n========\n\nRemoves datasets in error (red) from a collection.\n\n===========\nDescription\n===========\n\nThis tool takes a dataset collection and filters out (removes) datasets in the failed (red) state. This is useful for continuing a multi-sample analysis when one or more of the samples fails at some point.\n\n.. image:: ${static_path}/images/tools/collection_ops/filter_error.svg\n  :width: 500\n  :alt: Filter failed datasets\n\n-----\n\n.. class:: infomark\n\nThis tool will create new history datasets from your collection but your quota usage will not increase."
    },
    {
        "name": "Filter empty datasets",
        "description": "",
        "category": "Collection Operations",
        "version": "1.0.0",
        "help": "========\nSynopsis\n========\n\nRemoves empty elements from a collection.\n\nThis tool takes a dataset collection and filters out (removes) empty datasets. This is useful for continuing a multi-sample analysis when downstream tools require datasets to have content.\n\n.. image:: ${static_path}/images/tools/collection_ops/filter_empty.svg\n  :width: 500\n  :alt: Filtering empty datasets\n\n-----\n\n.. class:: infomark\n\nThis tool will create new history datasets from your collection but your quota usage will not increase."
    },
    {
        "name": "Filter null elements",
        "description": "",
        "category": "Collection Operations",
        "version": "1.0.0",
        "help": "========\nSynopsis\n========\n\nRemoves null elements from a collection.\n\nThis tool takes a dataset collection and filters out nulls. This is useful for removing elements that resulted from conditional execution of jobs.\n\n.. class:: infomark\n\nThis tool will create new history datasets from your collection but your quota usage will not increase."
    },
    {
        "name": "Flatten collection",
        "description": "",
        "category": "Collection Operations",
        "version": "1.0.0",
        "help": "========\nSynopsis\n========\n\nFlattens nested collection into a simple list.\n\n===========\nDescription\n===========\n\nThis tool takes nested collections such as a list of lists or a list of dataset pairs and produces a flat list from the inputs. It effectively \"flattens\" the hierarchy. The collection identifiers are merged together (using \"_\" as default) to create new collection identifiers in the flattened result:\n\n.. image:: ${static_path}/images/tools/collection_ops/flatten.svg\n  :width: 500\n  :alt: Flattening operation\n\n----\n\n.. class:: infomark\n\nThis tool will create new history datasets from your collection but your quota usage will not increase."
    },
    {
        "name": "Merge collections",
        "description": "",
        "category": "Collection Operations",
        "version": "1.0.0",
        "help": "========\nSynopsis\n========\n\nTakes two or more collections and creates a single collection from them. \n\n===========\nDescription\n===========\n\nBy default the tool assumes that collections that are being merged have unique dataset names. If it not the case only one (the first) of the datasets with a repeated name will be included in the merged collection. For example, suppose you have two collections. Each has two datasets named \"A\" and \"B\"::\n\n Collection 1: [Dataset A] \n               [Dataset B] \n               [Dataset X]\n Collection 2: [Dataset A] \n               [Dataset B] \n               [Dataset Y]\n\nMerging them will produce a single collection with only two datasets::\n\n Merged Collection: [Dataset A] \n                    [Dataset B] \n                    [Dataset X] \n                    [Dataset Y]\n\nThis behavior can be changed by clicking on \"*Advanced Options*\" link. The following options are available:\n\n**Keep first instance (Default behavior)**\n\nInput::\n\n Collection 1: [Dataset A] \n               [Dataset B] \n               [Dataset X]\n Collection 2: [Dataset A] \n               [Dataset B] \n               [Dataset Y]\n\nOutput::\n\n Merged Collection: [Dataset A] \n                    [Dataset B] \n                    [Dataset X] \n                    [Dataset Y]\n\nHere if two collection have identical dataset names, a dataset is chosen from the *first* collection.\n\n-----\n\n**Keep first instance**\n\nInput::\n\n Collection 1: [Dataset A] \n               [Dataset B] \n               [Dataset X]\n Collection 2: [Dataset A] \n               [Dataset B] \n               [Dataset Y]\nOutput::\n\n Merged Collection: [Dataset A] \n                    [Dataset B] \n                    [Dataset X] \n                    [Dataset Y]\n\nHere if two collection have identical dataset names, a dataset is chosen from the *last* collection.\n\n-----\n\n**Append suffix to conflicted element identifiers**\n\nInput::\n\n Collection 1: [Dataset A] \n               [Dataset B] \n               [Dataset X]\n Collection 2: [Dataset A] \n               [Dataset B] \n               [Dataset Y]\n\nOutput::\n\n Merged Collection: [Dataset A_1] \n                    [Dataset B_1]\n                    [Dataset A_2] \n                    [Dataset B_2]  \n                    [Dataset X] \n                    [Dataset Y]\n\n----\n\n**Append suffix to conflicted element identifiers after first on encountered**\n\nInput::\n\n Collection 1: [Dataset A] \n               [Dataset B] \n               [Dataset X]\n Collection 2: [Dataset A] \n               [Dataset B] \n               [Dataset Y]\n\nOutput::\n\n Merged Collection: [Dataset A] \n                    [Dataset B]\n                    [Dataset A_2] \n                    [Dataset B_2]  \n                    [Dataset X] \n                    [Dataset Y]\n\n------\n\n**Append suffix to every element identifier**\n\nInput::\n\n Collection 1: [Dataset A] \n               [Dataset B] \n               [Dataset X]\n Collection 2: [Dataset A] \n               [Dataset B] \n               [Dataset Y]\n\nOutput::\n\n Merged Collection: [Dataset A_1] \n                    [Dataset B_2]\n                    [Dataset A_2] \n                    [Dataset B_2]  \n                    [Dataset X_1] \n                    [Dataset Y_2]\n\n-----\n\n**Fail collection creation**\n\nThis option will simply trigger an error.\n\n------\n\n.. class:: infomark\n\nThis tool will create new history datasets for your collection but your quota usage will not increase."
    },
    {
        "name": "Relabel identifiers",
        "description": "",
        "category": "Collection Operations",
        "version": "1.1.0",
        "help": "========\nSynopsis\n========\n\nChanges identifiers of datasets within a collection using identifiers from a supplied file. \n\n===========\nDescription\n===========\n\nNew identifiers can be supplied as either a simple list or a tab-delimited file mapping old identifiers to new ones. This is controlled using **How should the new identifiers be specified?** drop-down:\n\n**Use lines in a simple text file as new identifiers**\n\nGiven a collection::\n\n Collection: [Dataset A] \n             [Dataset B] \n             [Dataset X]\n\nand a simple text file::\n\n             Alpha\n             Beta\n             Gamma\n\nthe tool will return::\n\n Collection: [Dataset Alpha] \n             [Dataset Beta] \n             [Dataset Gamma]\n\n.. class:: infomark\n\n**Note** that the order and number of entries in the text file must match the order of the items you want to rename in your dataset collection.\n\n-------\n\n**Map original identifiers to new ones using a two-column table**\n\nGiven a collection::\n\n Collection: [Dataset A] \n             [Dataset B] \n             [Dataset X]\n\nand a simple tabular file (you can see that entries do not have to be in order here)::\n\n             B Beta\n             X Gamma\n             A Alpha\n\nthe tool will return::\n\n Collection: [Dataset Alpha] \n             [Dataset Beta] \n             [Dataset Gamma]\n\n-------\n\n**Map original identifiers to new ones using a two-column table**\n\nThis mode works exactly as the previous one, but the tabular mapping file is allowed to have more than two columns, and you can specify which of them holds the original and new element identifiers, respectively.\n\n-------\n\n.. class:: warningmark\n\nValid identifiers must contain only characters (a-z, A-Z), numbers (0-9), dash (-), underscore (_), dot (.), space ( ) and comma (,). Other characters are not allowed.\n\n.. class:: infomark\n\nThis tool will create new history datasets from your collection but your quota usage will not increase."
    },
    {
        "name": "Filter collection",
        "description": "",
        "category": "Collection Operations",
        "version": "1.0.0",
        "help": "========\nSynopsis\n========\n\nFilters elements from a collection using a list supplied in a file.\n\n===========\nDescription\n===========\n\nThis tools allow filtering elements from a data collection.  It takes an input collection and a text file with names (i.e. identifiers). The tool behavious is controlled by **How should the elements to remove be determined?** drop-down. It has the following options:\n\n**Remove if identifiers are ABSENT from file**\n\nGiven a collection::\n\n Collection: [Dataset A] \n             [Dataset B] \n             [Dataset X]\n\nand a text file::\n\n             A\n             B\n             Z\n\nthe tool will return two collections::\n\n (filtered):  [Dataset A]\n              [Dataset B]\n\n (discarded): [Dataset X]\n\n------\n\n**Remove if identifiers are PRESENT in file**\n\nGiven a collection::\n\n Collection: [Dataset A] \n             [Dataset B] \n             [Dataset X]\n\nand a text file::\n\n             A\n             B\n             Z\n\nthe tool will return two collections::\n\n (filtered):  [Dataset X]\n\n (discarded): [Dataset A]\n              [Dataset B]\n\n.. class:: warningmark\n\n**Note** how the tool deals with the ``Z`` entry.\n\n.. class:: infomark\n\n\nThis tool will create new history datasets from your collection but your quota usage will not increase."
    },
    {
        "name": "Sort collection",
        "description": "",
        "category": "Collection Operations",
        "version": "1.0.0",
        "help": "========\nSynopsis\n========\n\nSorts dataset collection alphabetically, numerically, or using predetermined order from a supplied file.\n\n===========\nDescription\n===========\n\n**Numeric sort**\n\nThe tool sort in ascending order. When *numeric* sort is chosen, the tool ignores non-numeric characters. For example, if a collection contains the following elements::\n\n Collection: [Horse123] \n             [Donkey543] \n             [Mule176]\n\nThe tool will output::\n\n Collection: [Horse123]\n             [Mule176] \n             [Donkey543] \n\n-------\n\n**Sorting from file**\n\nAlternative, one can supply a single column text file containing elements identifiers in the desired sort order. For example, suppose there a collection::\n\n Collection: [Horse123] \n             [Donkey543] \n             [Mule176]\n\nand a file specifying sort order::\n\n Donkey543\n Horse123 \n Mule176\n \nthe output will predictably look like this::\n\n Collection: [Donkey543] \n             [Horse123] \n             [Mule176]\n\n-------\n\n.. class:: infomark\n\nThis tool will create new history datasets from your collection but your quota usage will not increase."
    },
    {
        "name": "Harmonize two collections",
        "description": "",
        "category": "Collection Operations",
        "version": "1.0.0",
        "help": "========\nSynopsis\n========\n\nHarmonize 2 collections: Inputs are 2 collections. Outputs are 2 collections with:\n- Same identifiers (identifiers which are specific to one or the other are removed)\n- Identifiers are in the same order\n\n=======\nExample\n=======\n\nIf the inputs are::\n\n Collection1: [Horse123] \n              [Donkey543] \n              [Mule176]\n\n Collection2: [Horse] \n              [Mule176] \n              [Donkey543]\nThe tool will output::\n\n Collection1: [Donkey543] \n              [Mule176]\n\n Collection2: [Donkey543] \n              [Mule176]\n\n-------\n\n.. class:: infomark\n\nThis tool will create new history datasets from your collection but your quota usage will not increase."
    },
    {
        "name": "Flat Cross Product",
        "description": "",
        "category": "Collection Operations",
        "version": "1.0.0",
        "help": "========\nSynopsis\n========\n\n\nThis tool organizes two dataset lists so that Galaxy's normal collection processing produces\nan all-vs-all style analyses of the initial inputs when applied to the outputs of this tool.\n\nWhile a description of what it does standalone is technical and math heavy, how\nit works within an ad-hoc analysis or workflow can be quite straight forward and hopefully is easier\nto understand. For this reason, the next section describes how to use this tool in context and\nthe technical details follow after that. Hopefully, the \"how it works\" details aren't nessecary to\nunderstand the \"how to use it\" details of this tool - at least for simple things.\n\n\n\n====================\nHow to use this tool\n====================\n\n\n\nThis tool can be used in and out of workflows, but workflows will be used to illustrate the ordering of\ntools and connections between them. Imagine a tool that compares two individual datasets and how\nthat might be connected to list inputs in a workflow. This simiple case is shown below:\n\n.. image:: ${static_path}/images/tools/collection_ops/dot_product.png\n  :alt: The Dot Product of Two Collections\n  :width: 500\n\nIn this configuration - the two datasets will be matched and compared element-wise. So the first dataset\nof \"Input List 1\" will be compared to the first dataset in \"Input List 2\" and the resulting\ndataset will be the first dataset in the output list generated using this comparison tool. In this configuration\nthe lists need to have the same number of elements and ideally matching element identifiers.\n\nThis matching up of elements is a very natural way to \"map\" an operation (or in Galaxy parlance, a tool)\nover two lists. However, sometimes the desire is to compare each element of the first list to each element of the\nsecond list. This tool enables that.\n\n\n\nRunning input lists through this tool produces new dataset lists (described in detail below) that when using\nthe same natural element-wise matching \"map over\" semantics described above produce every combination of the\nelements of the two lists compared against each other. Running a tool with these two outputs instead of the inital\ntwo input produces a list of the comparison of each combination of pairs from the respective inputs.\n\n.. image:: ${static_path}/images/tools/collection_ops/flat_crossproduct_output.png\n  :alt: The Flat Cartesian Product of Two Collections\n  :width: 500\n\nThe result of running a subsequent tool with the outputs produced by this tool will be a much larger list\nwhose element identifiers are the concatenation of the combinations of the elements identifiers from the\ntwo input lists.\n\n.. image:: ${static_path}/images/tools/collection_ops/flat_crossproduct_separator.png\n  :alt: Flat Cross Product Identifier Separator\n  :width: 500\n\n============================================\nWhat this tool does (technical details)\n============================================\n\nThis tool consumes two lists - we will call them ``input_a`` and ``input_b``. If ``input_a``\nhas length ``n`` and dataset elements identified as ``a1``, ``a2``, ... ``an`` and ``input_b``\nhas length ``m`` and dataset elements identified as ``b1``, ``b2``, ... ``bm``, then this tool\nproduces a pair of larger lists - each of size ``n*m``.\n\nBoth output lists will be the same length and contain the same set of element identifiers in the\nsame order. If the kth input can be described as ``(i-1)*n + (j-1)`` where ``1 <= i <= m`` and ``1 <= j <= n`` \nthen the element identifier for this kth element is the concatenation of the element identifier for\nthe ith item of ``input_a`` and the jth item of ``input_b``.\n\nIn the first output list, this kth element will be the ith element of ``input_a``. In the second\noutput list, the kth element will be the jth element of ``input_b``.\n\n.. image:: ${static_path}/images/tools/collection_ops/flat_cross_product_outputs.png\n  :alt: Flat Cross Product Outputs\n  :width: 500\n\nThese list structures might appear to be a little odd, but they have the very useful property\nthat if you match up corresponding elements of the lists the result is each combination of\nelements in ``input_a`` and ``input_b`` are matched up once.\n\n.. image:: ${static_path}/images/tools/collection_ops/flat_cross_product_matched.png\n  :alt: Flat Cross Product Matching Datasets\n  :width: 500\n\nRunning a downstream comparison tool that compares two datasets with these two lists produces a\nnew list with every combination of comparisons.\n\n.. image:: ${static_path}/images/tools/collection_ops/flat_cross_product_downstream.png\n  :alt: Flat Cross Product All-vs-All Result\n  :width: 500\n\n----\n\n.. class:: infomark\n\nThis tool will create new history datasets copied from your input collections but your quota usage will not increase."
    },
    {
        "name": "Nested Cross Product",
        "description": "",
        "category": "Collection Operations",
        "version": "1.0.0",
        "help": "========\nSynopsis\n========\n\n\nThis tool organizes two dataset lists so that Galaxy's normal collection processing produces\nan all-vs-all style analyses of the initial inputs when applied to the outputs of this tool.\n\nWhile a description of what it does standalone is technical and math heavy, how\nit works within an ad-hoc analysis or workflow can be quite straight forward and hopefully is easier\nto understand. For this reason, the next section describes how to use this tool in context and\nthe technical details follow after that. Hopefully, the \"how it works\" details aren't nessecary to\nunderstand the \"how to use it\" details of this tool - at least for simple things.\n\n\n\n====================\nHow to use this tool\n====================\n\n\n\nThis tool can be used in and out of workflows, but workflows will be used to illustrate the ordering of\ntools and connections between them. Imagine a tool that compares two individual datasets and how\nthat might be connected to list inputs in a workflow. This simiple case is shown below:\n\n.. image:: ${static_path}/images/tools/collection_ops/dot_product.png\n  :alt: The Dot Product of Two Collections\n  :width: 500\n\nIn this configuration - the two datasets will be matched and compared element-wise. So the first dataset\nof \"Input List 1\" will be compared to the first dataset in \"Input List 2\" and the resulting\ndataset will be the first dataset in the output list generated using this comparison tool. In this configuration\nthe lists need to have the same number of elements and ideally matching element identifiers.\n\nThis matching up of elements is a very natural way to \"map\" an operation (or in Galaxy parlance, a tool)\nover two lists. However, sometimes the desire is to compare each element of the first list to each element of the\nsecond list. This tool enables that.\n\n\n\nRunning input lists through this tool produces new list structures (described in detail below) that when using\nthe same natural element-wise matching \"map over\" semantics described above produce every combination of the\nelements of the two lists compared against each other. Running a tool with these two outputs instead of the inital\ntwo input produces a nested list structure where the jth element of the inner list of the ith element of the outer\nlist is a comparison of the ith element of the first list to the jth element of the second list. \nPut more simply, the result is a nested list where the identifiers of an element describe which inputs were\nmatched to produce the comparison output found at that element. \n\n.. image:: ${static_path}/images/tools/collection_ops/nested_crossproduct_output.png\n  :alt: The Cartesian Product of Two Collections\n  :width: 500\n\n============================================\nWhat this tool does (technical details)\n============================================\n\nThis tool consumes two flat lists. We will call the input collections ``input_a`` and ``input_b``. If ``input_a``\nhas length ``n`` and dataset elements identified as ``a1``, ``a2``, ... ``an`` and ``input_b``\nhas length ``m`` and dataset elements identified as ``b1``, ``b2``, ... ``bm``, then this tool\nproduces a pair of output nested lists (specifically of the ``list:list`` collection type) where\nthe outer list is of length ``n`` and each inner list has a length of ``m`` (a ``n X m`` nested list). The jth element\ninside the outer list's ith element is a pseudo copy of the ith dataset of ``inputa``. One\nway to think about the output nested lists is as matrices. Here is a diagram of the first output\nshowing the element identifiers of the outer and inner lists along with the what dataset is being\n\"copied\" into this new collection.\n\n.. image:: ${static_path}/images/tools/collection_ops/nested_cross_product_out_1.png\n  :alt: Nested Cross Product First Output\n  :width: 500\n\nThe second output is a nested list of pseudo copies of the elements of ``input_b`` instead of \n``input_a``. In particular the outer list is again of length ``n`` and each inner list is again\nof lenth ``m`` but this time the jth element inside the outer list's ith element is a pseudo copy\nof the jth dataset of ``inputb``. Here is the matrix of these outputs.\n\n.. image:: ${static_path}/images/tools/collection_ops/nested_cross_product_out_2.png\n  :alt: Nested Cross Product Second Output\n  :width: 500\n\nThese nested list structures might appear to be a little odd, but they have the very useful property\nthat if you match up corresponding elements of the nested lists the result is each combination of\nelements in ``input_a`` and ``input_b`` are matched up once. The following diagram describes these matching\ndatasets.\n\n.. image:: ${static_path}/images/tools/collection_ops/nested_cross_product_matching.png\n  :alt: Matching Inputs\n  :width: 500\n\nRunning a tool that compares two datasets with these two nested lists produces a new nested list\nas described above. The following diagram shows the structure of this output and how the element\nidentifiers are preserved and indicate what comparison was performed.\n\n.. image:: ${static_path}/images/tools/collection_ops/nested_cross_product_output.png\n  :alt: Matching Inputs\n  :width: 500\n\n----\n\n.. class:: infomark\n\nThis tool will create new history datasets copied from your input collections but your quota usage will not increase."
    },
    {
        "name": "Apply rules",
        "description": "",
        "category": "Collection Operations",
        "version": "1.1.0",
        "help": "========\nSynopsis\n========\n\nThis tool allows one to process an existing Galaxy dataset collection's metadata as tabular data,\napply a series of rules to it, and generate a new collection. \n\n===========\nDescription\n===========\n\nWhen used interactively in the tool form, a dynamic preview of the processing will be available in a tabular data viewer but this tool\nmay be used in workflows as well where no such preview can be generated.\n\nThis tool is an advanced feature but has a lot of flexibility - it can be used to process collections with arbitrary nesting and can do many kinds of filtering, re-sorting, nesting, flattening, and arbitrary combinations thereof not possible with Galaxy's other, more simple\ncollection operation tools.\n\nMore information about the rule processor in general can be found at `our training site`_. \n\n.. _our training site: https://training.galaxyproject.org/training-material/search?query=rule+builder\n\n\n\n.. class:: infomark\n\nThis tool will create new history datasets from your collection but your quota usage will not increase."
    },
    {
        "name": "Tag elements",
        "description": "",
        "category": "Collection Operations",
        "version": "1.0.0",
        "help": "========\nSynopsis\n========\n\nAdds tags (including name: and group: tags) to collection elements.\n\n===========\nDescription\n===========\n\nThe relationship between element names and tags is specified in a two column tab-delimited file. This file may contain less entries than elements in the collection. In that case only matching list identifiers will be tagged.\n\nTo create name: or group: tags prepend them with ``#`` (you can also use ``name:``) or ``group:``, respectively.\n\n===============\nMore about tags\n===============\n\nGalaxy allows tagging datasets to facilitate analyses. There are several types of tags including simple tags, name tags, and group tags. **Simple** tags allow you to attach an alternative label to a dataset, which will make it easier to find it later. **Name** tags allow you to track propagation of a dataset through the analyses: all datasets derived from the initial dataset labeled with a name tag will inherit it. Finally, **group** tags allow you to label group of datasets. This is useful. for example, for differential expression analysis where you can have two groups of datasets labeled as \"treatment\" and \"control\".\n\nTo learn mote about tags go to `our training site`_.\n\n.. _our training site: https://training.galaxyproject.org/training-material/search?query=tags"
    },
    {
        "name": "Build list",
        "description": "",
        "category": "Collection Operations",
        "version": "1.2.0",
        "help": "========\nSynopsis\n========\n\nBuilds a new list collection from individual datasets or collections.\n\n===========\nDescription\n===========\n\nThis tool combines individual datasets or collections into a new collection. The simplest scenario is building a new colection from individual datasets (case **A** in the image below). You can merge a collection with individual dataset(s). In this case (see **B** in the image below) the individual dataset(s) will be merged with each element of the input collection to create a nested collection. Finally, two or more collection can be merged together creating a nested collection (case **C** in the image below).\n\n.. class:: warningmark\n\n**Note**: When merging collections (e.g., case **C** below) the input collection **must** have equal number of elements.\n\n------\n\n.. image:: ${static_path}/images/tools/collection_ops/build_list.svg\n  :width: 800\n  :alt: Unzipping operation\n\n-------\n\n.. class:: infomark\n\n\nThis tool will create a new collection from your history datasets but your quota usage will not increase."
    },
    {
        "name": "Extract dataset",
        "description": "",
        "category": "Collection Operations",
        "version": "1.0.1",
        "help": "========\nSynopsis\n========\n\nExtracts datasets from a collection based on either position or identifier.\n\n===========\nDescription\n===========\n\nThe tool allow extracting datasets based on position (**The first dataset** and **Select by index** options) or name (**Select by element identifier** option). This tool effectively collapses the inner-most collection into a dataset. For nested collections (e.g a list of lists of lists: outer:middle:inner, extracting the inner dataset element) a new list is created where the selected element takes the position of the inner-most collection (so outer:middle, where middle is not a collection but the inner dataset element).\n\n.. class:: warningmark\n\n**Note**: Dataset index (numbering) begins with 0 (zero).\n\n.. class:: infomark\n\nThis tool will create new history datasets from your collection but your quota usage will not increase."
    },
    {
        "name": "Duplicate file to collection",
        "description": "",
        "category": "Collection Operations",
        "version": "1.0.0",
        "help": "========\nSynopsis\n========\n\nCreates a collection of arbitrary size by duplicating an input dataset N times, where N is a user-specified integer.\n\n===========\nDescription\n===========\n\nThis tool allows creation of a dataset collection of arbitrary size. It takes an input dataset and an integer parameter, which specifies the number of times to duplicate the dataset in the output collection. In addition, the user can specify the base name for the element identifier to use in the output. For example, if `Number` is specified as 3 and `Element identifier` as 'Element', the output collection will contain three identical datasets, with the identifiers `Element 1`, `Element 2` and `Element 3`.\n\n.. class:: infomark\n\nThis tool will create new history datasets but your quota usage will not increase."
    },
    {
        "name": "Collapse Collection",
        "description": "into single dataset in order of the collection",
        "category": "Collection Operations",
        "version": "5.1.0",
        "help": "Combines a list collection into a single file dataset with option to include dataset names or merge common header line."
    },
    {
        "name": "Split file",
        "description": "according to the values of a column",
        "category": "Collection Operations",
        "version": "0.2",
        "help": "**What it does**\n\nThis tool splits a file into different smaller files using a specific column.\nIt will work like the group tool, but every group is saved to its own file.\n\n-----\n\n**Example**\n\nSplitting on column 5 from this::\n\n    chr7  56632  56652  cluster 1\n    chr7  56736  56756  cluster 1\n    chr7  56761  56781  cluster 2\n    chr7  56772  56792  cluster 2\n    chr7  56775  56795  cluster 2\n\nwill produce 2 files with different clusters::\n\n    chr7  56632  56652  cluster 1\n    chr7  56736  56756  cluster 1\n\n\n    chr7  56761  56781  cluster 2\n    chr7  56772  56792  cluster 2\n    chr7  56775  56795  cluster 2"
    },
    {
        "name": "Unzip",
        "description": "Unzip a file",
        "category": "Collection Operations",
        "version": "6.0+galaxy0",
        "help": "**What it does**\n          \n        Unzip a folder containing file(s) of various types. If multiple files are asked to be retained, the output will be a collection containing all files within the zip or tar archive."
    },
    {
        "name": "Pick parameter value",
        "description": "",
        "category": "Collection Operations",
        "version": "0.1.0",
        "help": "A \"null\" value is an empty value, it means some input wasn't specified or some tool produced\nan empty value or was skipped entirely. This can be thought of as an NA in Excel or a None value\nin Python. This expression tool can be fed multiple values - from parameter inputs or the\noutputs or other tools or workflows - and pick one. This allows building workflows which can\nskip steps and branch in complex ways.\n\nThere are subtle but important distinctions between the selection style - though they all\nessentially pick the first value that is non-null (i.e. the first input with a real value).\n\n- \"Pick first value\" is the default and will just return its own null value if none of the inputs\nare non-null.\n- \"Pick first value (or provide default)\" allows picking a default value that will be used if none of the\ninputs are non-null.\n- \"Pick first value (or fail)\" will cause the tool execution to fail if none of the selections are\nnon-null. This can be important because it provides a clearer indication that something went\nwrong and can prevent the further evaluation of a workflow.\n- \"Pick only value\" is like \"First value (or fail)\" in that it will cause an error if none of the\ninputs are non-null but it will go further to ensure that exactly one value is non-null. When\nbuilding conditonal paths through a workflow this can ensure that at most one path is\ntaken."
    },
    {
        "name": "Split file",
        "description": "to dataset collection",
        "category": "Collection Operations",
        "version": "0.5.1",
        "help": "**Split file into a dataset collection**\n\nThis tool splits a data set consisting of records into multiple data sets within a collection.\nA record can be for instance simply a line, a FASTA sequence (header + sequence), a FASTQ sequence\n(headers + sequence + qualities), etc. The important property is that the records either have a \nspecific length (e.g. 4 lines for FASTQ) or that the beginning/end of a new record\ncan be specified by a regular expression, e.g. \".*\" for lines or \">.*\" for FASTA.\nThe tool has presets for text, tabular data sets (which are split after each line), FASTA (new records start with \">.*\"), FASTQ (records consist of 4 lines), SDF (records start with \"^BEGIN IONS\") and MGF (records end with \"^$$$$\").\nFor other data types the text delimiting records or the number of lines making up a record can be specified manually using the generic splitter. \nIf the generic splitter is used, an option is also available to split records either before or after the\nseparator. If a preset filetype is used, this is selected automatically (after for SDF, before for all\nothers).\n\nIf splitting by line (or by some other item, like a FASTA entry or an MGF record), the splitting can be either done alternatingly, in original record order, or at random.\n\nIf t records are to be distributed to n new data sets, then the i-th record goes to data set\n\n* floor(i / t * n) (for batch),\n* i % n (for alternating), or\n* a random data set\n\nFor instance, t=5 records are distributed as follows on n=2 data sets\n\n= === === ====\ni bat alt rand\n= === === ====\n0 0   0   0\n1 0   1   1\n2 0   0   1\n3 1   1   0\n4 1   0   0\n= === === ====\n\nIf the five records are distributed on n=3 data sets:\n\n= === === ====\ni bat alt rand\n= === === ====\n0 0   0   0\n1 0   1   1\n2 1   2   2\n3 1   0   0\n4 2   1   1\n= === === ====\n\nNote that there are no guarantees when splitting at random that every result file will be non-empty, so downstream tools should be able to gracefully handle empty files.\n\nIf a tabular file is used as input, you may choose to split by line or by column. If split by column, a new file is created for each unique value in the column.\nIn addition, (Python) regular expressions may be used to transform the value in the column to a new value. Caution should be used with this feature, as it could transform all values to the same value, or other unexpected behavior.\nThe default regular expression uses each value in the column without modifying it.\n\nTwo modes are available for the tool. For the main mode, the number of output files is selected. In this case, records are shared out between this number of files. Alternatively, 'chunking mode' can be selected, which puts a fixed number of records (the 'chunk size') into each output file."
    },
    {
        "name": "Split file",
        "description": "to dataset collection",
        "category": "Collection Operations",
        "version": "0.5.2",
        "help": "**Split file into a dataset collection**\n\nThis tool splits a data set consisting of records into multiple data sets within a collection.\nA record can be for instance simply a line, a FASTA sequence (header + sequence), a FASTQ sequence\n(headers + sequence + qualities), etc. The important property is that the records either have a \nspecific length (e.g. 4 lines for FASTQ) or that the beginning/end of a new record\ncan be specified by a regular expression, e.g. \".*\" for lines or \">.*\" for FASTA.\nThe tool has presets for text, tabular data sets (which are split after each line), FASTA (new records start with \">.*\"), FASTQ (records consist of 4 lines), SDF (records start with \"^BEGIN IONS\") and MGF (records end with \"^$$$$\").\nFor other data types the text delimiting records or the number of lines making up a record can be specified manually using the generic splitter. \nIf the generic splitter is used, an option is also available to split records either before or after the\nseparator. If a preset filetype is used, this is selected automatically (after for SDF, before for all\nothers).\n\nIf splitting by line (or by some other item, like a FASTA entry or an MGF record), the splitting can be either done alternatingly, in original record order, or at random.\n\nIf t records are to be distributed to n new data sets, then the i-th record goes to data set\n\n* floor(i / t * n) (for batch),\n* i % n (for alternating), or\n* a random data set\n\nFor instance, t=5 records are distributed as follows on n=2 data sets\n\n= === === ====\ni bat alt rand\n= === === ====\n0 0   0   0\n1 0   1   1\n2 0   0   1\n3 1   1   0\n4 1   0   0\n= === === ====\n\nIf the five records are distributed on n=3 data sets:\n\n= === === ====\ni bat alt rand\n= === === ====\n0 0   0   0\n1 0   1   1\n2 1   2   2\n3 1   0   0\n4 2   1   1\n= === === ====\n\nNote that there are no guarantees when splitting at random that every result file will be non-empty, so downstream tools should be able to gracefully handle empty files.\n\nIf a tabular file is used as input, you may choose to split by line or by column. If split by column, a new file is created for each unique value in the column.\nIn addition, (Python) regular expressions may be used to transform the value in the column to a new value. Caution should be used with this feature, as it could transform all values to the same value, or other unexpected behavior.\nThe default regular expression uses each value in the column without modifying it.\n\nTwo modes are available for the tool. For the main mode, the number of output files is selected. In this case, records are shared out between this number of files. Alternatively, 'chunking mode' can be selected, which puts a fixed number of records (the 'chunk size') into each output file."
    },
    {
        "name": "Pick parameter value",
        "description": "",
        "category": "Collection Operations",
        "version": "0.2.0",
        "help": "A \"null\" value is an empty value, it means some input wasn't specified or some tool produced\nan empty value or was skipped entirely. This can be thought of as an NA in Excel or a None value\nin Python. This expression tool can be fed multiple values - from parameter inputs or the\noutputs or other tools or workflows - and pick one. This allows building workflows which can\nskip steps and branch in complex ways.\n\nThere are subtle but important distinctions between the selection style - though they all\nessentially pick the first value that is non-null (i.e. the first input with a real value).\n\n- \"Pick first value\" is the default and will just return its own null value if none of the inputs\nare non-null.\n- \"Pick first value (or provide default)\" allows picking a default value that will be used if none of the\ninputs are non-null.\n- \"Pick first value (or fail)\" will cause the tool execution to fail if none of the selections are\nnon-null. This can be important because it provides a clearer indication that something went\nwrong and can prevent the further evaluation of a workflow.\n- \"Pick only value\" is like \"First value (or fail)\" in that it will cause an error if none of the\ninputs are non-null but it will go further to ensure that exactly one value is non-null. When\nbuilding conditonal paths through a workflow this can ensure that at most one path is\ntaken."
    },
    {
        "name": "Column Join",
        "description": "on Collections",
        "category": "Collection Operations",
        "version": "0.0.1",
        "help": "Joins lists of tabular datasets together on a field.\n\n-----\n\n**Example**\n\nTo join three files, with headers, based on the first column:\n\n**First file (in_1.tabular)**::\n\n    #KEY    c2  c3  c4\n    one     1-1 1-2 1-3\n    two     1-4 1-5 1-6\n    three   1-7 1-8 1-9\n\n\n**Second File (in_2.tabular)**::\n\n    #KEY    c2  c3  c4\n    one     2-1 2-2 2-3\n    two     2-4 2-5 2-6\n    three   2-7 2-8 2-9\n\n**Third file (in_3.tabular)**::\n\n    #KEY    c2  c3  c4\n    one     3-3 3-2 3-3\n    two     3-4 3-5 3-6\n    three   3-7 3-8 3-9\n\n\n**Joining** the files, using **identifier column of 1** and a **header lines of 1**, will return::\n\n    #KEY    in_1.tabular_c2 in_1.tabular_c3 in_1.tabular_c4 in_2.tabular_c2 in_2.tabular_c3 in_2.tabular_c4 in_3.tabular_c2 in_3.tabular_c3 in_3.tabular_c4\n    one     1-1              1-2            1-3             2-1              2-2             2-3             3-3             3-2             3-3\n    three   1-7              1-8            1-9             2-7              2-8             2-9             3-7             3-8             3-9\n    two     1-4              1-5            1-6             2-4              2-5             2-6             3-4             3-5             3-6"
    },
    {
        "name": "Collapse Collection",
        "description": "into single dataset in order of the collection",
        "category": "Collection Operations",
        "version": "4.0",
        "help": "Combines a list collection into a single file dataset with option to include dataset names or merge common header line."
    },
    {
        "name": "Column Join",
        "description": "on Collections",
        "category": "Collection Operations",
        "version": "0.0.2",
        "help": "Joins lists of tabular datasets together on a field.\n\n-----\n\n**Example**\n\nTo join three files, with headers, based on the first column:\n\n**First file (in_1.tabular)**::\n\n    #KEY    c2  c3  c4\n    one     1-1 1-2 1-3\n    two     1-4 1-5 1-6\n    three   1-7 1-8 1-9\n\n\n**Second File (in_2.tabular)**::\n\n    #KEY    c2  c3  c4\n    one     2-1 2-2 2-3\n    two     2-4 2-5 2-6\n    three   2-7 2-8 2-9\n\n**Third file (in_3.tabular)**::\n\n    #KEY    c2  c3  c4\n    one     3-3 3-2 3-3\n    two     3-4 3-5 3-6\n    three   3-7 3-8 3-9\n\n\n**Joining** the files, using **identifier column of 1** and a **header lines of 1**, will return::\n\n    #KEY    in_1.tabular_c2 in_1.tabular_c3 in_1.tabular_c4 in_2.tabular_c2 in_2.tabular_c3 in_2.tabular_c4 in_3.tabular_c2 in_3.tabular_c3 in_3.tabular_c4\n    one     1-1              1-2            1-3             2-1              2-2             2-3             3-3             3-2             3-3\n    three   1-7              1-8            1-9             2-7              2-8             2-9             3-7             3-8             3-9\n    two     1-4              1-5            1-6             2-4              2-5             2-6             3-4             3-5             3-6"
    },
    {
        "name": "Collapse Collection",
        "description": "Collapse collection into single dataset in order of the collection",
        "category": "Collection Operations",
        "version": "1.0",
        "help": "Single tool that combines a list collection into a single file"
    },
    {
        "name": "Split file",
        "description": "to dataset collection",
        "category": "Collection Operations",
        "version": "0.1.1",
        "help": "**Split file into a dataset collection**\n\nThis tool can split five types of files into a separate files within a dataset collection: MGF, FASTA, FASTQ, and tabular.\nIf a tabular file is used as input, you may choose to split by line or by column. If split by column, a new file is created for each unique value in the column.\nIn addition, (Python) regular expressions may be used to transform the value in the column to a new value. Caution should be used with this feature, as it could transform all values to the same value, or other unexpected behavior.\nThe default regular expression uses each value in the column without modifying it. \n\nIf splitting by line (or by some other item, like a FASTA entry or an MGF section), the splitting can be either done sequentially or at random. \nNote that there are no guarantees when splitting at random that every result file will be non-empty, so downstream tools should be able to gracefully handle empty files. \n\n**Note**\n\nDue to current limitations with dataset collections, a log file is produced when running this tool. It will usually be empty, but if the tool fails, any errors will be printed to the log file."
    },
    {
        "name": "Extract element identifiers",
        "description": "of a list collection",
        "category": "Collection Operations",
        "version": "0.0.2",
        "help": "This tool takes a list-type collection and produces a text dataset as output, containing the element identifiers of all datasets contained in the collection."
    },
    {
        "name": "Unzip",
        "description": "Unzip file",
        "category": "Collection Operations",
        "version": "0.2",
        "help": "**What it does**\n\n  Unzip folder to collection."
    },
    {
        "name": "Split file",
        "description": "to dataset collection",
        "category": "Collection Operations",
        "version": "0.2.0",
        "help": "**Split file into a dataset collection**\n\nThis tool splits a data sets consisting of records into multiple data sets within a collection.\nA record can be for instance simply a line, a FASTA sequence (header + sequence), a FASTQ sequence\n(headers + sequence + qualities), etc. The important property is that the begin of a new record\ncan be speciefied by a regular expression, e.g. \".*\" for lines, \">.*\" for FASTA, or \"@.*\" for FASTQ.\nThe tool has presets for text, tabular data sets (which are split by line), FASTA, FASTQ, and MGF.\nFor other data types the text delimiting records can be specified manually using the generic splitter.\n\nIf splitting by line (or by some other item, like a FASTA entry or an MGF record, the splitting can be either done alternating, in original record order, or at random.\n\nIf t records are to be distributed to n new data sets, then the i-th record goes to data set\n\n* floor(i / t * n) (for batch),\n* i % n (for alternating), or\n* a random data set\n\nFor instance, t=5 records are distributed as follows on n=2 data sets\n\n= === === ====\ni bat alt rand\n= === === ====\n0 0   0   0\n1 0   1   1\n2 0   0   1\n3 1   1   0\n4 1   0   0\n= === === ====\n\nIf the five records are distributed on n=3 data sets:\n\n= === === ====\ni bat alt rand\n= === === ====\n0 0   0   0\n1 0   1   1\n2 1   2   2\n3 1   0   0\n4 2   1   1\n= === === ====\n\nNote that there are no guarantees when splitting at random that every result file will be non-empty, so downstream tools should be able to gracefully handle empty files.\n\nIf a tabular file is used as input, you may choose to split by line or by column. If split by column, a new file is created for each unique value in the column.\nIn addition, (Python) regular expressions may be used to transform the value in the column to a new value. Caution should be used with this feature, as it could transform all values to the same value, or other unexpected behavior.\nThe default regular expression uses each value in the column without modifying it."
    },
    {
        "name": "Split file",
        "description": "to dataset collection",
        "category": "Collection Operations",
        "version": "0.3.0",
        "help": "**Split file into a dataset collection**\n\nThis tool splits a data set consisting of records into multiple data sets within a collection.\nA record can be for instance simply a line, a FASTA sequence (header + sequence), a FASTQ sequence\n(headers + sequence + qualities), etc. The important property is that the beginning of a new record\ncan be specified by a regular expression, e.g. \".*\" for lines, \">.*\" for FASTA, or \"@.*\" for FASTQ.\nThe tool has presets for text, tabular data sets (which are split by line), FASTA, FASTQ, SDF and MGF.\nFor other data types the text delimiting records can be specified manually using the generic splitter. \nIf the generic splitter is used, an option is also available to split records either before or after the\nseparator. If a preset filetype is used, this is selected automatically (after for SDF, before for all\nothers).\n\nIf splitting by line (or by some other item, like a FASTA entry or an MGF record), the splitting can be either done alternatingly, in original record order, or at random.\n\nIf t records are to be distributed to n new data sets, then the i-th record goes to data set\n\n* floor(i / t * n) (for batch),\n* i % n (for alternating), or\n* a random data set\n\nFor instance, t=5 records are distributed as follows on n=2 data sets\n\n= === === ====\ni bat alt rand\n= === === ====\n0 0   0   0\n1 0   1   1\n2 0   0   1\n3 1   1   0\n4 1   0   0\n= === === ====\n\nIf the five records are distributed on n=3 data sets:\n\n= === === ====\ni bat alt rand\n= === === ====\n0 0   0   0\n1 0   1   1\n2 1   2   2\n3 1   0   0\n4 2   1   1\n= === === ====\n\nNote that there are no guarantees when splitting at random that every result file will be non-empty, so downstream tools should be able to gracefully handle empty files.\n\nIf a tabular file is used as input, you may choose to split by line or by column. If split by column, a new file is created for each unique value in the column.\nIn addition, (Python) regular expressions may be used to transform the value in the column to a new value. Caution should be used with this feature, as it could transform all values to the same value, or other unexpected behavior.\nThe default regular expression uses each value in the column without modifying it.\n\nTwo modes are available for the tool. For the main mode, the number of output files is selected. In this case, records are shared out between this number of files. Alternatively, 'chunking mode' can be selected, which puts a fixed number of records (the 'chunk size') into each output file."
    },
    {
        "name": "Collapse Collection",
        "description": "into single dataset in order of the collection",
        "category": "Collection Operations",
        "version": "4.2",
        "help": "Combines a list collection into a single file dataset with option to include dataset names or merge common header line."
    },
    {
        "name": "Split file",
        "description": "to dataset collection",
        "category": "Collection Operations",
        "version": "0.4.0",
        "help": "**Split file into a dataset collection**\n\nThis tool splits a data set consisting of records into multiple data sets within a collection.\nA record can be for instance simply a line, a FASTA sequence (header + sequence), a FASTQ sequence\n(headers + sequence + qualities), etc. The important property is that the records either have a \nspecific length (e.g. 4 lines for FASTQ) or that the beginning/end of a new record\ncan be specified by a regular expression, e.g. \".*\" for lines or \">.*\" for FASTA.\nThe tool has presets for text, tabular data sets (which are split after each line), FASTA (new records start with \">.*\"), FASTQ (records consist of 4 lines), SDF (records start with \"^BEGIN IONS\") and MGF (records end with \"^$$$$\").\nFor other data types the text delimiting records or the number of lines making up a record can be specified manually using the generic splitter. \nIf the generic splitter is used, an option is also available to split records either before or after the\nseparator. If a preset filetype is used, this is selected automatically (after for SDF, before for all\nothers).\n\nIf splitting by line (or by some other item, like a FASTA entry or an MGF record), the splitting can be either done alternatingly, in original record order, or at random.\n\nIf t records are to be distributed to n new data sets, then the i-th record goes to data set\n\n* floor(i / t * n) (for batch),\n* i % n (for alternating), or\n* a random data set\n\nFor instance, t=5 records are distributed as follows on n=2 data sets\n\n= === === ====\ni bat alt rand\n= === === ====\n0 0   0   0\n1 0   1   1\n2 0   0   1\n3 1   1   0\n4 1   0   0\n= === === ====\n\nIf the five records are distributed on n=3 data sets:\n\n= === === ====\ni bat alt rand\n= === === ====\n0 0   0   0\n1 0   1   1\n2 1   2   2\n3 1   0   0\n4 2   1   1\n= === === ====\n\nNote that there are no guarantees when splitting at random that every result file will be non-empty, so downstream tools should be able to gracefully handle empty files.\n\nIf a tabular file is used as input, you may choose to split by line or by column. If split by column, a new file is created for each unique value in the column.\nIn addition, (Python) regular expressions may be used to transform the value in the column to a new value. Caution should be used with this feature, as it could transform all values to the same value, or other unexpected behavior.\nThe default regular expression uses each value in the column without modifying it.\n\nTwo modes are available for the tool. For the main mode, the number of output files is selected. In this case, records are shared out between this number of files. Alternatively, 'chunking mode' can be selected, which puts a fixed number of records (the 'chunk size') into each output file."
    },
    {
        "name": "Collapse Collection",
        "description": "into single dataset in order of the collection",
        "category": "Collection Operations",
        "version": "4.1",
        "help": "Combines a list collection into a single file dataset with option to include dataset names or merge common header line."
    },
    {
        "name": "Split file",
        "description": "to dataset collection",
        "category": "Collection Operations",
        "version": "0.5.0",
        "help": "**Split file into a dataset collection**\n\nThis tool splits a data set consisting of records into multiple data sets within a collection.\nA record can be for instance simply a line, a FASTA sequence (header + sequence), a FASTQ sequence\n(headers + sequence + qualities), etc. The important property is that the records either have a \nspecific length (e.g. 4 lines for FASTQ) or that the beginning/end of a new record\ncan be specified by a regular expression, e.g. \".*\" for lines or \">.*\" for FASTA.\nThe tool has presets for text, tabular data sets (which are split after each line), FASTA (new records start with \">.*\"), FASTQ (records consist of 4 lines), SDF (records start with \"^BEGIN IONS\") and MGF (records end with \"^$$$$\").\nFor other data types the text delimiting records or the number of lines making up a record can be specified manually using the generic splitter. \nIf the generic splitter is used, an option is also available to split records either before or after the\nseparator. If a preset filetype is used, this is selected automatically (after for SDF, before for all\nothers).\n\nIf splitting by line (or by some other item, like a FASTA entry or an MGF record), the splitting can be either done alternatingly, in original record order, or at random.\n\nIf t records are to be distributed to n new data sets, then the i-th record goes to data set\n\n* floor(i / t * n) (for batch),\n* i % n (for alternating), or\n* a random data set\n\nFor instance, t=5 records are distributed as follows on n=2 data sets\n\n= === === ====\ni bat alt rand\n= === === ====\n0 0   0   0\n1 0   1   1\n2 0   0   1\n3 1   1   0\n4 1   0   0\n= === === ====\n\nIf the five records are distributed on n=3 data sets:\n\n= === === ====\ni bat alt rand\n= === === ====\n0 0   0   0\n1 0   1   1\n2 1   2   2\n3 1   0   0\n4 2   1   1\n= === === ====\n\nNote that there are no guarantees when splitting at random that every result file will be non-empty, so downstream tools should be able to gracefully handle empty files.\n\nIf a tabular file is used as input, you may choose to split by line or by column. If split by column, a new file is created for each unique value in the column.\nIn addition, (Python) regular expressions may be used to transform the value in the column to a new value. Caution should be used with this feature, as it could transform all values to the same value, or other unexpected behavior.\nThe default regular expression uses each value in the column without modifying it.\n\nTwo modes are available for the tool. For the main mode, the number of output files is selected. In this case, records are shared out between this number of files. Alternatively, 'chunking mode' can be selected, which puts a fixed number of records (the 'chunk size') into each output file."
    },
    {
        "name": "Split file",
        "description": "according to the values of a column",
        "category": "Collection Operations",
        "version": "0.4",
        "help": "**What it does**\n\nThis tool splits a file into different smaller files using a specific column.\nIt will work like the group tool, but every group is saved to its own file.\nYou have the option to include the header (first line) in all splitted files.\nIf you have a header and don't want keep it, please remove it before you use this tool.\nFor example with the \"Remove beginning of a file\" tool.\n\n-----\n\n**Example**\n\nSplitting a file without header on column 5 from this::\n\n    chr7  56632  56652  cluster 1\n    chr7  56736  56756  cluster 1\n    chr7  56761  56781  cluster 2\n    chr7  56772  56792  cluster 2\n    chr7  56775  56795  cluster 2\n\nwill produce 2 files with different clusters::\n\n    chr7  56632  56652  cluster 1\n    chr7  56736  56756  cluster 1\n\n\n    chr7  56761  56781  cluster 2\n    chr7  56772  56792  cluster 2\n    chr7  56775  56795  cluster 2"
    },
    {
        "name": "Compose text parameter value",
        "description": "from parameters",
        "category": "Expression Tools",
        "version": "0.1.0",
        "help": "This tool concatenates each parameter value to a string.\nIf used in a workflow you can connect the output to any\ntext parameter value."
    },
    {
        "name": "Calculate numeric parameter value",
        "description": "from parameters",
        "category": "Expression Tools",
        "version": "0.1.0",
        "help": "This tool calculates an output (integer or float) parameter\nfrom integer and float input parameters and specified simple\narithmetic operations (addition, subtraction, multiplication,\ndivision, exponentiation, and modulus)."
    },
    {
        "name": "Map parameter value",
        "description": "",
        "category": "Expression Tools",
        "version": "0.1.0",
        "help": "**What it does**\n\nMaps a parameter value to another value.\nThis can be used to transform any non-data value (text, integer, float and boolean) to a different value of a different type.\n\n**Settings**\n\n If the value is not found in the mapping the unmodified value is returned by default.\n Select ``Fail if input parameter value not found in mappings`` if you wish the job to fail if an input could not be mapped.\n\n Select ``Provide a default value to use if input parameter value not found in mappings`` to provide a default value to use in case the input parameter value could not be mapped.\n Select the proper input and output parameter types based on your workflow input and output connections.\n\n**Examples**\n\nYou want a user to select from 3 simple options in a workflow, e.g. ``low``, ``medium``, ``high``, which correspond to distinct integer values.\n\nTurn ``Map this parameter value to a different value`` into a a connectable workflow input by clicking on \"Add connection to module\".\n\nSet the input parameter type to ``Text``, and add 3 mappings:\n\n..\n\n  #.\n\n    * Map from this value: ``low``\n    * to this value: ``1``\n\n  #.\n\n    * Map from this value: ``medium``\n    * to this value: ``2``\n\n  #.\n\n    * Map from this value: ``high``\n    * to this value: ``3``\n\nSet ``Select type of parameter to output`` to ``Integer``.\nYou can now connect the output to any connectable Integer input in your workflow."
    },
    {
        "name": "Map parameter value",
        "description": "",
        "category": "Expression Tools",
        "version": "0.1.1",
        "help": "**What it does**\n\nMaps a parameter value to another value.\nThis can be used to transform any non-data value (text, integer, float and boolean) to a different value of a different type.\n\n**Settings**\n\n If the value is not found in the mapping the unmodified value is returned by default.\n Select ``Fail if input parameter value not found in mappings`` if you wish the job to fail if an input could not be mapped.\n\n Select ``Provide a default value to use if input parameter value not found in mappings`` to provide a default value to use in case the input parameter value could not be mapped.\n Select the proper input and output parameter types based on your workflow input and output connections.\n\n**Examples**\n\nYou want a user to select from 3 simple options in a workflow, e.g. ``low``, ``medium``, ``high``, which correspond to distinct integer values.\n\nTurn ``Map this parameter value to a different value`` into a a connectable workflow input by clicking on \"Add connection to module\".\n\nSet the input parameter type to ``Text``, and add 3 mappings:\n\n..\n\n  #.\n\n    * Map from this value: ``low``\n    * to this value: ``1``\n\n  #.\n\n    * Map from this value: ``medium``\n    * to this value: ``2``\n\n  #.\n\n    * Map from this value: ``high``\n    * to this value: ``3``\n\nSet ``Select type of parameter to output`` to ``Integer``.\nYou can now connect the output to any connectable Integer input in your workflow."
    },
    {
        "name": "Extract element identifiers",
        "description": "of a list collection",
        "category": "Collection Operations",
        "version": "0.0.1",
        "help": "This tool takes a list-type collection and produces a text dataset as output, containing the element identifiers of all datasets contained in the collection."
    }
]