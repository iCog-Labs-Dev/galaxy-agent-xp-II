[
  {
    "category": "vgp-assembly-v2",
    "workflow_repository": "assembly-hifi-hic-phasing-vgp4",
    "workflow_files": [
      {
        "workflow_name": "Genome Assembly from Hifi reads with HiC phasing - VGP4",
        "number_of_steps": 69,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
          "toolshed.g2.bx.psu.edu/repos/lparsons/cutadapt/cutadapt/5.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/pick_value/pick_value/0.2.0",
          "toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.1",
          "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy3",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_replace_in_line/9.5+galaxy2",
          "Cut1",
          "__FILTER_EMPTY_DATASETS__",
          "Convert characters1",
          "param_value_from_file",
          "__UNZIP_COLLECTION__",
          "toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/5.1.0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/hifiasm/hifiasm/0.25.0+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/bandage/bandage_image/2022.09+galaxy4",
          "toolshed.g2.bx.psu.edu/repos/bgruening/gfastats/gfastats/1.3.11+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sed_tool/9.5+galaxy2",
          "join1",
          "toolshed.g2.bx.psu.edu/repos/iuc/busco/busco/5.8.0+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/compleasm/compleasm/0.2.6+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/merqury/merqury/1.3+galaxy4",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cut_tool/9.5+galaxy2",
          "__EXTRACT_DATASET__",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_uniq_tool/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/9.5+galaxy2"
        ],
        "file_name": "Assembly-Hifi-HiC-phasing-VGP4.ga"
      }
    ],
    "planemo_tests": [
      "Assembly-Hifi-HiC-phasing-VGP4-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Genome Assembly from Hifi reads with HiC phasing - VGP4\n\nGenerate phased assembly based on PacBio Hifi Reads using HiC data from the same individual for assembly phasing. Part of the VGP workflow suite, it needs to be run after the k-mer profiling workflow VGP1.\n\n## Inputs\n\n1. Hifi long reads [fastq].\n2. Trim Hi-C reads ? If yes, trim 5 bases at the beginning of each reads. Use with some Arima Hi-C data if the contact map looks \"noisy\". \n3. Paired collection of Hi-C reads [fastq].\n4. Genome profile summary generated by Genomescope [txt] generated by VGP1 workflow.\n5. K-mer database [meryldb] generated by VGP1 workflow.\n6. Database to use for Busco lineages. Recommended : latest version.\n7. Lineage. Select the taxonomic lineage of the assembled species.  \n8. Name of first assembly.\n9. Name of second assembly.\n10. Bits for bloom filter. Change for large genomes to save memory.\n11. Homozygous Read Coverage. Optional: specify if the coverage detected by Genomescope in VGP1 in not satisfactory.\n12. Genomescope model parameters [tabular] generated by VGP1 workflow.\n\n## Outputs\n\n1. Haplotype 1 assembly ([fasta] and [gfa])\n2. Haplotype 2 assembly ([fasta] and [gfa])\n3. Trimmed Hi-C reads collection\n4. QC: MultiQC report for HiFi reads trimming\n5. QC: BUSCO report for both assemblies\n6. QC: Compleasm report for both assemblies\n7. QC: Merqury report for both assemblies\n8. QC: Assembly statistics for both assemblies\n9. QC: Nx plot for both assemblies\n10. QC: Size plot for both assemblies\n",
    "has_changelog": true
  },
  {
    "category": "vgp-assembly-v2",
    "workflow_repository": "assembly-hifi-trio-phasing-vgp5",
    "workflow_files": [
      {
        "workflow_name": "Genome Assembly with Pacbio Hifi reads and Trio data for phasing - VGP5",
        "number_of_steps": 64,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
          "toolshed.g2.bx.psu.edu/repos/lparsons/cutadapt/cutadapt/5.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/pick_value/pick_value/0.2.0",
          "toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy3",
          "Cut1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_replace_in_line/9.5+galaxy2",
          "param_value_from_file",
          "Convert characters1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/hifiasm/hifiasm/0.25.0+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/gfastats/gfastats/1.3.11+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/bandage/bandage_image/2022.09+galaxy4",
          "toolshed.g2.bx.psu.edu/repos/iuc/busco/busco/5.8.0+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/compleasm/compleasm/0.2.6+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/merqury/merqury/1.3+galaxy4",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.5+galaxy2",
          "__EXTRACT_DATASET__",
          "join1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cut_tool/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/9.5+galaxy2"
        ],
        "file_name": "Assembly-Hifi-Trio-phasing-VGP5.ga"
      }
    ],
    "planemo_tests": [
      "Assembly-Hifi-Trio-phasing-VGP5-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Genome Assembly with Hifi reads and Trio Data\n\nGenerate phased assembly based on PacBio Hifi Reads using parental Illumina data for phasing. Part of the VGP workflow suite, it needs to be run after the Trio k-mer Profiling workflow VGP2.\n\n## Inputs\n\n1. Hifi long reads [fastq]\n2. Concatenated Illumina reads : Paternal [fastq]\n3. Concatenated Illumina reads : Maternal [fastq]\n4. K-mer database [meryldb] generated by VGP2 workflow.\n5. Paternal hapmer database [meryldb] generated by VGP2 workflow.\n6. Maternal hapmer database [meryldb] generated by VGP2 workflow.\n7. Bits for Bloom Filter. Change for large genomes to save memory.\n8. Database to use for Busco lineages. Recommended : latest version.\n8. Lineage. Select the taxonomic lineage of the assembled species.  \n9.  Homozygous read coverage (Estimated from the Genomescope model if not provided)\n10. Genome model parameters generated by Genomescope [tabular] generated by VGP2 workflow.\n11. Genome profile summary generated by Genomescope [txt] generated by VGP2 workflow.\n12. Name of first haplotype\n13. Name of second haplotype\n\n## Outputs\n\n1. Haplotype 1 assembly [fasta] and [gfa]\n2. Haplotype 2 assembly [fasta] and [gfa]\n9. QC: Size plot for both assemblies\n5. QC: MultiQC report for HiFi reads trimming\n6. QC: BUSCO report for both assemblies\n7. QC: Compleasm report for both assemblies\n8. QC: Merqury report for both assemblies\n9. QC: Assembly statistics for both assemblies\n10. QC: Nx plot for both assemblies\n11. QC: Size plot for both assemblies\n\n",
    "has_changelog": true
  },
  {
    "category": "vgp-assembly-v2",
    "workflow_repository": "assembly-hifi-only-vgp3",
    "workflow_files": [
      {
        "workflow_name": "Genome Assembly from Hifi reads - VGP3",
        "number_of_steps": 55,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
          "toolshed.g2.bx.psu.edu/repos/lparsons/cutadapt/cutadapt/5.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/pick_value/pick_value/0.2.0",
          "toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.1",
          "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy3",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_replace_in_line/9.5+galaxy2",
          "Cut1",
          "Convert characters1",
          "param_value_from_file",
          "toolshed.g2.bx.psu.edu/repos/bgruening/hifiasm/hifiasm/0.25.0+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/bandage/bandage_image/2022.09+galaxy4",
          "toolshed.g2.bx.psu.edu/repos/bgruening/gfastats/gfastats/1.3.11+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/busco/busco/5.8.0+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/compleasm/compleasm/0.2.6+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/merqury/merqury/1.3+galaxy4",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.5+galaxy2",
          "__EXTRACT_DATASET__",
          "join1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cut_tool/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/9.5+galaxy2"
        ],
        "file_name": "Assembly-Hifi-only-VGP3.ga"
      }
    ],
    "planemo_tests": [
      "Assembly-Hifi-only-VGP3-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "## Contiging Solo:\n\nGenerate assembly based on PacBio Hifi Reads.\n\n\n### Inputs\n\n\n1. Hifi long reads [fastq]\n2. K-mer database [meryldb]\n3. Genome profile summary generated by Genomescope [txt]\n4. Homozygous Read Coverage. Optional, use if you think the estimation from Genomescope is inacurate. \n5. Genomescope Model Parameters generated by Genomescope [tabular]\n6. Database for busco lineage (recommended: latest)\n7. Busco lineage (recommended: vertebrata)\n8. Name of first assembly\n9. Name of second assembly\n\n\n### Outputs\n\n1. Primary assembly\n2. Alternate assembly\n3. QC: Bandage image for the raw unitigs\n4. QC: BUSCO report for both assemblies\n5. QC: Compleasm report for both assemblies\n6. QC: Merqury report for both assemblies\n7. QC: Assembly statistics for both assemblies\n8. QC: Nx plot for both assemblies\n9. QC: Size plot for both assemblie\n",
    "has_changelog": true
  },
  {
    "category": "vgp-assembly-v2",
    "workflow_repository": "assembly-decontamination-vgp9",
    "workflow_files": [
      {
        "workflow_name": "Assembly-decontamination-VGP9",
        "number_of_steps": 12,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_dustmasker_wrapper/2.16.0+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sed_tool/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/kraken2/kraken2/2.1.3+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_blastn_wrapper/2.16.0+galaxy0",
          "Cut1",
          "toolshed.g2.bx.psu.edu/repos/iuc/parse_mito_blast/parse_mito_blast/1.0.2+galaxy0",
          "Filter1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cat/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/bgruening/gfastats/gfastats/1.3.11+galaxy0"
        ],
        "file_name": "Assembly-decontamination-VGP9.ga"
      }
    ],
    "planemo_tests": [
      "Assembly-decontamination-VGP9-tests.yml"
    ],
    "has_test_data": false,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "## Decontamination Workflow\n\nDecontamination (foreign contaminants and mitochondrial sequences) of genome assembly after scaffolding step. Part of the VGP Suite. \n\n### Inputs\n\n- Genome Assembly [fasta]\n- Database for Kraken2. Database containing the possible contaminants.\n\n### Ouput\n\n- List of contaminant scaffolds\n- List of mitochondrial scaffolds\n- Decontaminated assembly\n",
    "has_changelog": true
  },
  {
    "category": "vgp-assembly-v2",
    "workflow_repository": "mitogenome-assembly-vgp0",
    "workflow_files": [
      {
        "workflow_name": "Mitogenome Assembly VGP0",
        "number_of_steps": 9,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/mitohifi/mitohifi/3.2.3+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/compress_file/compress_file/0.1.0"
        ],
        "file_name": "Mitogenome-Assembly-VGP0.ga"
      }
    ],
    "planemo_tests": [
      "Mitogenome-Assembly-VGP0-tests.yml"
    ],
    "has_test_data": false,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Assembly of Mitochondrial DNA from PacBio HiFi reads\n\nGenerate mitochondrial assembly based on PacBio HiFi reads. Part of the VGP suite, it can be run at any time independently of the other workflows. This workflow uses MitoHiFi and a mitochondrial reference to assemble the mitochondrial genome from PacBio reads. You do not need to provide the reference yourself, only the Latin name of the species.\n\n\n## Inputs\n\n1. Name of the Species\n2. Name of the Assembly\n3. Hifi long reads [fastq]\n4. Email adress required for NCBI database query \n\n## Outputs\n\n1. Contigs Statistics\n2. Images : \n   1. Mitogenome Coverage\n   2. Mitogenome Annotation\n3. Genbank file of the assembled mitogenome\n4. Fasta file of the assembled mitogenome",
    "has_changelog": true
  },
  {
    "category": "vgp-assembly-v2",
    "workflow_repository": "plot-nx-size",
    "workflow_files": [
      {
        "workflow_name": "Generate Nx and Size plots for multiple assemblies",
        "number_of_steps": 15,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/bgruening/gfastats/gfastats/1.3.11+galaxy0",
          "sort1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/datamash_ops/datamash_ops/1.8+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/devteam/add_value/addValue/1.0.1",
          "param_value_from_file",
          "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
          "toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.1",
          "toolshed.g2.bx.psu.edu/repos/mvdbeek/add_input_name_as_column/addName/0.2.0",
          "toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/5.1.0",
          "Cut1",
          "toolshed.g2.bx.psu.edu/repos/iuc/ggplot2_point/ggplot2_point/3.5.1+galaxy2"
        ],
        "file_name": "Generate-Nx-and-Size-plots-for-multiple-assemblies.ga"
      }
    ],
    "planemo_tests": [
      "Generate-Nx-and-Size-plots-for-multiple-assemblies-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "## Generate Nx and Size plot for multiple assemblies\n\nGenerate Nx and size plots for multiple assemblies to compare the evolution of assembly quality through the scaffolding process. Inputs are the fasta files for each assembly to compare.\n\n### Inputs\n\nCollection of fasta files. The name of each item in the collection will be used as labels for the Nx and Size plots.\n\n### Outputs\n\n\n1. Nx plot \n2. Size plot \n",
    "has_changelog": true
  },
  {
    "category": "vgp-assembly-v2",
    "workflow_repository": "purge-duplicate-contigs-vgp6",
    "workflow_files": [
      {
        "workflow_name": "Purge duplicate contigs from a diploid assembly VGP6",
        "number_of_steps": 61,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
          "toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/purge_dups/purge_dups/1.2.6+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.1",
          "param_value_from_file",
          "Cut1",
          "cat1",
          "toolshed.g2.bx.psu.edu/repos/iuc/busco/busco/5.8.0+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/gfastats/gfastats/1.3.11+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/compleasm/compleasm/0.2.6+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/merqury/merqury/1.3+galaxy4",
          "__EXTRACT_DATASET__",
          "join1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cut_tool/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/9.5+galaxy2"
        ],
        "file_name": "Purge-duplicate-contigs-VGP6.ga"
      }
    ],
    "planemo_tests": [
      "Purge-duplicate-contigs-VGP6-tests.yml"
    ],
    "has_test_data": false,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Purge Duplicate Contigs\n\nPurge contigs marked as duplicates by purge_dups (could be haplotypic duplication or overlap duplication). The contigs are purged from the first assembly (hap1, pri...), added to the second assembly (hp2, alt... ), then the 2nd assembly is purged as well. If you think only one of the assemblies needs purging, use the VGP6b workflow. \nThis workflow is the 6th workflow of the VGP pipeline. It is meant to be run after one of the contigging steps (Workflow 3, 4, or 5)\n\n## Inputs\n\n1. Hifi long reads - trimmed [fastq] (Generated by Cutadapt in the contigging workflow)\n2. Primary Assembly (hap1) [fasta] (Generated by the contigging workflow)\n3. Alternate Assembly (hap2) [fasta] (Generated by the contigging workflow)\n4. K-mer database [meryldb]  (Generated by the k-mer profiling workflow)\n5. Genomescope model parameters [txt] (Generated by the k-mer profiling workflow)\n6. Estimated Genome Size [txt]\n7. Database for busco lineage (recommended: latest) \n8. Lineage of your species for Busco Orthologs (recommended: vertebrata)\n9. Name of first haplotype\n10. Name of second haplotype\n\n\n## Outputs\n\n1. Haplotype 1 purged assembly (Fasta and gfa)\n2. Haplotype 2 purged assembly (Fasta and gfa)\n3. QC: BUSCO report for both assemblies\n4. QC: Compleasm report for both assemblies\n5. QC: Merqury report for both assemblies\n6. QC: Assembly statistics for both assemblies\n7. QC: Nx plot for both assemblies\n8. QC: Size plot for both assemblies\n",
    "has_changelog": true
  },
  {
    "category": "vgp-assembly-v2",
    "workflow_repository": "purge-duplicates-one-haplotype-vgp6b",
    "workflow_files": [
      {
        "workflow_name": "Purging duplicates in one haplotype VGP6b",
        "number_of_steps": 52,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
          "toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.1",
          "toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/purge_dups/purge_dups/1.2.6+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/gfastats/gfastats/1.3.11+galaxy0",
          "param_value_from_file",
          "Cut1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/merqury/merqury/1.3+galaxy4",
          "toolshed.g2.bx.psu.edu/repos/iuc/busco/busco/5.8.0+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/compleasm/compleasm/0.2.6+galaxy2",
          "__EXTRACT_DATASET__",
          "join1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cut_tool/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/9.5+galaxy2"
        ],
        "file_name": "Purging-duplicates-one-haplotype-VGP6b.ga"
      }
    ],
    "planemo_tests": [
      "Purging-duplicates-one-haplotype-VGP6b-tests.yml"
    ],
    "has_test_data": false,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Purge Duplicate Contigs\n\nPurge contigs marked as duplicates by purge_dups in a single haplotype (could be haplotypic duplication or overlap duplication). If you think the purged contigs might belong to the other haplotype, use the workflow VGP6 instead. \nThis workflow is the 6th workflow of the VGP pipeline. It is meant to be run after one of the contigging steps (Workflow 3, 4, or 5).\n\n## Inputs\n\n1. Genomescope model parameters [txt] (Generated by the k-mer profiling workflow)\n2. Hifi long reads - trimmed [fastq] (Generated by Cutadapt in the contigging workflow)\n3. Assembly to purge (e.g. hap1) [fasta] (Generated by the contigging workflow)\n4. K-mer database [meryldb]  (Generated by the k-mer profiling workflow)\n5. Assembly to leave alone (used for merqury statistics) (e.g. hap2) [fasta] (Generated by the contigging workflow)\n6. Estimated Genome Size [txt]\n7. Database for busco lineage (recommended: latest)\n8. Busco lineage (recommended: vertebrata)\n9. Name of un-altered assembly\n10. Name of purged assembly\n\n\n## Outputs\n\n1. Purged assembly (Fasta and gfa)\n2. QC: BUSCO report for the purged assembly\n3. QC: Compleasm report for the purged assembly\n4. QC: Merqury report for both assemblies\n5. QC: Assembly statistics for both assemblies\n6. QC: Nx plot for both assemblies\n7. QC: Size plot for both assemblies\n",
    "has_changelog": true
  },
  {
    "category": "vgp-assembly-v2",
    "workflow_repository": "scaffolding-bionano-vgp7",
    "workflow_files": [
      {
        "workflow_name": "Scaffolding-BioNano-VGP7",
        "number_of_steps": 17,
        "tools_used": [
          "param_value_from_file",
          "toolshed.g2.bx.psu.edu/repos/bgruening/gfastats/gfastats/1.3.6+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/bionano_scaffold/bionano_scaffold/3.7.0+galaxy3",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/9.3+galaxy1",
          "Cut1",
          "toolshed.g2.bx.psu.edu/repos/iuc/ggplot2_point/ggplot2_point/3.4.0+galaxy1"
        ],
        "file_name": "Scaffolding-BioNano-VGP7.ga"
      }
    ],
    "planemo_tests": [
      "Scaffolding-BioNano-VGP7-tests.yml"
    ],
    "has_test_data": false,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Scaffolding with Bionano\n\nScaffolding using Bionano optical map data\n\n## Inputs\n\n1. Bionano data [cmap]\n2. Estimated genome size [txt]\n3. Phased assembly generated by Hifiasm [gfa1]\n\n## Outputs\n\n1. Scaffolds\n2. Non-scaffolded contigs\n3. QC: Assembly statistics\n4. QC: Nx plot\n5. QC: Size plot",
    "has_changelog": true
  },
  {
    "category": "vgp-assembly-v2",
    "workflow_repository": "scaffolding-hic-vgp8",
    "workflow_files": [
      {
        "workflow_name": "Scaffolding with Hi-C data VGP8",
        "number_of_steps": 60,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/gfastats/gfastats/1.3.11+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/map_param_value/map_param_value/0.2.0",
          "param_value_from_file",
          "toolshed.g2.bx.psu.edu/repos/devteam/bamtools_filter/bamFilter/2.5.2+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/samtools_merge/samtools_merge/1.20+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/yahs/yahs/1.2a.2+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/devteam/samtools_stats/samtools_stats/2.0.5",
          "toolshed.g2.bx.psu.edu/repos/iuc/pretext_map/pretext_map/0.1.9+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/pretext_snapshot/pretext_snapshot/0.0.4+galaxy0",
          "__EXTRACT_DATASET__",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_tail_tool/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/bwa_mem2/bwa_mem2/2.2.1+galaxy4",
          "toolshed.g2.bx.psu.edu/repos/iuc/busco/busco/5.8.0+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/compleasm/compleasm/0.2.6+galaxy2",
          "Cut1",
          "toolshed.g2.bx.psu.edu/repos/iuc/ggplot2_point/ggplot2_point/3.5.1+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy3",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_easyjoin_tool/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cut_tool/9.5+galaxy2"
        ],
        "file_name": "Scaffolding-HiC-VGP8.ga"
      }
    ],
    "planemo_tests": [
      "Scaffolding-HiC-VGP8-tests.yml"
    ],
    "has_test_data": false,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Scaffolding with HiC data\n\nThis workflow performs genome assembly scaffolding using HiC data with YAHS. It is designed to be run as part of the VGP analysis trajectories, but can be used on any assembly in GFA format with Hi-C data. To generate a GFA from a fasta assembly, you can use the gfastats tool in Galaxy.  \n\nExample of VGP trajectory : \n- VGP1: Kmer profiling \n- VGP4: Genome assembly with HiC phasing\n- VGP6: Purge duplicated haplotigs\n- VGP8: Scaffolding with HiC\n\n## Inputs\n\n1. Genome assembly [gfa]\n2. Haplotype being scaffolded (Will be added to scaffold names: e.g. `>scaffold_01_H1`)\n3. HiC reads paired collection [fastq]\n5. Trim Hi-C data? If `yes`, trim five bases at the beginning of each read. Use with Arima Hi-C data if the Hi-C map looks \"noisy\" and the reads haven't been trimmed before. \n6. Minimum Mapping Quality [int] (Default:20). Minimum mapping quality for Hi-C alignments. Set to 0 if you want no filtering.  \n6. Database for busco lineage (recommended: latest)\n7. Busco lineage (recommended for VGP data: vertebrata)\n8. Restriction enzyme sequence (recommended for VGP data: Arima Hi-C 2.0)\n9. Estimated genome size [txt] (Output from the contigging workflows 3,4, or 5). A simple text file containing the estimated genome size as an integer. E.g. `2288021`\n\n\n### Outputs\n\n1. Scaffolds in [fasta] and [gfa] format with the haplotype in the scaffold names.\n2. If you selected `yes` for Hi-C trimming, the trimmed collections of Hi-C reads.\n3. QC: Assembly statistics.\n4. QC: Nx plot.\n5. QC: Size plot.\n6. QC: BUSCO report.\n7. QC: Compleasm report.\n8. QC: Pretext Maps before and after scaffolding.\n9. QC: Statistics on Hi-C alignements before and after scaffolding\n",
    "has_changelog": true
  },
  {
    "category": "vgp-assembly-v2",
    "workflow_repository": "hi-c-contact-map-for-assembly-manual-curation",
    "workflow_files": [
      {
        "workflow_name": "PretextMap Generation from 1 or 2 haplotypes",
        "number_of_steps": 45,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/map_param_value/map_param_value/0.2.0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cat/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/pick_value/pick_value/0.2.0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/gfastats/gfastats/1.3.11+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/seqtk/seqtk_telo/1.4+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/pretext_map/pretext_map/0.1.9+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.1",
          "Cut1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_genomecoveragebed/2.31.1",
          "toolshed.g2.bx.psu.edu/repos/iuc/pretext_graph/pretext_graph/0.0.7+galaxy0",
          "Filter1",
          "param_value_from_file",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_replace_in_column/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/pretext_snapshot/pretext_snapshot/0.0.4+galaxy0",
          "__EXTRACT_DATASET__"
        ],
        "file_name": "hi-c-map-for-assembly-manual-curation.ga"
      }
    ],
    "planemo_tests": [
      "hi-c-map-for-assembly-manual-curation-tests.yml"
    ],
    "has_test_data": false,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Hi-C Contact map generation for manual curation of genome assemblies\n\nThis workflow generates Hi-C contact maps for diploid genome assemblies in the Pretext format. It includes tracks for PacBio read coverage, Gaps, and telomeres. The Pretext files can be open in PretextView for the manual curation of genome assemblies. \n\n\n## Inputs\n\n1. **Haplotype 1** [fasta]\n2. **Will you use a second haplotype?** \n3. **Haplotype 2** [fasta]\n4. **Do you want to add suffixes to the scaffold names?** Select yes if the scaffold names in your assembly do not contain haplotype information.\n5. **Haplotype 1 suffix** This suffix will be added to haplotype 1 scaffold names if you selected to add suffixes to the scaffold names.\n6. **Haplotype 2 suffix** This suffix will be added to haplotype 2 scaffold names if you selected to add suffixes to the scaffold names.\n7. **Hi-C reads**  [fastq] Paired Collection containing the Hi-D data\n8. **Do you want to trim the Hi-C data?** If *yes*, remove 5bp at the end of Hi-C reads. Use with Arima Hi-C data if the Hi-C map looks \"noisy\".\n9. **Minimum Mapping Score** Minimum mapping score to keep for Hi-C alignments in the filtered PretextMap. Set to 0 to keep all mapped reads. Default: 20 .\n10. **Telomere repeat to suit species** Expected value of the repeated sequences in the telomeres. Default value [CCCTAA] is suited to vertebrates.\n11. **PacBio reads** [fastq] Collection of PacBio reads.\n\n\n## Outputs\n\n1. Concatenated Assembly [fasta] If two haplotypes are used. \n2. Trimmed Hi-C data (If trimming option is selected) [fastq]\n3. Mapped Hi-C reads [bam]\n4. Telomeres track [bedgraph]\n5. Gap track [bedgraph] \n6. Coverage track [bigwig]\n7. Gaps in coverage track [bedgraph]\n7. Pretext Map without tracks [pretext], filtered and unfiltered.\n8. Pretext Map with tracks [pretext], filtered and unfiltered.\n9. Pretext Snapshot image of the Hi-C contact map [png], filtered and unfiltered.\n",
    "has_changelog": true
  },
  {
    "category": "vgp-assembly-v2",
    "workflow_repository": "kmer-profiling-hifi-vgp1",
    "workflow_files": [
      {
        "workflow_name": "K-mer profiling and reads statistics VGP1",
        "number_of_steps": 19,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
          "toolshed.g2.bx.psu.edu/repos/iuc/pick_value/pick_value/0.2.0",
          "toolshed.g2.bx.psu.edu/repos/iuc/meryl_count_kmers/meryl_count_kmers/1.3+galaxy7",
          "toolshed.g2.bx.psu.edu/repos/iuc/meryl_groups_kmers/meryl_groups_kmers/1.3+galaxy7",
          "toolshed.g2.bx.psu.edu/repos/iuc/meryl_histogram_kmers/meryl_histogram_kmers/1.3+galaxy7",
          "toolshed.g2.bx.psu.edu/repos/iuc/genomescope/genomescope/2.0.1+galaxy0",
          "param_value_from_file",
          "toolshed.g2.bx.psu.edu/repos/richard-burhans/rdeval/rdeval/0.0.7+galaxy4",
          "toolshed.g2.bx.psu.edu/repos/richard-burhans/rdeval/rdeval_report/0.0.7+galaxy4"
        ],
        "file_name": "kmer-profiling-hifi-VGP1.ga"
      }
    ],
    "planemo_tests": [
      "kmer-profiling-hifi-VGP1-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# VGP Workflow #1\n\nThis workflow produces a Meryl database and Genomescope outputs that will be used to determine parameters for following workflows, and assess the quality of genome assemblies. Specifically, it provides information about the genomic complexity, such as the genome size and levels of heterozygosity and repeat content, as well about the data quality. It also provides statistics on the PacBio Hifi reads. \n\n### Inputs\n\n1. The name of the species being assembled\n2. The Name of the assembly\n3. A collection of Hifi long reads in FASTQ format\n4. *k*-mer length\n5. Ploidy\n\n### Outputs\n\n-   Meryl Database of *k*-mer counts\n-   GenomeScope\n    -   Linear plot\n    -   Log plot\n    -   Transformed linear plot\n    -   Transformed log plot\n    -   Summary\n    -   Model\n    -   Model parameteres\n- RDeval for PacBio Hifi Reads QC\n    -   Reads statistics\n    -   HTML report\n  \n\n ![image](https://github.com/galaxyproject/iwc/assets/4291636/565238fc-f8a9-46ac-8b31-6276410fa436)\n",
    "has_changelog": true
  },
  {
    "category": "vgp-assembly-v2",
    "workflow_repository": "kmer-profiling-hifi-trio-vgp2",
    "workflow_files": [
      {
        "workflow_name": "kmer-profiling-hifi-trio-VGP2",
        "number_of_steps": 16,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/pick_value/pick_value/0.2.0",
          "toolshed.g2.bx.psu.edu/repos/iuc/meryl_count_kmers/meryl_count_kmers/1.3+galaxy7",
          "toolshed.g2.bx.psu.edu/repos/iuc/meryl/meryl/1.3+galaxy6",
          "toolshed.g2.bx.psu.edu/repos/iuc/meryl_groups_kmers/meryl_groups_kmers/1.3+galaxy7",
          "toolshed.g2.bx.psu.edu/repos/iuc/genomescope/genomescope/2.0.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/meryl_histogram_kmers/meryl_histogram_kmers/1.3+galaxy7"
        ],
        "file_name": "kmer-profiling-hifi-trio-VGP2.ga"
      }
    ],
    "planemo_tests": [
      "kmer-profiling-hifi-trio-VGP2-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# VGP Workflow #1\n\nThis workflow collects the metrics on the properties of the genome under consideration by analyzing the *k*-mer frequencies. It provides information about the genomic complexity, such as the genome size and levels of heterozygosity and repeat content, as well about the data quality. It uses reads from two parental genomes to partition long reads from the offspring into haplotype-specific *k*-mer databases.\n\n### Inputs\n\n-   Collection of Hifi long reads [fastq] (Collection)\n-   Paternal short-read Illumina sequencing reads [fastq] (Collection)\n-   Maternal short-read Illumina sequencing reads [fastq] (Collection)\n-   *k*-mer length\n-   Ploidy\n\n### Outputs\n\n-   Meryl databases of k-mer counts\n    - Child\n    - Paternal haplotype\n    - Maternal haplotype\n-   GenomeScope metrics for child and the two parental genomes (three GenomeScope profiles in total)\n    -   Linear plot\n    -   Log plot\n    -   Transformed linear plot\n    -   Transformed log plot\n    -   Summary\n    -   Model\n    -   Model parameteres\n \n    ![image](https://github.com/galaxyproject/iwc/assets/4291636/35282f8e-d021-44f6-8e03-7b58b32d6d00)\n",
    "has_changelog": true
  },
  {
    "category": "amplicon",
    "workflow_repository": "amplicon-mgnify",
    "workflow_files": [],
    "planemo_tests": [],
    "has_test_data": false,
    "has_dockstore_yml": false,
    "has_readme": true,
    "readme_content": "# MGnify Amplicon Workflow and Add-on-workflows\n\nThis directory contains the following workflows:\n* **MGnify amplicon workflow v5.0** and its sub-workflows:\n    * **Quality control for single-end reads**\n    * **Quality control for paired-end reads**\n    * **rRNA-prediction**\n    * **ITS**\n    * **Summary tables**\n",
    "has_changelog": false
  },
  {
    "category": "amplicon",
    "workflow_repository": "dada2",
    "workflow_files": [
      {
        "workflow_name": "dada2 amplicon analysis pipeline - for paired end data",
        "number_of_steps": 19,
        "tools_used": [
          "__APPLY_RULES__",
          "toolshed.g2.bx.psu.edu/repos/iuc/dada2_plotqualityprofile/dada2_plotQualityProfile/1.34.0+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/dada2_filterandtrim/dada2_filterAndTrim/1.34.0+galaxy0",
          "__UNZIP_COLLECTION__",
          "toolshed.g2.bx.psu.edu/repos/iuc/dada2_learnerrors/dada2_learnErrors/1.34.0+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/dada2_dada/dada2_dada/1.34.0+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/dada2_mergepairs/dada2_mergePairs/1.34.0+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/dada2_makesequencetable/dada2_makeSequenceTable/1.34.0+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/dada2_removebimeradenovo/dada2_removeBimeraDenovo/1.34.0+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/dada2_seqcounts/dada2_seqCounts/1.34.0+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/dada2_assigntaxonomyaddspecies/dada2_assignTaxonomyAddspecies/1.34.0+galaxy0"
        ],
        "file_name": "dada2_paired.ga"
      }
    ],
    "planemo_tests": [
      "dada2_paired-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Dada2: amplicon analysis for paired end data\n\n## Inputs dataset\n\n- `Paired input data` paired input collection in FASTQ format\n\n## Inputs values\n\n- `Read length forward/reverse reads` length of the forward/reverse reads to which they should be truncated in the filter and trim step\n- `Pool samples` pooling may increase sensitivity\n- `Reference database` that should be used for taxonomic assignment\n\n## Processing\n\nThe workflow follows the steps described in the [dada2 tutorial](https://benjjneb.github.io/dada2/tutorial.html).\n\nAs a first step the input collection is sorted. This is important because the dada2 step outputs\na collection in sorted order. If the input collection would not be sorted then the mergePairs step\nsamples would be mixed up.\n\n- `FilterAndTrim` Quality control by filtering and trimming reads\n- `QualityProfile` is called before and after the FilterAndTrim step\n- `Unzip Collection` separates forward and reverse reads (the next steps are evaluated separately on forward and reverse reads)\n- `learnErrors` learn error rates\n- `dada` filter noisy reads\n- `mergePairs` merge forward and reverse reads\n- `makeSequenceTable` create the sequence table\n- `removeBimeraDenovo` remove chimeric sequencs\n- `assignTaxonomy` assign taxonomic information from a reference data base\n\n## TODO\n\nSome possibilities to extend/improve the workflow\n\n- output BIOM\n- use ASV1, ... in sequence table and taxonomy output, and output additional fasta\n- allow to use custom taxonomy / make it optional\n",
    "has_changelog": true
  },
  {
    "category": "amplicon",
    "workflow_repository": "qiime2",
    "workflow_files": [],
    "planemo_tests": [],
    "has_test_data": false,
    "has_dockstore_yml": false,
    "has_readme": true,
    "readme_content": "# QIIME2 workflows\n\n## Available workflows\n\n1. Import of fastqsanger.gz data into QIIME artifact files. Available for\n   - paired / single end data\n   - demultiplexed / multiplexed data (the latter according to the EMP protocol)\n   For data that is multiplexed with another protocol the cutadapt tool can be used.\n2. Denoising with DADA2\n   - paired / single end data\n",
    "has_changelog": false
  },
  {
    "category": "bacterial_genomics",
    "workflow_repository": "amr_gene_detection",
    "workflow_files": [
      {
        "workflow_name": "amr_gene_detection",
        "number_of_steps": 9,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/nml/staramr/staramr_search/0.10.0+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/amrfinderplus/amrfinderplus/3.12.8+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/abricate/abricate/1.0.1",
          "toolshed.g2.bx.psu.edu/repos/iuc/tooldistillator/tooldistillator/0.9.1+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/tooldistillator_summarize/tooldistillator_summarize/0.9.1+galaxy1"
        ],
        "file_name": "amr_gene_detection.ga"
      }
    ],
    "planemo_tests": [
      "amr_gene_detection-tests.yml"
    ],
    "has_test_data": false,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# AMR gene detection workflow in an assembled bacterial genome (v1.0)\n\nThis workflow uses assembled bacterial genome fasta files (but can be any fasta file) and executes the following steps:\n1. Genomic detection\n    - Antimicrobial resistance gene identification:\n        - **staramr** to blast against ResFinder and PlasmidFinder database\n        - **AMRFinderPlus** to find antimicrobial resistance genes and point mutations \n    - Virulence gene identification:\n        - **ABRicate** with VFDB_A database\n2. Aggregating outputs into a single JSON file\n    - **ToolDistillator** to extract and aggregate information from different tool outputs to JSON parsable files\n\n## Inputs\n\n1. Assembled bacterial genome in fasta format.\n\n## Outputs\n\n1. Genomic detection\n    - Antimicrobial resistance gene identification:\n        - AMR gene list\n        - MLST typing\n        - Plasmid gene identification\n        - Blast hits\n        - AMR gene fasta (assembled nucleotide sequences)\n        - Point mutation list\n    - Virulence gene identification:\n        - Gene identification in tabular format\n2. Aggregating outputs:\n    - JSON file with information about the outputs of **staramr**, **AMRFinderPlus**, **ABRicate**",
    "has_changelog": true
  },
  {
    "category": "bacterial_genomics",
    "workflow_repository": "bacterial_genome_annotation",
    "workflow_files": [
      {
        "workflow_name": "bacterial_genome_annotation",
        "number_of_steps": 10,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/isescan/isescan/1.7.2.3+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/integron_finder/integron_finder/2.0.5+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/plasmidfinder/plasmidfinder/2.1.6+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/bakta/bakta/1.9.4+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/tooldistillator/tooldistillator/0.9.3+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/tooldistillator_summarize/tooldistillator_summarize/0.9.3+galaxy0"
        ],
        "file_name": "bacterial_genome_annotation.ga"
      }
    ],
    "planemo_tests": [
      "bacterial_genome_annotation-tests.yml"
    ],
    "has_test_data": false,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Bacterial genome annotation workflow (v1.1.8)\n\nThis workflow uses assembled bacterial genome fasta files (but can be any fasta file) and executes the following steps:\n1. Genomic annotation\n    - **Bakta** to predict CDS and small proteins (sORF)\n2. Integron identification\n    - **IntegronFinder2** to identify CALIN elements, In0 elements, and complete integrons\n3. Plasmid gene identification\n    - **Plasmidfinder** to identify and typing plasmid sequences\n4. Inserted sequence (IS) detection\n    - **ISEScan** to detect IS elements\n5. Aggregating outputs into a single JSON file\n    - **ToolDistillator** to extract and aggregate information from different tool outputs to JSON parsable files\n\n## Inputs\n\n1. Assembled bacterial genome in fasta format.\n\n## Outputs\n\n1. Genomic annotation:\n    - genome annotation in tabular, gff and several other formats\n    - annotation plot\n    - nucleotide and protein sequences identified\n    - summary of genomic identified elements\n2. Integron identification:\n    - integron identification in tabular format and a summary\n3. Plasmid gene identification:\n    - plasmid gene identified and associated blast hits\n4. Inserted Element (IS) detection:\n    - IS element list in tabular format\n    - is hits in fasta format\n    - ORF hits in protein and nucleotide fasta format\n    - IS annotation gff format\n5. Aggregating outputs:\n    - JSON file with information about the outputs of **Bakta**, **IntegronFinder2**, **Plasmidfinder**, **ISEScan**",
    "has_changelog": true
  },
  {
    "category": "computational-chemistry",
    "workflow_repository": "fragment-based-docking-scoring",
    "workflow_files": [
      {
        "workflow_name": "Fragment-based virtual screening using rDock for docking and SuCOS for pose scoring",
        "number_of_steps": 18,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/bgruening/openbabel_compound_convert/openbabel_compound_convert/3.1.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/ctb_frankenstein_ligand/ctb_frankenstein_ligand/2013.1-0+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/enumerate_charges/enumerate_charges/2020.03.4+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/rxdock_rbcavity/rxdock_rbcavity/2013.1.1_148c5bd1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/split_file_to_collection/split_file_to_collection/0.5.0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/rxdock_rbdock/rxdock_rbdock/2013.1.1_148c5bd1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/4.2",
          "toolshed.g2.bx.psu.edu/repos/bgruening/sucos_docking_scoring/sucos_docking_scoring/2020.03.4+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/rdock_sort_filter/rdock_sort_filter/2013.1-0+galaxy0"
        ],
        "file_name": "fragment-based-docking-scoring.ga"
      }
    ],
    "planemo_tests": [
      "fragment-based-docking-scoring-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Fragment-based virtual screening with docking and pose scoring\n\nDock a compound library against a target protein with rDock and validate the\nposes generated against a reference fragment using SuCOS to compare the feature\noverlap. Poses are filtered by a user-specified SuCOS threshold.\n\nA list of fragments should be specified which will be used to define the cavity\nfor docking, using the 'Frankenstein ligand' technique. For more details, please\nsee https://www.informaticsmatters.com/blog/2018/11/23/cavities-and-frankenstein-molecules.html\n\nCompounds are split into collections and then recombined to allow the workflow\nto be run in a highly parallelized fashion. To specify the level of\nparallelization, use the 'Collection size' parameter.\n",
    "has_changelog": true
  },
  {
    "category": "computational-chemistry",
    "workflow_repository": "gromacs-dctmd",
    "workflow_files": [
      {
        "workflow_name": "dcTMD calculations with GROMACS",
        "number_of_steps": 36,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/bgruening/get_online_data/ctb_online_data_fetch/0.4",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_text_file_with_recurring_lines/1.1.0",
          "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
          "toolshed.g2.bx.psu.edu/repos/chemteam/gmx_solvate/gmx_solvate/2022+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/split_file_to_collection/split_file_to_collection/0.5.0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/add_line_to_file/add_line_to_file/0.1.0",
          "toolshed.g2.bx.psu.edu/repos/chemteam/gmx_em/gmx_em/2022+galaxy0",
          "param_value_from_file",
          "toolshed.g2.bx.psu.edu/repos/chemteam/gmx_makendx/gmx_makendx/2022+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/chemteam/gmx_sim/gmx_sim/2022+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sed_tool/1.1.1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cat/0.1.1",
          "toolshed.g2.bx.psu.edu/repos/chemteam/biomd_neqgamma/biomd_neqgamma/0.1.5.2+galaxy1"
        ],
        "file_name": "gromacs-dctmd.ga"
      }
    ],
    "planemo_tests": [
      "gromacs-dctmd-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# GROMACS dcTMD free energy calculation\n\nPerform an ensemble of targeted MD simulations of a user-specified size using\nthe GROMACS PULL code and calculate dcTMD free energy and friction profiles\nfor the resulting dissocation pathway. Note that pathway separation is not\nperformed by the workflow; the user is responsible for checking the ensemble themselves.\n\nThe input protein (PDB) and ligand (SDF) files provided are parameterized by\nthe 'Protein-ligand complex parameterization' subworkflow.\n\nNote that the workflow uses a MDP file for configuring the TMD simulations; this\nis packaged alongside the workflow as `tmd.mdp`.\n\n## Citations\n* Steffen Wolf and Gerhard Stock (2018), Targeted Molecular Dynamics Calculations of Free Energy Profiles Using a Nonequilibrium Friction Correction, J. Chem. Theory Comput. doi:10.1021/acs.jctc.8b00835\n* Steffen Wolf, Benjamin Lickert, Simon Bray and Gerhard Stock (2020), Multisecond ligand dissociation dynamics from atomistic simulations, Nat. Commun. doi:10.1038/s41467-020-16655-1\n",
    "has_changelog": true
  },
  {
    "category": "computational-chemistry",
    "workflow_repository": "gromacs-mmgbsa",
    "workflow_files": [
      {
        "workflow_name": "MMGBSA calculations with GROMACS",
        "number_of_steps": 28,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_text_file_with_recurring_lines/1.1.0",
          "toolshed.g2.bx.psu.edu/repos/chemteam/gmx_editconf/gmx_editconf/2022+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/split_file_to_collection/split_file_to_collection/0.5.1",
          "param_value_from_file",
          "toolshed.g2.bx.psu.edu/repos/chemteam/gmx_solvate/gmx_solvate/2022+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/chemteam/gmx_em/gmx_em/2022+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/chemteam/parmconv/parmconv/21.10+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/chemteam/gmx_sim/gmx_sim/2022+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/chemteam/md_converter/md_converter/1.9.6+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/chemteam/mmpbsa_mmgbsa/mmpbsa_mmgbsa/21.10+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/1.1.1",
          "toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/5.1.0",
          "Cut1",
          "Summary_Statistics1"
        ],
        "file_name": "gromacs-mmgbsa.ga"
      }
    ],
    "planemo_tests": [
      "gromacs-mmgbsa-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# GROMACS MMGBSA free energy calculation\n\nPerform an ensemble of MD simulations of a user-specified size using GROMACS,\nand calculate MMGBSA free energies using AmberTools. An ensemble average is\ncalculated and returned to the user as the final input.\n\nThe input protein (PDB) and ligand (SDF) files provided are parameterized by\nthe 'Protein-ligand complex parameterization' subworkflow.\n",
    "has_changelog": true
  },
  {
    "category": "computational-chemistry",
    "workflow_repository": "protein-ligand-complex-parameterization",
    "workflow_files": [
      {
        "workflow_name": "Create GRO and TOP complex files",
        "number_of_steps": 14,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/bgruening/openbabel_compound_convert/openbabel_compound_convert/3.1.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/ctb_rdkit_descriptors/ctb_rdkit_descriptors/2020.03.4+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/chemteam/gmx_setup/gmx_setup/2021.3+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/1.1.1",
          "Cut1",
          "param_value_from_file",
          "toolshed.g2.bx.psu.edu/repos/chemteam/ambertools_antechamber/ambertools_antechamber/21.10+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/chemteam/ambertools_acpype/ambertools_acpype/21.10+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/chemteam/gmx_merge_topology_files/gmx_merge_topology_files/3.4.3+galaxy0"
        ],
        "file_name": "protein-ligand-complex-parameterization.ga"
      }
    ],
    "planemo_tests": [
      "protein-ligand-complex-parameterization-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Protein-ligand complex parameterization\n\nParameterizes an input protein (PDB) and ligand (SDF) file prior to molecular\ndynamics simulation with GROMACS.\n\nThis is a simple workflow intended for use as a subworkflow in more complex\nMD workflows. It is used as a subworkflow by the GROMACS MMGBSA and dcTMD\nworkflows. \n",
    "has_changelog": true
  },
  {
    "category": "data-fetching",
    "workflow_repository": "parallel-accession-download",
    "workflow_files": [
      {
        "workflow_name": "Parallel Accession Download",
        "number_of_steps": 5,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/bgruening/split_file_to_collection/split_file_to_collection/0.5.2",
          "toolshed.g2.bx.psu.edu/repos/iuc/sra_tools/fasterq_dump/3.1.1+galaxy0",
          "__APPLY_RULES__"
        ],
        "file_name": "parallel-accession-download.ga"
      }
    ],
    "planemo_tests": [
      "parallel-accession-download-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Parallel Accession Download\n\nDownloads fastq files for sequencing run accessions provided in a text file\nusing fasterq-dump. Creates one job per listed run accession, and is therefore\nmuch faster and more robust to errors when many accessions need to be\ndownloaded.\n",
    "has_changelog": true
  },
  {
    "category": "data-fetching",
    "workflow_repository": "sra-manifest-to-concatenated-fastqs",
    "workflow_files": [
      {
        "workflow_name": "sra_manifest_to_concatenated_fastqs_parallel",
        "number_of_steps": 16,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/map_param_value/map_param_value/0.2.0",
          "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
          "Cut1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/9.3+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/split_file_to_collection/split_file_to_collection/0.5.2",
          "toolshed.g2.bx.psu.edu/repos/iuc/sra_tools/fasterq_dump/3.1.1+galaxy0",
          "__RELABEL_FROM_FILE__",
          "__APPLY_RULES__",
          "toolshed.g2.bx.psu.edu/repos/artbio/concatenate_multiple_datasets/cat_multi_datasets/1.4.3"
        ],
        "file_name": "sra-manifest-to-concatenated-fastqs.ga"
      }
    ],
    "planemo_tests": [
      "sra-manifest-to-concatenated-fastqs-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# SRA manifest to concatenated fastqs\n\nThis workflow takes as input a SRA manifest from SRA Run Selector (or a tabular with a header line), downloads all sequencing run data from the SRA and arranges it into per-sample fastq or pairs of fastq datasets.\n\nIt will work out the relationship between runs and samples from the user-indicated run and sample columns in the input and will concatenate sequencing run data as needed to obtain per-sample datasets.\n\n## Input dataset\n\n- The workflow needs a single tabular input dataset, which is supposed to list SRA run identifiers in one column and sample names in another, and which needs to have a header line.\n- SRA manifests obtained via the SRA Run Selector and turned into tabular format represent valid input.\n\n## Input values\n\n- Column number with SRA run ID\n\n  For manifests obtained through the SRA Run Selector this is column 1\n\n- Column number with sample names\n\n  The number of the column that should be used to assign sequencing runs to samples\n  The names in the column will also serve as the labels of datasets in the output collection.\n  For manifests obtained through the SRA Run Selector suitable columns might be number 6 (BioSample), 16 (Experiment) or 36 (Sample Name).\n\n## Processing\n\n- The workflow downloads sequencing run data in fastq format with fasterqdump (one job per SRA run ID).\n- Run data gets concatenated if it comes from the same sample.\n\n## Outputs\n\n- There are 2 outputs, one with paired-end datasets, one with single-read datasets.\n\n## Limitations\n\n- Special characters in sample names (anything that is not an English alphabet character, digit, underscore, dash, space, dot or comma (`[a-zA-Z0-9_\\- \\.,]`) will be converted to dashes (`-`).\n",
    "has_changelog": true
  },
  {
    "category": "epigenetics",
    "workflow_repository": "atacseq",
    "workflow_files": [
      {
        "workflow_name": "ATAC-seq Analysis: Chromatin Accessibility Profiling",
        "number_of_steps": 31,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/lparsons/cutadapt/cutadapt/4.9+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/devteam/bowtie2/bowtie2/2.5.3+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/devteam/bamtools_filter/bamFilter/2.5.2+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/devteam/samtools_idxstats/samtools_idxstats/2.0.5",
          "toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_MarkDuplicates/3.1.1.0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.3+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_bamtobed/2.31.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/pe_histogram/pe_histogram/1.0.1",
          "toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.20+galaxy3",
          "toolshed.g2.bx.psu.edu/repos/iuc/macs2/macs2_callpeak/2.2.9.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/9.3+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.1",
          "wig_to_bigWig",
          "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_slopbed/2.31.1+galaxy0",
          "param_value_from_file",
          "__APPLY_RULES__",
          "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_mergebed/2.31.1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bigwig_average/deeptools_bigwig_average/3.5.4+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_coveragebed/2.31.1+galaxy0",
          "cat1",
          "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.24.1+galaxy0"
        ],
        "file_name": "atacseq.ga"
      }
    ],
    "planemo_tests": [
      "atacseq-tests.yml"
    ],
    "has_test_data": false,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# ATAC-seq Analysis: Chromatin Accessibility Profiling\n\nThis workflow is highly concordant with the corresponding training material.\nYou can have more information about ATAC-seq analysis in the [slides](https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/atac-seq/slides.html) and the [tutorial](https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/atac-seq/tutorial.html).\n\n## Inputs dataset\n\n- The workflow needs a single input which is a list of dataset pairs of fastqsanger.\n\n## Inputs values\n\n- reference_genome: this field will be adapted to the genomes available for bowtie2 and the genomes available for bedtools slopbed (dbkeys table)\n- effective_genome_size: this is used by macs2 and may be entered manually (indications are provided for heavily used genomes)\n- bin_size: this is used when normalization of coverage is performed. Large values will allow to have smaller output files but with less resolution while small values will increase computation time and size of output files to produce more resolutive bigwigs.\n\n## Processing\n\n- The workflow will remove nextera adapters and low quality bases and filter out any read smaller than 15bp.\n- The filtered reads are mapped with bowtie2 allowing dovetail and fragment length up to 1kb.\n- The BAM is filtered to keep only MAPQ30, concordant pairs and pairs outside of the mitochondria.\n- The PCR duplicates are removed with Picard (only from version 0.8).\n- The BAM is converted to BED to enable macs2 to take both pairs into account.\n- The peaks are called with macs2 which at the same time generates a coverage file.\n- The coverage file is converted to bigwig\n- The amount of reads 500bp from summits and the total number of reads are computed.\n- Two normalizations are computed:\n  - By million reads\n  - By million reads in peaks (500bp from summits)\n- Other QC are performed:\n  - A histogram with fragment length is computed.\n  - The evaluation of percentage of reads to chrM or MT is computed.\n- A multiQC is run to have an overview of the QC.\n\n### Warning\n\n- The `reference_genome` parameter value is used to select references in bowtie2 and bedtools slopbed. Only references that are present in bowtie2 **and** bedtools slopbed are selectable. If your favorite reference genome is not available ask your administrator to make sure that each bowtie2 reference has a corresponding len file for use in bedtools slopbed.\n",
    "has_changelog": true
  },
  {
    "category": "epigenetics",
    "workflow_repository": "average-bigwig-between-replicates",
    "workflow_files": [
      {
        "workflow_name": "BigWig Replicates Averaging Workflow",
        "number_of_steps": 4,
        "tools_used": [
          "__APPLY_RULES__",
          "toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bigwig_average/deeptools_bigwig_average/3.5.4+galaxy0"
        ],
        "file_name": "average-bigwig-between-replicates.ga"
      }
    ],
    "planemo_tests": [
      "average-bigwig-between-replicates-tests.yml"
    ],
    "has_test_data": false,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# BigWig Replicates Averaging Workflow\n\nThis workflow is very useful when you processed multiple samples in collections and you want to generate an average coverage per condition.\n\n## Inputs dataset\n\n- The workflow needs a single input which is a list of bigwigs (normalized). The identifiers of your bigwigs must be like:\n  - whatever_sample1_identificationOfReplicate1\n  - whatever_sample1_identificationOfReplicate2\n  - ...\n  - whatever_sample2_identificationOfReplicate1\n  - whatever_sample2_identificationOfReplicate2\n  - ...\n\n## Inputs values\n\n- bin_size: this is used when average of coverage is performed. Large values will allow to have smaller output files but with less resolution while small values will increase computation time and size of output files to produce more resolutive bigwigs. I suggest 5bp for RNA-seq and 50bp for other applications.\n\n## Processing\n\n- The workflow will split identifiers between everything which is before the last underscore which will be the *sample* and everything which is after the last underscore which will be the *replicate identifier*. And restructure the collection as list:list:\n  - whatever_sample1:\n    - identificationOfReplicate1\n    - identificationOfReplicate2\n    - ...\n  - whatever_sample2:\n    - identificationOfReplicate1\n    - identificationOfReplicate2\n    - ---\n  - ...\n- Then it will average bigwigs into each inner list\n\n## Outputs\n\n- The output is a collection of bigwig datasets like:\n  - whatever_sample1\n  - whatever_sample2\n  - ...\n",
    "has_changelog": true
  },
  {
    "category": "epigenetics",
    "workflow_repository": "chipseq-pe",
    "workflow_files": [
      {
        "workflow_name": "ChIP-seq Analysis: Paired-End Read Processing",
        "number_of_steps": 13,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/lparsons/cutadapt/cutadapt/5.0+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/devteam/bowtie2/bowtie2/2.5.3+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/devteam/samtool_filter2/samtool_filter2/1.8+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/macs2/macs2_callpeak/2.2.9.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/9.5+galaxy0",
          "wig_to_bigWig",
          "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy3"
        ],
        "file_name": "chipseq-pe.ga"
      }
    ],
    "planemo_tests": [
      "chipseq-pe-tests.yml"
    ],
    "has_test_data": false,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# ChIP-seq Analysis: Paired-End Read Processing\n\n## Inputs dataset\n\n- The workflow needs a single input which is a list of dataset pairs of fastqsanger.\n\n## Inputs values\n\n- adapters sequences: this depends on the library preparation. If you don't know, use FastQC to determine if it is Truseq or Nextera.\n- reference_genome: this field will be adapted to the genomes available for bowtie2.\n- effective_genome_size: this is used by MACS2 and may be entered manually (indications are provided for heavily used genomes).\n- normalize_profile: Whether you want to have a profile normalized as Signal to Million Fragments.\n\n## Processing\n\n- The workflow will remove illumina adapters and low quality bases and filter out any pair with mate smaller than 15bp.\n- The filtered reads are mapped with bowtie2 with default parameters.\n- The BAM is filtered to keep only MAPQ30 and concordant pairs.\n- The peaks are called with MACS2 which at the same time generates a coverage file (normalized or not).\n- The coverage is converted to bigwig.\n- A MultiQC is run to have an overview of the QC.\n\n### Warning\n\n- The filtered bam still has PCR duplicates which are removed by MACS2.\n\n## Contribution\n\n@lldelisle wrote the workflow.\n\n@nagoue updated the tools, made it work in usegalaxy.org, fixed the best practices and wrote the tests.\n",
    "has_changelog": true
  },
  {
    "category": "epigenetics",
    "workflow_repository": "chipseq-sr",
    "workflow_files": [
      {
        "workflow_name": "ChIP-seq Analysis: Single-End Read Processing",
        "number_of_steps": 12,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/lparsons/cutadapt/cutadapt/5.0+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/devteam/bowtie2/bowtie2/2.5.3+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/devteam/samtool_filter2/samtool_filter2/1.8+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/macs2/macs2_callpeak/2.2.9.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/9.5+galaxy0",
          "wig_to_bigWig",
          "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy3"
        ],
        "file_name": "chipseq-sr.ga"
      }
    ],
    "planemo_tests": [
      "chipseq-sr-tests.yml"
    ],
    "has_test_data": false,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# ChIP-seq Analysis: Single-End Read Processing\n\n## Inputs dataset\n\n- The workflow needs a single input which is a list of fastqsanger files.\n\n## Inputs values\n\n- adapters sequence_forward: this depends on the library preparation. If you don't know, use FastQC to determine if it is Truseq or Nextera.\n- reference_genome: this field will be adapted to the genomes available for bowtie2.\n- effective_genome_size: this is used by MACS2 and may be entered manually (indications are provided for heavily used genomes).\n- normalize_profile: Whether you want to have a profile normalized as Signal to Million Reads.\n\n## Processing\n\n- The workflow will remove illumina adapters and low quality bases and filter out any read smaller than 15bp.\n- The filtered reads are mapped with bowtie2 with default parameters.\n- The BAM is filtered to keep only MAPQ30.\n- The peaks are called with MACS2 with a fixed extension of 200bp which at the same time generates a coverage file (normalized or not).\n- The coverage is converted to bigwig.\n- A MultiQC is run to have an overview of the QC.\n\n### Warning\n\n- The filtered bam still has PCR duplicates which are removed by MACS2.\n",
    "has_changelog": true
  },
  {
    "category": "epigenetics",
    "workflow_repository": "consensus-peaks",
    "workflow_files": [
      {
        "workflow_name": "Consensus Peak Calling for ATAC-seq and CUT&RUN Replicates",
        "number_of_steps": 28,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_bamtobed/2.31.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.20+galaxy3",
          "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
          "toolshed.g2.bx.psu.edu/repos/iuc/macs2/macs2_callpeak/2.2.9.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/5.1.0",
          "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_multiintersectbed/2.31.1",
          "wig_to_bigWig",
          "toolshed.g2.bx.psu.edu/repos/iuc/table_compute/table_compute/1.2.4+galaxy0",
          "wc_gnu",
          "Filter1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bigwig_average/deeptools_bigwig_average/3.5.4+galaxy0",
          "param_value_from_file",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_text_file_with_recurring_lines/9.3+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/split_file_to_collection/split_file_to_collection/0.5.2",
          "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_intersectbed/2.31.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy0",
          "Cut1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sorted_uniq/9.3+galaxy1"
        ],
        "file_name": "consensus-peaks-atac-cutandrun.ga"
      },
      {
        "workflow_name": "Consensus Peak Calling for ChIP-seq Paired-End Replicates",
        "number_of_steps": 26,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.20+galaxy3",
          "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
          "toolshed.g2.bx.psu.edu/repos/iuc/macs2/macs2_callpeak/2.2.9.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/5.1.0",
          "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_multiintersectbed/2.31.1",
          "wig_to_bigWig",
          "toolshed.g2.bx.psu.edu/repos/iuc/table_compute/table_compute/1.2.4+galaxy0",
          "wc_gnu",
          "Filter1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bigwig_average/deeptools_bigwig_average/3.5.4+galaxy0",
          "param_value_from_file",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_text_file_with_recurring_lines/9.3+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/split_file_to_collection/split_file_to_collection/0.5.2",
          "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_intersectbed/2.31.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy0",
          "Cut1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sorted_uniq/9.3+galaxy1"
        ],
        "file_name": "consensus-peaks-chip-pe.ga"
      },
      {
        "workflow_name": "Consensus Peak Calling for ChIP-seq Single-End Replicates",
        "number_of_steps": 26,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.20+galaxy3",
          "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
          "toolshed.g2.bx.psu.edu/repos/iuc/macs2/macs2_callpeak/2.2.9.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/5.1.0",
          "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_multiintersectbed/2.31.1",
          "wig_to_bigWig",
          "toolshed.g2.bx.psu.edu/repos/iuc/table_compute/table_compute/1.2.4+galaxy0",
          "wc_gnu",
          "Filter1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bigwig_average/deeptools_bigwig_average/3.5.4+galaxy0",
          "param_value_from_file",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_text_file_with_recurring_lines/9.3+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/split_file_to_collection/split_file_to_collection/0.5.2",
          "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_intersectbed/2.31.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy0",
          "Cut1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sorted_uniq/9.3+galaxy1"
        ],
        "file_name": "consensus-peaks-chip-sr.ga"
      }
    ],
    "planemo_tests": [
      "consensus-peaks-atac-cutandrun-tests.yml",
      "consensus-peaks-chip-pe-tests.yml",
      "consensus-peaks-chip-sr-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Consensus Peak Calling for ChIP-seq, ATAC-seq and CUT&RUN Replicates\n\nThe goal of this workflow is to get a list of confident peaks with summits from n replicates.\n\n## Inputs dataset\n\n- The workflow needs a single input which is a list of datasets with n BAM where PCR duplicates have been removed (the workflow also works for nested list if you have multiple conditions each with multiple replicates).\n\n## Inputs values\n\n- Minimum number of overlap: Minimum number of replicates into which the final summit should be present.\n- effective_genome_size: this is used by MACS2 and may be entered manually (indications are provided for heavily used genomes).\n- bin_size: this is the bin sized used to compute the average of normalized profiles. Large values will allow to have a smaller output file but with less resolution while small values will increase computation time and size of the output file to produce a more resolutive bigwig.\n\n## Strategy summary\n\nHere is a generated example to highlight the strategy:\n![strategy](https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/epigenetics/consensus-peaks/strategy.png)\n\n## Processing\n\n- The workflow will:\n  - first part:\n    - call peaks and compute normalized coverage on each BAM individually\n    - average normalized profiles\n    - compute the intersection between all peaks and filter when at least x replicate overlaps\n  - second part:\n    - subset all BAM to get the same number of reads\n    - call peaks on all subsetted BAM combined\n  - finally, keep only peaks from the second part that have summits overlapping the filtered intersection of the first part.\n",
    "has_changelog": true
  },
  {
    "category": "epigenetics",
    "workflow_repository": "cutandrun",
    "workflow_files": [
      {
        "workflow_name": "CUT&RUN/CUT&TAG Analysis: Protein-DNA Interaction Mapping",
        "number_of_steps": 15,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/lparsons/cutadapt/cutadapt/4.9+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/devteam/bowtie2/bowtie2/2.5.3+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/devteam/samtool_filter2/samtool_filter2/1.8+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_MarkDuplicates/3.1.1.0",
          "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_bamtobed/2.31.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/macs2/macs2_callpeak/2.2.9.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/9.3+galaxy1",
          "wig_to_bigWig",
          "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy0"
        ],
        "file_name": "cutandrun.ga"
      }
    ],
    "planemo_tests": [
      "cutandrun-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# CUT&RUN/CUT&TAG Analysis: Protein-DNA Interaction Mapping\n\n## Inputs dataset\n\n- The workflow needs a single input which is a list of dataset pairs of fastqsanger.\n\n## Inputs values\n\n- adapter sequences: this depends on the library preparation. Usually CUT&RUN is Truseq and CUT&TAG is Nextera. If you don't know, use FastQC to determine if it is Truseq or Nextera\n- reference_genome: this field will be adapted to the genomes available for bowtie2\n- effective_genome_size: this is used by macs2 and may be entered manually (indications are provided for heavily used genomes)\n- normalize_profile: Whether you want to have a profile normalized as Signal to Million Reads.\n\n## Processing\n\n- The workflow will remove illumina adapters and low quality bases and filter out any read smaller than 15bp\n- The filtered reads are mapped with bowtie2 allowing dovetail and fragment length up to 1kb\n- The BAM is filtered to keep only MAPQ30 and concordant pairs\n- The PCR duplicates are removed with Picard (only from version 0.6)\n- The BAM is converted to BED to enable macs2 to take both pairs into account\n- The peaks are called with macs2 which at the same time generates a coverage file (normalized or not).\n- The coverage file is converted to bigwig\n- A multiQC is run to have an overview of the QC\n",
    "has_changelog": true
  },
  {
    "category": "epigenetics",
    "workflow_repository": "hic-hicup-cooler",
    "workflow_files": [
      {
        "workflow_name": "Capture Hi-C Processing: FASTQ to Balanced Cool Files",
        "number_of_steps": 17,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
          "Filter1",
          "toolshed.g2.bx.psu.edu/repos/lldelisle/cooler_csort_tabix/cooler_csort_tabix/0.9.3+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/pygenometracks/pygenomeTracks/3.8+galaxy2"
        ],
        "file_name": "chic-fastq-to-cool-hicup-cooler.ga"
      },
      {
        "workflow_name": "Hi-C Processing: FASTQ to Balanced Cool Files",
        "number_of_steps": 12,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/lldelisle/cooler_csort_tabix/cooler_csort_tabix/0.9.3+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/pygenometracks/pygenomeTracks/3.8+galaxy2"
        ],
        "file_name": "hic-fastq-to-cool-hicup-cooler.ga"
      },
      {
        "workflow_name": "Hi-C Data Processing: FASTQ to Valid Interaction Pairs",
        "number_of_steps": 9,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/bgruening/hicup_hicup/hicup_hicup/0.9.2+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/hicup2juicer/hicup2juicer/0.9.2+galaxy0",
          "Filter1"
        ],
        "file_name": "hic-fastq-to-pairs-hicup.ga"
      },
      {
        "workflow_name": "Hi-C Format Conversion: Juicer Medium to Cooler Files",
        "number_of_steps": 7,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/lldelisle/cooler_makebins/cooler_makebins/0.9.3+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/lldelisle/cooler_cload_tabix/cooler_cload_tabix/0.9.3+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/lldelisle/cooler_balance/cooler_balance/0.9.3+galaxy0"
        ],
        "file_name": "hic-juicermediumtabix-to-cool-cooler.ga"
      }
    ],
    "planemo_tests": [
      "chic-fastq-to-cool-hicup-cooler-tests.yml",
      "hic-fastq-to-cool-hicup-cooler-tests.yml",
      "hic-fastq-to-pairs-hicup-tests.yml",
      "hic-juicermediumtabix-to-cool-cooler-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# (Capture) Hi-C Processing: FASTQ to Balanced Cool Files\n\nThis can also be used for Hi-ChIP experiments, in that case the output with `matrix with iced values` is ignored and the matrix to use is `matrix with raw values`.\n\n## Input datasets\n\n- The workflow needs a list of dataset pairs of fastqsanger.\n\n## Input values\n\n- genome name: suggested from the bowtie2 indices, it is used to map and build the list of bins.\n- restriction enzyme: Restriction enzyme used e.g. A^GATCT,BglII. The '^' is used to express where the enzyme cuts.\n- No fill-in: If you used a biotin fill-in protocol, put this to false, else, put it to true.\n- minimum MAPQ: Filtering to apply to pairs you want to keep in your matrix, set it to 0 to not apply filtering (HiCUP already filter for uniquely mapped or MAPQ30).\n- Bin size in bp: Used to generate your first matrix but you will be able to rerun the subworkflow `hic_tabix_to_cool_cooler` to get other resolutions.\n- Interactions to consider to calculate weights in normalization step: this is a parameter for the last correction step (ICE).\n\nFor the region capture workflow:\n\n- chromosome, start and end positions of the capture region\n\nFor the Hi-C workflow:\n\n- region to use in pyGenomeTracks to check the matrices.\n\n## Processing\n\n- Reads are processed with HiCUP which comprises these steps:\n  - Truncation of reads for the religation motif\n  - Mapping of mates independently with bowtie2\n  - Pairing the mates when both mates are uniquely mapped or MAPQ30\n  - Filtering the pairs for undigested, self-ligated...\n  - Removing duplicates\n- The output BAM file is converted to medium juicer format: `<readname> <str1> <chr1> <pos1> <frag1> <str2> <chr2> <pos2> <frag2> <mapq1> <mapq2>` where str = strand (0 for forward, anything else for reverse) and pos is the middle of the fragment.\n- The pairs are filtered for MAPQ if specified.\n- For the region capture Hi-C workflow the pairs are filtered for both mates in the captured region.\n- The filtered pairs are sorted and indexed with cooler_csort.\n- The pairs are loaded into a matrix of the given resolution and balanced with cooler.\n- A final plot is made with pyGenomeTracks using the balanced matrices on the region provided or the capture region.\n\n## Subworkflows\n\nThere are 2 subworkflows: `hic_tabix_to_cool_cooler` and `hic_fastq_to_pairs_hicup.ga`.\n\n### hic_tabix_to_cool_cooler\n\nThis first subworkflow can be used to generate matrices to different resolutions using one of the output of the full workflow (`valid pairs filtered and sorted`).\n\nIf the dataset are still in galaxy (format: juicer_medium_tabix.gz), the workflow can be run directly.\n\nIf the dataset is not anymore in galaxy, you need to upload and specify the datatype as: juicer_medium_tabix.gz\n\n### hic_fastq_to_pairs_hicup\n\nThe second subworkflow has no real reason to be launched by itself except for QC tests.\n\nIf you want to run the first subworkflow from these results:\n\n- You first need to filter the pairs (`valid pairs in juicebox format MAPQ filtered`) for the capture region if relevent using the tool Filter1 (**Filter** data on any column using simple expressions) with the condition `(c3=='chr2' and c4<180000000 and c4>170000000) and (c7==\"chr2\" and c8<180000000 and c8>170000000)` if your capture region is chr2:170000000-180000000.\n- Then you need to run cooler_csort (**cooler csort with tabix** Sort and index a contact list.) with as input the `valid pairs in juicebox format MAPQ filtered` or the output of the previous step and for \"Format of your input file\" use \"Juicer Medium Format\".\n\nThe output of `cooler_csort` can be used as input of the first subworkflow.\n",
    "has_changelog": true
  },
  {
    "category": "genome-assembly",
    "workflow_repository": "assembly-with-flye",
    "workflow_files": [
      {
        "workflow_name": "Genome assembly with Flye",
        "number_of_steps": 5,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/bgruening/flye/flye/2.9.3+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/quast/quast/5.2.0+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/fasta_stats/fasta-stats/2.0",
          "toolshed.g2.bx.psu.edu/repos/iuc/bandage/bandage_image/2022.09+galaxy4"
        ],
        "file_name": "Genome-assembly-with-Flye.ga"
      }
    ],
    "planemo_tests": [
      "Genome-assembly-with-Flye-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Genome assembly with Flye workflow\n\n\n## Why use this workflow?\n\n- This is a fairly simple workflow that assembles a genome from long sequencing reads.\n- It takes in sequencing reads from PacBio (Hifi or non-Hifi), or Oxford Nanopore.\n- If you have PacBio Hifi reads, you may prefer to use a workflow with the assembly tool Hifiasm, such as the those in the suite of VGP workflows. \n\n## Inputs\n\nRaw sequencing reads from PacBio or Oxford Nanopore in format:\nfasta, fasta.gz, fastq, fastq.gz, fastqsanger.gz or fastqsanger\n\n## What does the workflow do\n\n- Assembles the reads with the tool Flye\n- Summarizes the statistics with the tool Fasta statistics\n- Report with the tool Quast\n- Renders the assembly graph with the tool Bandage\n\n## Settings\n\nRun as-is or change parameters at runtime\n\nFor example:\n- change the Flye option of \"mode\" to the correct sequencing type\n- change the Quast option for \"Type of organism\" to correct taxon\n \n## Outputs\n\n- Flye assembly output - four files: fasta, gfa for bandage, graph_dot file, assembly info\n- Fasta statistics\n- Bandage image\n- Quast report\n",
    "has_changelog": true
  },
  {
    "category": "genome-assembly",
    "workflow_repository": "bacterial-genome-assembly",
    "workflow_files": [
      {
        "workflow_name": "Bacterial Genome Assembly using Shovill",
        "number_of_steps": 11,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/shovill/shovill/1.1.0+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/quast/quast/5.3.0+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/checkm2/checkm2/1.0.2+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/nml/refseq_masher/refseq_masher_matches/0.1.2",
          "toolshed.g2.bx.psu.edu/repos/iuc/bandage/bandage_info/2022.09+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/bandage/bandage_image/2022.09+galaxy4",
          "toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/5.1.0",
          "toolshed.g2.bx.psu.edu/repos/iuc/tooldistillator/tooldistillator/0.9.3+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/tooldistillator_summarize/tooldistillator_summarize/0.9.3+galaxy0"
        ],
        "file_name": "bacterial_genome_assembly.ga"
      }
    ],
    "planemo_tests": [
      "bacterial_genome_assembly-tests.yml"
    ],
    "has_test_data": false,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Bacterial genome assembly workflow for paired end data\n\nThis workflow uses paired-end illumina trimmed reads fastq(.gz) files and executes the following steps:\n1. Assembly raw reads to a final contig fasta file \n    - **Shovill**\n2. Quality control of the assembly\n    - **Checkm2** to predict the completeness and contamination\n    - **Quast**\n    - **Bandage** to plot assembly graph\n    - **Refseqmasher** to identify the closed reference genome\n3. Aggregating outputs into a single JSON file\n    - **ToolDistillator** to extract and aggregate information from different tool outputs to JSON parsable files\n\n## Inputs\n\n1. Paired-end illumina trimmed reads in fastq(.gz) format.\n\n## Outputs\n\n1. Assembly:\n    - Assembly with contig in fasta\n    - Mapped read on assembly in bam format\n    - Graph assembly in gfa format\n2. Quality of Assembly:\n    - Expected completeness and contamination report\n    - Assembly report\n    - Assembly Graph\n    - Tabular result of closed reference genome\n3. Aggregating outputs\n    - JSON file with information about the outputs of **Shovill**, **Checkm2**, **Quast**, **Bandage**, **Refseqmasher**",
    "has_changelog": true
  },
  {
    "category": "genome-assembly",
    "workflow_repository": "polish-with-long-reads",
    "workflow_files": [
      {
        "workflow_name": "Assembly polishing with long reads",
        "number_of_steps": 11,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.26+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/bgruening/racon/racon/1.5.0+galaxy1"
        ],
        "file_name": "Assembly-polishing-with-long-reads.ga"
      }
    ],
    "planemo_tests": [
      "Assembly-polishing-with-long-reads-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Assembly polishing with Racon workflow\n\n## Inputs\n\n- Sequencing reads in format: fastq, fastq.gz, fastqsanger.gz or fastqsanger\n- Genome assembly to be polished, in fasta format\n\n## What does the workflow do\n\n- After long reads have been assembled into a genome (contigs), this can be polished with the same long reads. \n- This workflow uses the tool minimap2 to map the long reads back to the assembly, and then uses Racon to make polishes. \n- This is repeated a further 3 times. \n\nIn more detail:\n\n- minimap2 : long reads are mapped to assembly => overlaps.paf.\n- overaps, long reads, assembly => Racon => polished assembly 1\n- using polished assembly 1 as input; repeat minimap2 + racon => polished assembly 2\n- using polished assembly 2 as input, repeat minimap2 + racon => polished assembly 3\n- using polished assembly 3 as input, repeat minimap2 + racon => polished assembly 4\n\n## Settings\n\n- Run as-is or change parameters at runtime.\n- For the input at \"minimap settings for long reads\", enter (map-pb) for PacBio reads, (map-hifi) for PacBio HiFi reads, or (map-ont) for Oxford Nanopore reads.\n\n## Outputs\n\nThere is one output: the polished assembly in fasta format. \n\n",
    "has_changelog": true
  },
  {
    "category": "genome-assembly",
    "workflow_repository": "quality-and-contamination-control",
    "workflow_files": [
      {
        "workflow_name": "Quality and Contamination Control For Genome Assembly",
        "number_of_steps": 12,
        "tools_used": [
          "__ZIP_COLLECTION__",
          "toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.26.0+galaxy0",
          "__UNZIP_COLLECTION__",
          "toolshed.g2.bx.psu.edu/repos/iuc/kraken2/kraken2/2.1.3+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/bracken/est_abundance/3.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/recentrifuge/recentrifuge/1.16.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/tooldistillator/tooldistillator/0.9.3+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/tooldistillator_summarize/tooldistillator_summarize/0.9.3+galaxy0"
        ],
        "file_name": "quality_and_contamination_control.ga"
      }
    ],
    "planemo_tests": [
      "quality_and_contamination_control-tests.yml"
    ],
    "has_test_data": false,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Quality and Contamination control workflow for paired end data\n\nThis workflow uses paired-end illumina fastq(.gz) files and executes the following steps:\n1. Quality control and trimming\n    - **fastp** QC control and trimming\n2. Taxonomic assignation on trimmed data\n    - **Kraken2** assignation\n    - **Bracken** to re-estimate abundance to the species level\n    - **Recentrifuge** to make a krona chart\n3. Aggregating outputs into a single JSON file\n    - **ToolDistillator** to extract and aggregate information from different tool outputs to JSON parsable files\n\n## Inputs\n\n1. Paired-end illumina raw reads in fastq(.gz) format.\n\n## Outputs\n\n1. Quality control:\n    - quality report\n    - trimmed raw reads\n2. Taxonomic assignation:\n    - Tabular report of identified species\n    - Tabular file with assigned read to a taxonomic level\n    - Krona chart to illustrate species diversity of the sample\n3. Aggregating outputs:\n    - JSON file with information about the outputs of **fastp**, **Kraken2**, **Bracken**, **Recentrifuge** \n",
    "has_changelog": true
  },
  {
    "category": "genome_annotation",
    "workflow_repository": "annotation-helixer",
    "workflow_files": [
      {
        "workflow_name": "Genome annotation with Helixer",
        "number_of_steps": 8,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/genouest/helixer/helixer/0.3.3+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/busco/busco/5.8.0+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/devteam/gffread/gffread/2.2.1.4+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/jcvi_gff_stats/jcvi_gff_stats/0.8.4",
          "toolshed.g2.bx.psu.edu/repos/iuc/jbrowse/jbrowse/1.16.11+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/omark/omark/0.3.0+galaxy2"
        ],
        "file_name": "Galaxy-Workflow-annotation_helixer.ga"
      }
    ],
    "planemo_tests": [
      "Galaxy-Workflow-annotation_helixer-tests.yml"
    ],
    "has_test_data": false,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Genome annotation workflow with Helixer\n\nThis workflow allows you to annotate a genome with Helixer and evaluate the quality of the annotation using BUSCO and Genome Annotation statistics. GFFRead is also used to predict protein sequences derived from this annotation, and BUSCO and OMArk are used to assess proteome quality. \n\n\nHelixer is an annotation software with a new and different approach: it performs evidence-free predictions (no need for RNASeq data or sequence aligments), using Graphics Processing Unit (GPU), with a much faster execution time. The annotation is based on the development and use of a cross-species deep learning model. The software is used to configure and train models for ab initio prediction of gene structure. In other words, it identifies the base pairs in a genome that belong to the UTR/CDS/Intron genes.\n\nTo assess the quality of the proteome, we will use the GFFRead tool to extract the predicted protein sequences from the annotation (i.e. the Helixer annotation).\n\nTo assess the quality of the annotation, we will use different tools:\n- Genome Annotation Statistics: is a program designed to analyze and provide statistics on genomic annotations. This software performs its analyses from a GFF3 file.\n- BUSCO (Benchmarking Universal Single-Copy Orthologs):  is a tool allowing to evaluate the quality of a genome assembly or of a genome annotation. By comparing genomes from various more or less related species, the authors determined sets of ortholog genes that are present in single copy in (almost) all the species of a clade (Bacteria, Fungi, Plants, Insects, Mammals, \u2026). Most of these genes are essential for the organism to live, and are expected to be found in any newly sequenced and annotated genome from the corresponding clade. Using this data, BUSCO is able to evaluate the proportion of these essential genes (also named BUSCOs) found in a set of (predicted) transcript or protein sequences. This is a good evaluation of the \u201ccompleteness\u201d of the annotation.\n- OMArk: is proteome quality assessment software. It provides measures of proteome completeness, characterises the consistency of all protein-coding genes with their homologues and identifies the presence of contamination by other species. OMArk is based on the OMA orthology database, from which it exploits orthology relationships, and on the OMAmer software for rapid placement of all proteins in gene families.\n\nThe final step is to view the generated annotation using a genome browser such as JBrowse. This browser allows you to navigate along the chromosomes of the genome and view the structure of each predicted gene.\n\n## Input dataset for Helixer\nHelixer requires the genome sequence to be annotated, in fasta format.\n\n## Output dataset for Helixer\nHelixer produces a single output dataset: a GFF3 file. \n\n\n## Input dataset for Genome Annotation Statistics\nThis software requires a GFF3 file. In this workflow, the output generated is Helixer.\n\n## Output dataset for Genome Annotation Statistics\nTwo output files are generated:\n- a file containing graphs in pdf format\n- a summary in txt format\n\n## Input dataset for GFFRead\nIn this workflow, GFFRead requires two inputs:\n- an annotation file in GFF3 format (the Helixer format)\n- the genome sequence in fasta format\n\n## Output dataset for GFFRead\nIn this workflow, a unique output will be generated. This file, in fasta format, contains the protein sequences predicted from the annotation.\n\n## Input dataset for BUSCO\nBUSCO requires a fasta file.\nBUSCO will be used twice for this workflow. Firstly on the predicted protein sequences and secondly on the genome sequence. \n\n## Output dataset for BUSCO\nWith BUSCO, we can obtain different output files:\n- short summary : statistical summary of the quality of genomic assembly or annotation, including total number of genes evaluated, percentage of complete genes, percentage of partial genes, etc.\n- full table : list of universal orthologs found in the assembled or annotated genome, with information on their completeness, location in the genome, quality score, etc.\n- missing BUSCOs : list of orthologs not found in the genome, which may indicate gaps in assembly or annotation.\n- summary image : graphics and visualizations to visually represent the results of the evaluation, such as bar charts showing the proportion of complete, partial and missing genes.\n- GFF : contain information on gene locations, exons, introns, etc.\n\n## Input dataset for OMArk\nOMAk requires the fasta file produced by GFFRead, containing the predicted protein sequences. \n\n## Output dataset for OMArk\nIn this tutorial, a single output file will be generated: a file detailing the assessment of completeness, consistency and species composition. \n\n## Input dataset for JBrowse\nJBrowse requires two inputs:\n- the genome sequence in fasta format\n- the annotation file in gff3 format, generated by Helixer\n\n## Output dataset for JBrowse\nAn html file is generated for browsing the genome.",
    "has_changelog": true
  },
  {
    "category": "genome_annotation",
    "workflow_repository": "annotation-maker",
    "workflow_files": [
      {
        "workflow_name": "Genome annotation with Maker",
        "number_of_steps": 14,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/fasta_stats/fasta-stats/2.0",
          "toolshed.g2.bx.psu.edu/repos/iuc/busco/busco/5.7.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/maker/maker/2.31.11+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/devteam/gffread/gffread/2.2.1.4+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/jcvi_gff_stats/jcvi_gff_stats/0.8.4",
          "toolshed.g2.bx.psu.edu/repos/iuc/maker_map_ids/maker_map_ids/2.31.11",
          "toolshed.g2.bx.psu.edu/repos/iuc/jbrowse/jbrowse/1.16.11+galaxy1"
        ],
        "file_name": "Genome_annotation_with_maker_short.ga"
      }
    ],
    "planemo_tests": [],
    "has_test_data": false,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Genome annotation workflow with Maker\n\nThis workflow allows for genome annotation using Maker and evaluates the quality of the annotation with BUSCO and genome annotation statistics. The annotation can then be improved, standardized, and visualized with additional tools.\n\n**Maker** is a genome model prediction software that uses ab initio predictors (SANP and Augustus) to improve its predictions. Maker is capable of annotating both prokaryotes and eukaryotes. It works by aligning as much evidence as possible along the genome sequence, then reconciling all these signals to determine likely genetic structures.\n\n## Workflow Steps\n\n- Annotation with Maker: Maker uses the genome sequence, protein evidence, ab-initio predictions, and ESTs to produce the annotation.\n- Quality Evaluation:\n    - Run Fasta Statistics to assess genome assembly quality.\n    - Use BUSCO to evaluate annotation completeness.\n- Annotation Statistics: Analyze the annotation using Genome Annotation Statistics, producing graphical and textual summaries.\n- Sequence Extraction: Extract predicted protein sequences using GFFRead for downstream analysis.\n- Improve Gene Names: Standardize gene names using Map annotation ids for better readability.\n- Visualization: Load the genome sequence and annotation into JBrowse for interactive browsing.\n\n## Input data\nThe following input files are required for the workflow:\n- Genome sequence (FASTA format): The genome to be annotated. Used by Maker, Fasta Statistics, and BUSCO.\n- Protein sequences (FASTA format): Evidence to assist annotation in Maker.\n- EST evidences (FASTA format): Alignments used as evidence by Maker.\n- Ab-initio gene predictions: Supplementary data for Maker to refine annotations.\n\n\n## Output Data\nThe workflow generates the following outputs:\n- Annotation file (GFF3): Contains the final consensus gene models produced by Maker.\n- Genome statistics: A tabular file summarizing contig sizes and base content, produced by Fasta Statistics.\n- BUSCO results: Assess the completeness of the annotation and include:\n    - A summary of results.\n    - A table of all searched BUSCO genes with their status.\n    - A table of missing BUSCO genes.\n- Annotation statistics: Summary and graphical analyses of the annotation, produced by Genome Annotation Statistics.\n- Protein sequences (FASTA): Predicted from the annotation using GFFRead.\n- Renamed GFF annotation file: Contains standardized gene names, produced by Map annotation ids.\n- Genome browser visualization (HTML): An interactive genome view produced by JBrowse.",
    "has_changelog": true
  },
  {
    "category": "genome_annotation",
    "workflow_repository": "functional-annotation",
    "workflow_files": [],
    "planemo_tests": [],
    "has_test_data": false,
    "has_dockstore_yml": false,
    "has_readme": false,
    "readme_content": null,
    "has_changelog": false
  },
  {
    "category": "genome_annotation",
    "workflow_repository": "lncrnas-annotation",
    "workflow_files": [
      {
        "workflow_name": "lncRNAs annotation workflow",
        "number_of_steps": 9,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/map_param_value/map_param_value/0.2.0",
          "toolshed.g2.bx.psu.edu/repos/devteam/gffread/gffread/2.2.1.4+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/feelnc/feelnc/0.2.1+galaxy0",
          "cat1"
        ],
        "file_name": "Galaxy-Workflow-lncRNAs_annotation_workflow.ga"
      }
    ],
    "planemo_tests": [
      "Galaxy-Workflow-lncRNAs_annotation_workflow-tests.yml"
    ],
    "has_test_data": false,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# lncRNAs annotation workflow\n\nThis workflow uses the FEELnc tool to annotate long non-coding RNAs. Before annotating these long non-coding RNAs, StringTie will be used to assemble the RNA-seq alignments into potential trancriptions. The gffread tool provides a genome annotation file in GTF format.\n\nFor future analyses, it would be interesting to use an updated annotation containing messenger RNA and long non-coding RNA. The concatenante tool merges the reference annotation with the long non-coding RNA annotation obtained with FEELnc.\n\nThis workflow is taken from the tutorial \u201cLong non-coding RNAs (lncRNAs) annotation with FEELnc\u201d on the GTN.\n\n## Workflows steps\n- Transcript Assembly with StringTie: RNA-seq alignments are assembled into potential transcripts to provide a comprehensive view of expressed regions.\n- Genome Annotation Conversion with GFFRead: Genome annotations are converted into a standardized format (GTF) to ensure compatibility with downstream tools.\n- lncRNA Annotation with FEELnc: The FEELnc pipeline identifies and classifies long non-coding RNAs (lncRNAs) through three main steps:\n    - Filter: Removes unwanted transcripts and those overlapping reference exons.\n    - Codpot: Evaluates coding potential to differentiate lncRNAs from coding RNAs.\n    - Classifier: Assigns lncRNAs to categories based on their genomic location and transcriptional direction.\n- Annotation Merging with Concatenate: The lncRNA annotation is merged with the reference annotation to create a unified genome annotation containing both mRNAs and lncRNAs.\n\n## Input data\nThe following input files are required for the workflow:\n- RNA-seq alignments (BAM format): Required by StringTie for transcript assembly.\n- Genome annotation (GFF3 format): Used by StringTie and GFFRead for processing.\n- Genome sequence (FASTA format): Required by FEELnc for lncRNA identification.\n- Reference annotation (GTF format): Provided by GFFRead for FEELnc analysis.\n\n## Output data\nThe workflow produces the following outputs:\n- Transcript annotation (GTF format): Generated by StringTie, containing assembled transcripts from RNA-seq data.\n- Converted genome annotation (GTF format): Produced by GFFRead, used as input for FEELnc.\n- lncRNA annotation (GTF format): Generated by FEELnc, containing identified lncRNAs.\n- mRNA annotation (GTF format): Produced by FEELnc for downstream use.\n- lncRNA classification table: Produced by FEELnc, detailing genomic relationships of lncRNAs.\n- Comprehensive genome annotation (GTF format): Generated by Concatenate, combining mRNA and lncRNA annotations.\n",
    "has_changelog": true
  },
  {
    "category": "imaging",
    "workflow_repository": "fluorescence-nuclei-segmentation-and-counting",
    "workflow_files": [
      {
        "workflow_name": "Segmentation and counting of cell nuclei in fluorescence microscopy images",
        "number_of_steps": 8,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/imgteam/2d_simple_filter/ip_filter_standard/1.12.0+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/imgteam/2d_histogram_equalization/ip_histogram_equalization/0.18.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/imgteam/2d_auto_threshold/ip_threshold/0.18.1+galaxy3",
          "toolshed.g2.bx.psu.edu/repos/imgteam/bfconvert/ip_convertimage/6.7.0+galaxy3",
          "toolshed.g2.bx.psu.edu/repos/imgteam/binary2labelimage/ip_binary_to_labelimage/0.5+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/imgteam/overlay_images/ip_overlay_images/0.0.4+galaxy4",
          "toolshed.g2.bx.psu.edu/repos/imgteam/count_objects/ip_count_objects/0.0.5-2"
        ],
        "file_name": "segmentation-and-counting.ga"
      }
    ],
    "planemo_tests": [
      "segmentation-and-counting-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Segmentation and counting of cell nuclei in fluorescence microscopy images\n\nThis workflow performs segmentation and counting of cell nuclei using fluorescence microscopy images. The segmentation step is performed using Otsu thresholding (Otsu, 1979). The workflow is based on the tutorial: https://training.galaxyproject.org/training-material/topics/imaging/tutorials/imaging-introduction/tutorial.html\n\n![](test-data/overlay_image.png)\n\n## Inputs\n\n**`input_image`:** The fluorescence microscopy images to be segmented. Must be the single image channel, which contains the cell nuclei.\n\n## Outputs\n\n**`overlay_image`:** An overlay of the original image and the outlines of the segmentated objects, each also annotated with a unique number.\n\n**`objects_count`:** Table with a single column `objects` and a single row (the actual number of objects).\n\n**`label_image`:** The segmentation result (label map, which contains a unique label for each segmented object).\n",
    "has_changelog": true
  },
  {
    "category": "imaging",
    "workflow_repository": "tissue-microarray-analysis",
    "workflow_files": [],
    "planemo_tests": [],
    "has_test_data": false,
    "has_dockstore_yml": false,
    "has_readme": false,
    "readme_content": null,
    "has_changelog": false
  },
  {
    "category": "metabolomics",
    "workflow_repository": "gcms-metams",
    "workflow_files": [
      {
        "workflow_name": "Mass spectrometry: GCMS with metaMS",
        "number_of_steps": 9,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/lecorguille/msnbase_readmsdata/msnbase_readmsdata/2.16.1+galaxy3",
          "toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_xcmsset/abims_xcms_xcmsSet/3.12.0+galaxy3",
          "toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_plot_chromatogram/xcms_plot_chromatogram/3.12.0+galaxy3",
          "toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_merge/xcms_merge/3.12.0+galaxy3",
          "toolshed.g2.bx.psu.edu/repos/yguitton/metams_rungc/metams_runGC/3.0.0+metaMS1.24.0-galaxy0",
          "toolshed.g2.bx.psu.edu/repos/ethevenot/checkformat/checkFormat/3.0.0",
          "toolshed.g2.bx.psu.edu/repos/ethevenot/multivariate/Multivariate/2.3.10"
        ],
        "file_name": "Mass-spectrometry__GCMS-with-metaMS.ga"
      }
    ],
    "planemo_tests": [
      "Mass-spectrometry__GCMS-with-metaMS-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Mass spectrometry: GCMS with metaMS \n\nThis workflow uses the XCMS tool R package [(Smith, C.A. 2006)](https://bioconductor.org/packages/release/bioc/html/xcms.html) to extract, filter, align and fill gaps, and uses the CAMERA R package [(Kuhl, C 2012)](https://bioconductor.org/packages/release/bioc/html/CAMERA.html) to annotate isotopes, adducts and fragments.\n\nThis workflow is composed with the XCMS tool R package [(Smith, C.A. 2006)](https://bioconductor.org/packages/release/bioc/html/xcms.html) to extract and the metaMS R package [(Wehrens, R 2014)](https://bioconductor.org/packages/release/bioc/html/metaMS.html) for the field of untargeted metabolomics. \n\n\ud83c\udf93 For more information see the [Galaxy Training Network tutorial: Mass spectrometry: GC-MS analysis with metaMS package](https://training.galaxyproject.org/training-material/topics/metabolomics/tutorials/gcms/tutorial.html)\n\n## Inputs\n### sampleMetadata\nThe sampleMetadata tabular file corresponds to a table containing information about your samples\n\nA sample metadata file contains various information for each of your raw files:\n- Classes which will be used during the preprocessing steps\n- Analytical batches which will be useful for a batch correction step, along with sample types (pool/sample) and injection order\n- Different experimental conditions which can be used for statistics\n- Any information about samples that you want to keep, in a column format\n\nThe content of your sample metadata file has to be filled by you, since it is not contained in your raw data. Note that you can either:\n- Upload an existing metadata file\n- Use a template to create one (because it can be painful to get the sample list without misspelling or omission)\n  - Generate a template with the `xcms get a sampleMetadata file` tool available in Galaxy\n  - Fill it using your favorite table editor (Excel, LibreOffice)\n  - Upload it within Galaxy\n\n**Formats:** tab-separated values as tsv, tab, txt, ...\n\n### Mass-spectrometry Dataset Collection\nMass-spectrometry data files gathered in a Galaxy Dataser Collection\n\n**Formats:** open format as mzXML, mzMl, mzData and netCDF\n\n## Main steps\n1. MSnbase readMSData: read the mzXML and prepare for xcms\n2. XCMS findChromPeaks: peak picking\n3. metaMS.runGC: definition of pseudo-spectra\n",
    "has_changelog": true
  },
  {
    "category": "metabolomics",
    "workflow_repository": "lcms-preprocessing",
    "workflow_files": [
      {
        "workflow_name": "Mass spectrometry: LC-MS preprocessing with XCMS",
        "number_of_steps": 14,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/lecorguille/msnbase_readmsdata/msnbase_readmsdata/2.16.1+galaxy3",
          "toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_plot_chromatogram/xcms_plot_chromatogram/3.12.0+galaxy3",
          "toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_xcmsset/abims_xcms_xcmsSet/3.12.0+galaxy3",
          "toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_merge/xcms_merge/3.12.0+galaxy3",
          "toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_group/abims_xcms_group/3.12.0+galaxy3",
          "toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_retcor/abims_xcms_retcor/3.12.0+galaxy3",
          "toolshed.g2.bx.psu.edu/repos/ethevenot/checkformat/checkFormat/3.0.0",
          "toolshed.g2.bx.psu.edu/repos/melpetera/intensity_checks/intens_check/1.3.0",
          "toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_fillpeaks/abims_xcms_fillPeaks/3.12.0+galaxy3",
          "toolshed.g2.bx.psu.edu/repos/lecorguille/camera_annotate/abims_CAMERA_annotateDiffreport/2.2.7+camera1.48.0-galaxy1"
        ],
        "file_name": "Mass_spectrometry__LC-MS_preprocessing_with_XCMS.ga"
      }
    ],
    "planemo_tests": [
      "Mass_spectrometry__LC-MS_preprocessing_with_XCMS-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Mass spectrometry: LC-MS preprocessing with XCMS \n\nThis workflow uses the XCMS tool R package [(Smith, C.A. 2006)](https://bioconductor.org/packages/release/bioc/html/xcms.html) to extract, filter, align and fill gaps, and uses the CAMERA R package [(Kuhl, C 2012)](https://bioconductor.org/packages/release/bioc/html/CAMERA.html) to annotate isotopes, adducts and fragments.\n\n\ud83c\udf93 For more information see the [Galaxy Training Network tutorial: Mass spectrometry: LC-MS preprocessing with XCMS](https://training.galaxyproject.org/training-material/topics/metabolomics/tutorials/lcms-preprocessing/tutorial.html)\n\n## Inputs\n### sampleMetadata\nThe sampleMetadata tabular file corresponds to a table containing information about your samples\n\nA sample metadata file contains various information for each of your raw files:\n- Classes which will be used during the preprocessing steps\n- Analytical batches which will be useful for a batch correction step, along with sample types (pool/sample) and injection order\n- Different experimental conditions which can be used for statistics\n- Any information about samples that you want to keep, in a column format\n\nThe content of your sample metadata file has to be filled by you, since it is not contained in your raw data. Note that you can either:\n- Upload an existing metadata file\n- Use a template to create one (because it can be painful to get the sample list without misspelling or omission)\n  - Generate a template with the `xcms get a sampleMetadata file` tool available in Galaxy\n  - Fill it using your favorite table editor (Excel, LibreOffice)\n  - Upload it within Galaxy\n\n**Formats:** tab-separated values as tsv, tab, txt, ...\n\n### Mass-spectrometry Dataset Collection\nMass-spectrometry data files gathered in a Galaxy Dataser Collection\n\n**Formats:** open format as mzXML, mzMl, mzData and netCDF\n\n## Main steps\n1. MSnbase readMSData: read the mzXML and prepare for xcms\n2. XCMS findChromPeaks: peak picking\n3. XCMS groupChromPeaks: determining shared ions across samples\n4. XCMS adjustRtime: retention time correction\n5. XCMS fillChromPeaks: integrating areas of missing peaks\n6. CAMERA.annotate: annotation\n",
    "has_changelog": true
  },
  {
    "category": "metabolomics",
    "workflow_repository": "mfassignr",
    "workflow_files": [
      {
        "workflow_name": "Molecular formula assignment and recalibration with MFAssignR package.",
        "number_of_steps": 10,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_kmdnoise/mfassignr_kmdnoise/1.1.2+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_histnoise/mfassignr_histnoise/1.1.2+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_snplot/mfassignr_snplot/1.1.2+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_isofiltr/mfassignr_isofiltr/1.1.2+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_mfassigncho/mfassignr_mfassignCHO/1.1.2+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_recallist/mfassignr_recallist/1.1.2+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_findrecalseries/mfassignr_findRecalSeries/1.1.2+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_recal/mfassignr_recal/1.1.2+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_mfassign/mfassignr_mfassign/1.1.2+galaxy1"
        ],
        "file_name": "mfassignr.ga"
      }
    ],
    "planemo_tests": [
      "mfassignr-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Molecular Formula Assignment and Recalibration Workflow with MFAssignR\n\nThis workflow is designed for molecular formula assignment and recalibration of mass spectrometry data using the MFAssignR tool. It processes feature tables to generate recalibrated series, molecular formula assignments, and various diagnostic plots.\n\n## Workflow Steps\n\n1. **Input Feature Table**:\n   - Accepts a feature table in tabular format containing mass spectrometry data.\n\n2. **Molecular Formula Assignment**:\n   - Assigns molecular formulas to features based on mass-to-charge ratios and isotopic patterns.\n\n3. **Recalibration**:\n   - Recalibrates mass spectrometry data to improve accuracy.\n\n4. **Visualization**:\n   - Generates diagnostic plots, including:\n     - Signal-to-noise (SN) plots.\n     - Mass-to-charge (MZ) error plots.\n     - Van Krevelen (VK) diagrams.\n     - Molecular formula assignment plots.\n\n## Inputs\n\n- **Feature Table**: A tabular file containing mass spectrometry data. Example input: `mfassignr_input.txt`.\n\n## Outputs\n\n- **Recalibrated Series**:\n  - `recal_series.tabular`: Recalibrated data series.\n  - `final_series.tabular`: Final recalibrated series.\n\n- **Molecular Formula Assignments**:\n  - `Ambig.tabular`: Ambiguous assignments.\n  - `Unambig.tabular`: Unambiguous assignments.\n\n- **Diagnostic Plots**:\n  - Signal-to-noise plot: `SNplot.png`.\n  - Mass-to-charge error plot: `MZplot.png`.\n  - Van Krevelen diagrams and molecular formula assignment plots for CHO and other elements.\n\n## Usage\n\nThis workflow is designed to be run on the Galaxy platform. Users can upload their feature table, configure parameters, and execute the workflow to obtain recalibrated data, molecular formula assignments, and diagnostic plots.\n\n## References\n\n- [MFAssignR Documentation](https://github.com/your-repo/mfassignr)\n\n## License\n\nThis workflow is distributed under the MIT License. Please ensure proper attribution when using or modifying this workflow.",
    "has_changelog": true
  },
  {
    "category": "metabolomics",
    "workflow_repository": "qcxms-sdf",
    "workflow_files": [
      {
        "workflow_name": "QCxMS Spectra Prediction from SDF",
        "number_of_steps": 5,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/bgruening/openbabel_compound_convert/openbabel_compound_convert/3.1.1+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/recetox/qcxms_neutral_run/qcxms_neutral_run/5.2.1+galaxy6",
          "toolshed.g2.bx.psu.edu/repos/recetox/qcxms_production_run/qcxms_production_run/5.2.1+galaxy4",
          "toolshed.g2.bx.psu.edu/repos/recetox/qcxms_getres/qcxms_getres/5.2.1+galaxy3"
        ],
        "file_name": "QCxMS-Spectra-Prediction-from-SDF.ga"
      }
    ],
    "planemo_tests": [
      "QCxMS-Spectra-Prediction-from-SDF-tests.yml"
    ],
    "has_test_data": true,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# QCxMS Spectra Prediction from SDF Workflow\n\nThis workflow predicts electron ionization (EI) mass spectra using QCxMS, starting from a single SDF file containing the 3D coordinates of all atoms in the molecule. These files can typically be obtained from PubChem. The workflow converts the input file, performs neutral and production runs, and generates predicted spectra in MSP format.\n\n## Workflow Steps\n\n1. **Input SDF File**:\n   - Accepts an SDF file containing one or multiple molecular structures with pre-generated conformers.\n\n2. **Conversion to XYZ Format**:\n   - Converts the input SDF file to XYZ format using Open Babel.\n\n3. **QCxMS Neutral Run**:\n   - Performs a neutral run to prepare the molecular structure for production calculations.\n\n4. **QCxMS Production Run**:\n   - Executes the production run to simulate fragmentation and generate intermediate results.\n\n5. **QCxMS Get Results**:\n   - Processes the results from the production run and generates the predicted EI mass spectra in MSP format.\n\n## Inputs\n\n- **Input SDF File**: A file containing molecular structures with 3D coordinates (e.g., obtained from PubChem).\n\n## Outputs\n\n- **Predicted Spectra**:\n  - An MSP file containing the predicted EI mass spectra for the input molecules.\n\n## Usage\n\nThis workflow is designed to be run on the Galaxy platform. Users can upload their SDF file, configure parameters, and execute the workflow to obtain predicted EI mass spectra.\n\n## References\n\n- [QCxMS Documentation](https://github.com/recetox/qcxms)\n- [Open Babel Documentation](http://openbabel.org/)\n\n## License\n\nThis workflow is distributed under the MIT License. Please ensure proper attribution when using or modifying this workflow.",
    "has_changelog": true
  },
  {
    "category": "microbiome",
    "workflow_repository": "mags-building",
    "workflow_files": [
      {
        "workflow_name": "Metagenome-Assembled Genomes (MAGs) generation",
        "number_of_steps": 54,
        "tools_used": [
          "toolshed.g2.bx.psu.edu/repos/iuc/map_param_value/map_param_value/0.2.0",
          "__UNZIP_COLLECTION__",
          "toolshed.g2.bx.psu.edu/repos/iuc/megahit/megahit/1.2.9+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/nml/metaspades/metaspades/4.2.0+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/pick_value/pick_value/0.2.0",
          "toolshed.g2.bx.psu.edu/repos/iuc/quast/quast/5.3.0+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/devteam/bowtie2/bowtie2/2.5.3+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/concoct_cut_up_fasta/concoct_cut_up_fasta/1.1.0+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/devteam/samtools_sort/samtools_sort/2.0.5",
          "toolshed.g2.bx.psu.edu/repos/iuc/semibin/semibin/2.0.2+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/concoct_coverage_table/concoct_coverage_table/1.1.0+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/metabat2_jgi_summarize_bam_contig_depths/metabat2_jgi_summarize_bam_contig_depths/2.17+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/fasta_to_contig2bin/Fasta_to_Contig2Bin/1.1.7+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/concoct/concoct/1.1.0+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/mbernt/maxbin2/maxbin2/2.2.7+galaxy6",
          "toolshed.g2.bx.psu.edu/repos/iuc/metabat2/metabat2/2.17+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/concoct_merge_cut_up_clustering/concoct_merge_cut_up_clustering/1.1.0+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/concoct_extract_fasta_bins/concoct_extract_fasta_bins/1.1.0+galaxy2",
          "__BUILD_LIST__",
          "toolshed.g2.bx.psu.edu/repos/iuc/binette/binette/1.1.1+galaxy0",
          "__FLATTEN__",
          "toolshed.g2.bx.psu.edu/repos/iuc/checkm2/checkm2/1.0.2+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.5+galaxy2",
          "toolshed.g2.bx.psu.edu/repos/iuc/drep_dereplicate/drep_dereplicate/3.6.2+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/gtdbtk_classify_wf/gtdbtk_classify_wf/2.4.1+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/checkm_lineage_wf/checkm_lineage_wf/1.2.3+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/coverm_genome/coverm_genome/0.7.0+galaxy0",
          "toolshed.g2.bx.psu.edu/repos/iuc/bakta/bakta/1.9.4+galaxy1",
          "toolshed.g2.bx.psu.edu/repos/iuc/collection_column_join/collection_column_join/0.0.3",
          "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy3"
        ],
        "file_name": "MAGs-generation.ga"
      }
    ],
    "planemo_tests": [
      "MAGs-generation-tests.yml"
    ],
    "has_test_data": false,
    "has_dockstore_yml": true,
    "has_readme": true,
    "readme_content": "# Metagenome-Assembled Genomes (MAGs) Generation  \n\nThis workflow generates Metagenome-Assembled Genomes (MAGs) from paired short reads.  \nDereplicated MAGs for the complete input sample set are reported.\n\n## Workflow Logic  \n\nThe workflow supports assembly using **metaSPADES** and **MEGAHIT**.  \nFor binning, it utilizes four different tools: **MetaBAT2, MaxBin2, SemiBin, and CONCOCT**. The resulting bins are then refined using **Binette**, the successor of metaWRAP.  \n\n## MAGs Annotation and Quality Control  \n\nAfter binning, the resulting MAGs are **dereplicated** across all input samples based on **CheckM2 quality metrics** using **dRep**. The following processing steps are then performed:  \n\n- **Annotation** with Bakta  \n- **Taxonomic Assignment** using GTDB-Tk  \n- **Quality Control** via QUAST and CheckM/CheckM2  \n- **Abundance Estimation** per sample with CoverM  \n\nAll results are consolidated into a single **MultiQC report** for easy analysis.  \n\n## Input Requirements  \n\nInput reads must be quality-filtered, with host reads removed. \n\n- **Trimmed reads**: Quality-trimmed reads from individual samples, used solely for abundance estimation.  \n- **Trimmed reads from grouped samples**: These reads need to be grouped based on the desired MAGs generation approach:  \n  - **Individual MAGs Generation**: Use the same input as `Sample-wise Trimmed Paired Reads` to generate MAGs per sample.  \n  - **Pooled MAGs Generation (Co-assembly/Binning)**: Merge all reads input one file for a fully pooled MAGs approach.  \n  - **Grouped MAGs Generation (Co-assembly/Binning)**: Merge samples based on predefined groups.  \n  - **Hybrid MAGs Generation**: Combine individual and grouped reads for a mixed approach.  \n\n> **Note**: Merging reads can result in large input files, significantly increasing computational demands\u2014especially during assembly and binning, which may require substantial RAM. Our tests with synthetic samples up to **50 GB** showed feasible performance. For larger datasets, we recommend limiting the approach to **individual or pooled MAGs generation**.  \n",
    "has_changelog": true
  },
  {
    "category": "microbiome",
    "workflow_repository": "pathogen-identification",
    "workflow_files": [],
    "planemo_tests": [],
    "has_test_data": false,
    "has_dockstore_yml": false,
    "has_readme": true,
    "readme_content": "# Microbiome Workflows\n\nIn this directory, you will find a collection of workflows designed for microbiome data analysis, pathogen detection, and tracking. These workflows are ready to use and can be adapted for various sequencing techniques using Galaxy's customizable and automatable API.\n\n## Avaiable Workflows\n\n- **Nanopore Preprocessing**\n\n- **Taxonomy Profiling and Visualisation with Krona**\n\n- **Gene-based Pathogen Identification**\n\n- **Allele-based Pathogen Identification**\n\n- **Pathogen Detection: PathoGFAIR Samples Aggregation and Visualisation**\n\n## Getting Started\n\nTo learn more about these workflows and to try them with real datasets, please visit our Microbiome tutorials on the Galaxy Training Network (GTN):\n\n[Microbiome Tutorials on GTN](https://training.galaxyproject.org/training-material/topics/microbiome/)\n\n\n## Dedicated Training Material\n\nThe workflows for **Nanopore Preprocessing**, **Taxonomy Profiling and Visualization with Krona**, **Gene-based Pathogen Identification**, **Allele-based Pathogen Identification**, and **Pathogen Detection: PathoGFAIR Samples Aggregation and Visualization** can all be tried out in a dedicated training material on GTN for foodborne pathogen detection and tracking:\n\n[GTN Tutorial for Foodborne Pathogen Detection and Tracking](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/pathogen-detection-from-nanopore-foodborne-data/tutorial.html)\n",
    "has_changelog": false
  }
]