{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 1** - Importing Required modules to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model imported successfully\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('intfloat/e5-base-v2')\n",
    "print(\"model imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2** Loading The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Unzip collection\n",
      "Category: Collection Operations\n",
      "Help: Synopsis\n",
      "This tool takes a paired collection and \"unzips\" it into two simple dataset collections (lists of datasets).\n",
      "\n",
      "Description\n",
      "1. **Functionality**\n",
      "   - Given a paired collection of forward and re...\n",
      "--------------------------------------------------\n",
      "Name: Zip collections\n",
      "Category: Collection Operations\n",
      "Help: Synopsis\n",
      "This tool takes two collections and creates a paired collection from them.\n",
      "\n",
      "Description\n",
      "1. **Functionality**\n",
      "   - If you have one collection containing only forward reads and another containi...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Load the dataset\n",
    "with open(\"dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "for entry in data[:2]:  \n",
    "    print(f\"Name: {entry['name']}\")\n",
    "    print(f\"Category: {entry['category']}\")\n",
    "    print(f\"Help: {entry['help'][:200]}...\") \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3** Extract The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unzip collection -  - Synopsis\\nThis tool takes a paired collection and \"unzips\" it into two simple dataset collections (lists of datasets).\\n\\nDescription\\n1. **Functionality**\\n   - Given a paired collection of forward and reverse reads, this tool separates them into two distinct collections.\\n   - The first output collection contains all forward reads, and the second output collection contains all reverse reads.\\n\\n2. **Use Case**\\n   - Useful for processing paired-end sequencing data.\\n   - Enables downstream analysis by handling forward and reverse reads separately.\\n\\nThis tool simplifies paired dataset management, allowing for more flexible analysis workflows in Galaxy.', 'Zip collections -  - Synopsis\\nThis tool takes two collections and creates a paired collection from them.\\n\\nDescription\\n1. **Functionality**\\n   - If you have one collection containing only forward reads and another containing only reverse reads, this tool will combine them into a paired collection.\\n   - The resulting collection maintains the pairing between corresponding forward and reverse reads.\\n\\n2. **Use Case**\\n   - Useful for handling paired-end sequencing data in workflows.\\n   - Ensures that forward and reverse reads are correctly associated for downstream analysis.\\n\\nThis tool simplifies dataset management by efficiently merging forward and reverse read collections into a structured paired collection.', 'Filter failed datasets -  - Synopsis\\nRemoves datasets in error (red) from a collection.\\n\\nDescription\\nThis tool takes a dataset collection and filters out (removes) datasets in the failed (red) state. This is useful for continuing a multi-sample analysis when one or more of the samples fails at some point.\\n\\nThis tool will create new history datasets from your collection but your quota usage will not increase.']\n"
     ]
    }
   ],
   "source": [
    "# Extract text data\n",
    "sentences = [f\"{entry['name']} - {entry['description']} - {entry['help']}\" for entry in data]\n",
    "\n",
    "# Print sample\n",
    "print(sentences[:3])  # Check first 3 processed sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4** Creating Training Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample\n",
    "\n",
    "# Creating training examples (assuming similarity-based fine-tuning)\n",
    "train_examples = []\n",
    "for i in range(len(sentences) - 1):\n",
    "    train_examples.append(InputExample(texts=[sentences[i], sentences[i+1]], label=1.0))  # assuming similarity for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 5** Define Data loader and Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, losses\n",
    "import datasets\n",
    "\n",
    "# Define a DataLoader\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=8)\n",
    "\n",
    "# Define the loss function for contrastive learning\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 6** Fine Tune SBERT Model With Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fine-tune the E5 model\n",
    "#model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=20, warmup_steps=100)\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)\n",
    "# Save the fine-tuned model\n",
    "version = 1\n",
    "model.save(f\"fine_tuned_E5_for_galaxy_v{version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01452434 -0.02709516 -0.03902506 ... -0.01207174  0.01802179\n",
      "   0.03497765]\n",
      " [ 0.00367054 -0.04538389 -0.02575246 ... -0.04581482  0.02639549\n",
      "   0.01231393]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "fine_tuned_model = SentenceTransformer(f\"fine_tuned_E5_for_galaxy_v{version}\")\n",
    "sentences = [\"This tool helps in dataset filtering.\", \"Merging collections is useful.\"]\n",
    "embeddings = fine_tuned_model.encode(sentences)\n",
    "print(embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
