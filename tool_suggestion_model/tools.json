{
  "$schema": "./validators/tool-validator.schema.json",
  "tools":[
    {
        "name": "Unzip collection",
        "description": "lksndf",
        "category": ["Collection Operations"],
        "help": "Synopsis\nThis tool takes a paired collection and \"unzips\" it into two simple dataset collections (lists of datasets).\n\nDescription\n1. **Functionality**\n   - Given a paired collection of forward and reverse reads, this tool separates them into two distinct collections.\n   - The first output collection contains all forward reads, and the second output collection contains all reverse reads.\n\n2. **Use Case**\n   - Useful for processing paired-end sequencing data.\n   - Enables downstream analysis by handling forward and reverse reads separately.\n\nThis tool simplifies paired dataset management, allowing for more flexible analysis workflows in Galaxy."
    },
    {
        "name": "Zip collections",
        "description": "",
        "category": ["Collection Operations"],
        "help": "Synopsis\nThis tool takes two collections and creates a paired collection from them.\n\nDescription\n1. **Functionality**\n   - If you have one collection containing only forward reads and another containing only reverse reads, this tool will combine them into a paired collection.\n   - The resulting collection maintains the pairing between corresponding forward and reverse reads.\n\n2. **Use Case**\n   - Useful for handling paired-end sequencing data in workflows.\n   - Ensures that forward and reverse reads are correctly associated for downstream analysis.\n\nThis tool simplifies dataset management by efficiently merging forward and reverse read collections into a structured paired collection."
    },
    {
        "name": "Filter failed datasets",
        "description": "",
        "category": ["Collection Operations"],
        "help": "Synopsis\nRemoves datasets in error (red) from a collection.\n\nDescription\nThis tool takes a dataset collection and filters out (removes) datasets in the failed (red) state. This is useful for continuing a multi-sample analysis when one or more of the samples fails at some point.\n\nThis tool will create new history datasets from your collection but your quota usage will not increase."

    },
    {
        "name": "Filter empty datasets",
        "description": "",
        "category": ["Collection Operations"],
        "help": "Synopsis\nRemoves empty elements from a collection.\n\nDescription\nThis tool takes a dataset collection and filters out (removes) empty datasets. This is useful for continuing a multi-sample analysis when downstream tools require datasets to have content.\n\nThis tool will create new history datasets from your collection but your quota usage will not increase."

    },
    {
        "name": "Filter null elements",
        "description": "",
        "category": ["Collection Operations"],
        "help": "Synopsis\nRemoves null elements from a collection.\n\nDescription\nThis tool takes a dataset collection and filters out nulls. This is useful for removing elements that resulted from conditional execution of jobs.\n\nThis tool will create new history datasets from your collection but your quota usage will not increase."

    },
    {
        "name": "Flatten collection",
        "description": "",
        "category": ["Collection Operations"],
        "help": "Synopsis\nFlattens nested collection into a simple list.\n\nDescription\nThis tool takes nested collections such as a list of lists or a list of dataset pairs and produces a flat list from the inputs. It effectively \"flattens\" the hierarchy. The collection identifiers are merged together (using \"_\" as default) to create new collection identifiers in the flattened result."
    },
    {
        "name": "Merge collections",
        "description": "",
        "category": ["Collection Operations"],
        "help": "Synopsis\nTakes two or more collections and creates a single collection from them.\n\nDescription\nBy default, the tool assumes that collections being merged have unique dataset names. If this is not the case, only one (the first) of the datasets with a repeated name will be included in the merged collection. For example, suppose you have two collections. Each has two datasets named \"A\" and \"B\":\n\nCollection 1:\n[Dataset A]\n[Dataset B]\n[Dataset X]\n\nCollection 2:\n[Dataset A]\n[Dataset B]\n[Dataset Y]\n\nMerging them will produce a single collection with only two datasets:\n\nMerged Collection:\n[Dataset A]\n[Dataset B]\n[Dataset X]\n[Dataset Y]\n\nThis behavior can be changed by clicking on the \"Advanced Options\" link. The following options are available:\n\nKeep first instance (Default behavior)\n\nInput:\n\nCollection 1:\n[Dataset A]\n[Dataset B]\n[Dataset X]\n\nCollection 2:\n[Dataset A]\n[Dataset B]\n[Dataset Y]\n\nOutput:\n\nMerged Collection:\n[Dataset A]\n[Dataset B]\n[Dataset X]\n[Dataset Y]\n\nHere, if two collections have identical dataset names, a dataset is chosen from the first collection.\n\nKeep last instance\n\nInput:\n\nCollection 1:\n[Dataset A]\n[Dataset B]\n[Dataset X]\n\nCollection 2:\n[Dataset A]\n[Dataset B]\n[Dataset Y]\n\nOutput:\n\nMerged Collection:\n[Dataset A]\n[Dataset B]\n[Dataset X]\n[Dataset Y]\n\nHere, if two collections have identical dataset names, a dataset is chosen from the last collection."

    },
    {
        "name": "Relabel identifiers",
        "description": "",
        "category": ["Collection Operations"],
        "help": ""
    },
    {
        "name": "Filter collection",
        "description": "",
        "category": ["Collection Operations"],
        "help": "Synopsis\nFilters elements from a collection using a list supplied in a file.\n\nDescription\nThis tools allow filtering elements from a data collection. It takes an input collection and a text file with names (i.e. identifiers). The tool behavious is controlled by How should the elements to remove be determined? drop-down. It has the following options:\n\nRemove if identifiers are ABSENT from file\nNote how the tool deals with the Z entry.\n\nThis tool will create new history datasets from your collection but your quota usage will not increase."

    },
    {
        "name": "Sort collection",
        "description": "",
        "category": ["Collection Operations"],
        "help": "Synopsis\nThis tool sorts a dataset collection alphabetically, numerically, or using a predefined order from a supplied file.\n\nDescription\n1. **Numeric Sort**\n   - Sorts datasets in ascending numerical order, ignoring non-numeric characters.\n   - Example:\n     \n     Given a collection:\n     \n     Collection: [Horse123]\n                 [Donkey543]\n                 [Mule176]\n     - The tool will output:\n     \n     Collection: [Horse123]\n                 [Mule176]\n                 [Donkey543]\n\n2. **Sorting from File**\n   - Allows sorting based on a specified order in a single-column text file.\n   - Example:\n     \n     Given a collection:\n     \n     Collection: [Horse123]\n                 [Donkey543]\n                 [Mule176]\n     - And a file specifying sort order:\n     \n     Donkey543\n     Horse123\n     Mule176\n     - The tool will output:\n     \n     Collection: [Donkey543]\n                 [Horse123]\n                 [Mule176]\n\nThis tool provides flexible sorting options for dataset collections, ensuring the desired order for further processing."

    },
    {
        "name": "Harmonize two collections",
        "description": "",
        "category": ["Collection Operations"],
        "help": "Synopsis\nHarmonize 2 collections: Inputs are 2 collections. Outputs are 2 collections with:\n- Same identifiers (identifiers which are specific to one or the other are removed)\n- Identifiers are in the same order\n\nExample\nIf the inputs are:\n\nCollection1:\n[Horse123]\n[Donkey543]\n[Mule176]\n\nCollection2:\n[Horse]\n[Mule176]\n[Donkey543]\n\nThe tool will output:\n\nCollection1:\n[Donkey543]\n[Mule176]\n\nCollection2:\n[Donkey543]\n[Mule176]"

    },
    {
        "name": "Flat Cross Product",
        "description": "",
        "category": ["Collection Operations"],
        "help": "Synopsis\nThis tool organizes two dataset lists so that Galaxy's normal collection processing produces an all-vs-all style analyses of the initial inputs when applied to the outputs of this tool.\n\nDescription\nWhile a description of what it does standalone is technical and math heavy, how it works within an ad-hoc analysis or workflow can be quite straight forward and hopefully is easier to understand. For this reason, the next section describes how to use this tool in context and the technical details follow after that. Hopefully, the \"how it works\" details aren't necessary to understand the \"how to use it\" details of this tool - at least for simple things.\n\nHow to use this tool\nThis tool can be used in and out of workflows, but workflows will be used to illustrate the ordering of tools and connections between them. Imagine a tool that compares two individual datasets and how that might be connected to list inputs in a workflow. This simple case is shown below."

    },
    {
        "name": "Nested Cross Product",
        "description": "",
        "category": ["Collection Operations"],
        "help": "Synopsis\nThis tool organizes two dataset lists so that Galaxy's normal collection processing produces an all-vs-all style analysis of the initial inputs when applied to the outputs of this tool.\n\nHow to use this tool\nThis tool can be used in and out of workflows. Workflows will be used to illustrate the ordering of tools and connections between them. Imagine a tool that compares two individual datasets and how that might be connected to list inputs in a workflow.\n\nThe Dot Product of Two Collections\nIn this configuration, datasets will be matched and compared element-wise. The first dataset of \"Input List 1\" will be compared to the first dataset in \"Input List 2,\" and the resulting dataset will be the first dataset in the output list generated using this comparison tool. In this setup, the lists need to have the same number of elements and ideally matching element identifiers.\n\nSometimes, however, the goal is to compare each element of the first list to every element of the second list. This tool enables that functionality.\n\nThe Cartesian Product of Two Collections\nThis tool consumes two flat lists (input_a and input_b). If input_a has length n and dataset elements identified as a1, a2, ... an, and input_b has length m with dataset elements identified as b1, b2, ... bm, this tool produces a pair of output nested lists (list:list collection type). The outer list has a length of n, and each inner list has a length of m, forming an n × m nested list.\n\nThe first output is a nested list where the jth element inside the outer list's ith element is a pseudo copy of the ith dataset of input_a. The second output is a nested list where the jth element inside the outer list's ith element is a pseudo copy of the jth dataset of input_b.\n\nThese nested structures ensure that when corresponding elements of the nested lists are matched, each combination of elements from input_a and input_b is compared once, producing a full cross-product comparison."

    },
    {
        "name": "Tag elements",
        "description": "",
        "category": ["Collection Operations"],
       "help": "Synopsis\nThis tool adds tags (including name: and group: tags) to collection elements.\n\nDescription\n1. **Tagging Collection Elements**\n   - The relationship between element names and tags is specified in a two-column tab-delimited file.\n   - If the file contains fewer entries than elements in the collection, only matching list identifiers will be tagged.\n   \n2. **Creating Name and Group Tags**\n   - To create `name:` or `group:` tags, prepend them with `#`, `name:`, or `group:`.\n\n3. **More About Tags**\n   - **Simple Tags:** Attach an alternative label to a dataset for easier retrieval.\n   - **Name Tags:** Track dataset propagation through analyses—derived datasets inherit the tag.\n   - **Group Tags:** Label groups of datasets (e.g., 'treatment' and 'control' for differential expression analysis).\n\nThis tool enhances dataset organization and facilitates structured analysis workflows in Galaxy."

    },
    {
        "name": "Apply rules",
        "description": "",
        "category": ["Collection Operations"],
        "help": "Synopsis: This tool allows one to process an existing Galaxy dataset collection's metadata as tabular data, apply a series of rules to it, and generate a new collection.\n\nDescription: When used interactively in the tool form, a dynamic preview of the processing will be available in a tabular data viewer but this tool may be used in workflows as well where no such preview can be generated.\n\nThis tool is an advanced feature but has a lot of flexibility - it can be used to process collections with arbitrary nesting and can do many kinds of filtering, re-sorting, nesting, flattening, and arbitrary combinations thereof not possible with Galaxy's other, more simple collection operation tools.\n\nMore information about the rule processor in general can be found at our training site.\n\nThis tool will create new history datasets from your collection but your quota usage will not increase."
    },
    {
        "name": "Build list",
        "description": "",
        "category": ["Collection Operations"],
        "help": "Synopsis\nBuilds a new list collection from individual datasets or collections.\n\nDescription\nThis tool combines individual datasets or collections into a new collection. The simplest scenario is building a new collection from individual datasets (case A in the image below). You can merge a collection with individual dataset(s). In this case (see B in the image below), the individual dataset(s) will be merged with each element of the input collection to create a nested collection. Finally, two or more collections can be merged together creating a nested collection (case C in the image below).\n\nNote: When merging collections (e.g., case C below), the input collection must have equal number of elements."
    },
    {
        "name": "Extract dataset",
        "description": "",
        "category": ["Collection Operations"],
        "help": "Synopsis\nExtracts datasets from a collection based on either position or identifier.\n\nDescription\nThe tool allow extracting datasets based on position (The first dataset and Select by index options) or name (Select by element identifier option). This tool effectively collapses the inner-most collection into a dataset. For nested collections (e.g a list of lists of lists: outer:middle:inner, extracting the inner dataset element) a new list is created where the selected element takes the position of the inner-most collection (so outer:middle, where middle is not a collection but the inner dataset element).\n\nNote: Dataset index (numbering) begins with 0 (zero).\n\nThis tool will create new history datasets from your collection but your quota usage will not increase."
    },
    {
        "name": "Duplicate file to collection",
        "description": "",
        "category": ["Collection Operations"],
        "help": "Synopsis\nCreates a collection of arbitrary size by duplicating an input dataset N times, where N is a user-specified integer.\n\nDescription\nThis tool allows creation of a dataset collection of arbitrary size. It takes an input dataset and an integer parameter, which specifies the number of times to duplicate the dataset in the output collection. In addition, the user can specify the base name for the element identifier to use in the output. For example, if Number is specified as 3 and Element identifier as 'Element', the output collection will contain three identical datasets, with the identifiers Element 1, Element 2 and Element 3.\n\nThis tool will create new history datasets but your quota usage will not increase."
    },
    {
        "name": "Parse parameter value",
        "description": "from dataset",
        "category": ["Expression Tools"],
        "help": ""
    },
    {
        "name": "Convert genome coordinates",
        "description": "between assemblies and genomes",
        "category": ["Lift-Over"],
        "help": "This tool converts genomic coordinates and annotations between different genome assemblies using the LiftOver utility from UCSC Genome Browser. It works with interval, GFF, and GTF datasets, converting them between genome assemblies, and produces two output files: one with mapped coordinates and the other with unmapped coordinates. The interval datasets should have chromosome in column 1, start coordinate in column 2, and end coordinate in column 3. BED comments and track lines will be ignored. This tool is useful for genome assembly liftover tasks and ensures annotation compatibility between assemblies. For example, converting hg16 intervals to hg18 intervals."

    },
    {
        "name": "Add column",
        "description": "to an existing dataset",
        "category": ["Text Manipulation"],
        "help": "This tool allows you to add a new column to your dataset by entering any value in the text box. The value will be appended to each row in the dataset. For example, if your original data looks like: chr1 10 100 geneA, chr2 200 300 geneB, chr2 400 500 geneC, typing '+' in the text box will generate: chr1 10 100 geneA +, chr2 200 300 geneB +, chr2 400 500 geneC +. You can also add line numbers by selecting Iterate: YES. If you enter '1' in the text box, it will add line numbers: chr1 10 100 geneA 1, chr2 200 300 geneB 2, chr2 400 500 geneC 3."

    },
    {
        "name": "Concatenate datasets",
        "description": "tail-to-head",
        "category": ["Text Manipulation"],
        "help": "This tool helps to concatenate datasets from different sources into a single dataset. WARNING: Be careful not to concatenate datasets of different kinds (e.g., sequences with intervals), as this tool does not check if the datasets being concatenated are in the same format.\n\nWhat it does\nConcatenates datasets from multiple sources into one combined dataset. It does not perform any format checks, so ensure that the datasets being concatenated are compatible.\n\nExample\nConcatenating Dataset:\nchrX 151087187 151087355 A 0 -\nchrX 151572400 151572481 B 0 +\nwith Dataset1:\nchr1 151242630 151242955 X 0 +\nchr1 151271715 151271999 Y 0 +\nchr1 151278832 151279227 Z 0 -\nand with Dataset2:\nchr2 100000030 200000955 P 0 +\nchr2 100000015 200000999 Q 0 +\nwill result in the following:\nchrX 151087187 151087355 A 0 -\nchrX 151572400 151572481 B 0 +\nchr1 151242630 151242955 X 0 +\nchr1 151271715 151271999 Y 0 +\nchr1 151278832 151279227 Z 0 -\nchr2 100000030 200000955 P 0 +\nchr2 100000015 200000999 Q 0 +"
    },
    {
        "name": "Cut",
        "description": "columns from a table",
        "category": ["Text Manipulation"],
        "help": "This tool helps to cut specified columns from a dataset, reformatting the data into a tabular format.\n\nWhat it does\n1. **Functionality**\n   - Selects (cuts out) specified columns from a dataset.\n   - The columns are specified as c1, c2, and so on, with the column count starting at 1.\n   - Columns can be specified in any order (e.g., c2,c1,c6).\n   - If more columns are specified than actually exist, empty spaces will be filled with dots (.)\n\n2. **Example**\n   - Input dataset (six columns: c1, c2, c3, c4, c5, and c6):\n     chr1 10   1000  gene1 0 +\n     chr2 100  1500  gene2 0 +\n   - Cutting on columns \"c1,c4,c6\" will return:\n     chr1 gene1 +\n     chr2 gene2 +\n   - Cutting on columns \"c6,c5,c4,c1\" will return:\n     + 0 gene1 chr1\n     + 0 gene2 chr2\n   - Cutting on columns \"c1-c3\" will return:\n     chr1 10   1000\n     chr2 100  1500\n   - Cutting on columns \"c8,c7,c4\" will return:\n     . . gene1\n     . . gene2\n\n3. **Use Case**\n   - Useful for reorganizing data by selecting specific columns for downstream analysis."

    },
    {
        "name": "Merge Columns",
        "description": "together",
        "category": ["Text Manipulation"],
        "help": "This tool helps to merge columns together in a dataset.\n\nWhat it does\n1. **Functionality**\n   - Merges any number of valid columns in any order.\n   - The merged result is added as the rightmost column, while preserving the original columns.\n\n2. **Example**\n   - Input dataset (five columns: c1, c2, c3, c4, and c5):\n     1 10   1000  gene1 chr\n     2 100  1500  gene2 chr\n   - Merging columns 'c5,c1' will return:\n     1 10   1000  gene1 chr chr1\n     2 100  1500  gene2 chr chr2"

    },
    {
        "name": "Convert",
        "description": "delimiters to TAB",
        "category": ["Text Manipulation"],
        "help": "This tool helps to convert all delimiters of a specified type into TABs. It also condenses consecutive delimiters into a single TAB.\n\nWhat it does\n1. **Functionality**\n   - Replaces specified delimiters (such as pipe `|`) with TABs.\n   - Condenses consecutive delimiters into a single TAB.\n\n2. **Example**\n   - Input file with pipe delimiters:\n     chrX||151283558|151283724|NM_000808_exon_8_0_chrX_151283559_r|0|-\n     chrX|151370273|151370486|NM_000808_exon_9_0_chrX_151370274_r|0|-\n     chrX|151559494|151559583|NM_018558_exon_1_0_chrX_151559495_f|0|+\n     chrX|151564643|151564711|NM_018558_exon_2_0_chrX_151564644_f||||0|+\n   - The tool converts all pipe delimiters to TABs and condenses consecutive delimiters, resulting in:\n     chrX  151283558  151283724  NM_000808_exon_8_0_chrX_151283559_r  0  -\n     chrX  151370273  151370486  NM_000808_exon_9_0_chrX_151370274_r  0  -\n     chrX  151559494  151559583  NM_018558_exon_1_0_chrX_151559495_f  0  +\n     chrX  151564643  151564711  NM_018558_exon_2_0_chrX_151564644_f  0  +\n\n3. **Use Case**\n   - Useful for cleaning and standardizing dataset formats, especially when working with inconsistent delimiters."
    },
    {
        "name": "Create single interval",
        "description": "as a new dataset",
        "category": ["Text Manipulation"],
        "help": "This tool helps to create a single genomic interval. The resulting history item will be in the BED format.\n\nWhat it does\n1. **Functionality**\n   - Creates a single genomic interval based on the user-specified values.\n   - The resulting output will be in BED format.\n\n2. **Example**\n   - Input values:\n     - Chromosome: chrX\n     - Start position: 151087187\n     - End position: 151370486\n     - Name: NM_000808\n     - Strand: minus\n   - The tool generates the following output:\n     chrX  151087187  151370486  NM_000808  0  -\n\n3. **Use Case**\n   - Useful for creating individual genomic intervals to be used in other genomic analyses or workflows."

    },
    {
        "name": "Change Case",
        "description": "of selected columns",
        "category": ["Text Manipulation"],
        "help": "This tool breaks column assignments. To re-establish column assignments run the tool and click on the pencil icon in the resulting history item.\nThe format of the resulting dataset from this tool is always tabular.\n\nWhat it does\nThis tool selects specified columns from a dataset and converts the values of those columns to upper or lower case.\nColumns are specified as c1, c2, and so on.\nColumns can be specified in any order (e.g., c2,c1,c6).\n\nExample\nChanging columns 1 and 3 (delimited by Comma) to upper case in:\napple,is,good\nwindows,is,bad\nwill result in:\nAPPLE is GOOD\nWINDOWS is BAD"

    },
    {
        "name": "Paste",
        "description": "two files side by side",
        "category": ["Text Manipulation"],
        "help": "This tool helps to merge two datasets side by side while preserving the column assignments of the first dataset.\n\nWhat it does\n1. **Functionality**\n   - Merges two datasets side by side.\n   - The first (left) dataset’s column assignments (such as chromosome, start, end, and strand) will be preserved.\n   - If needed, column assignments can be modified by clicking the pencil icon in the history item.\n\n2. **Example**\n   - First dataset:\n     a 1\n     a 2\n     a 3\n   - Second dataset:\n     20\n     30\n     40\n   - Pasting them together will produce:\n     a 1 20\n     a 2 30\n     a 3 40"

    },
    {
        "name": "Remove beginning",
        "description": "of a file",
        "category": ["Text Manipulation"],
        "help": "This tool helps to remove a specified number of lines from the beginning of a dataset.\n\nWhat it does\n1. **Functionality**\n   - Removes a given number of lines from the start of a dataset.\n\n2. **Example**\n   - **Input File:**\n     chr7  56632  56652   D17003_CTCF_R6  310  +\n     chr7  56736  56756   D17003_CTCF_R7  354  +\n     chr7  56761  56781   D17003_CTCF_R4  220  +\n     chr7  56772  56792   D17003_CTCF_R7  372  +\n     chr7  56775  56795   D17003_CTCF_R4  207  +\n   - **After removing the first 3 lines:**\n     chr7  56772  56792   D17003_CTCF_R7  372  +\n     chr7  56775  56795   D17003_CTCF_R4  207  +"

    },
    {
        "name": "Select random lines",
        "description": "from a file",
        "category": ["Text Manipulation"],
        "help": "This tool selects N random lines from a file, ensuring no repeats and preserving the original ordering.\n\nWhat it does\n1. **Functionality**\n   - Randomly extracts a specified number of lines from the dataset.\n   - Ensures that no line is selected more than once.\n   - Maintains the relative order of the selected lines.\n\n2. **Usage**\n   - Input the dataset and specify the number of lines (N) to extract.\n   - The tool will return N randomly selected lines while preserving their original order.\n\n3. **Example**\n   - Input dataset:\n     ```\n     chr7  56632  56652   D17003_CTCF_R6  310  +\n     chr7  56736  56756   D17003_CTCF_R7  354  +\n     chr7  56761  56781   D17003_CTCF_R4  220  +\n     chr7  56772  56792   D17003_CTCF_R7  372  +\n     chr7  56775  56795   D17003_CTCF_R4  207  +\n     ```\n   - Selecting 2 random lines might return:\n     ```\n     chr7  56736  56756   D17003_CTCF_R7  354  +\n     chr7  56775  56795   D17003_CTCF_R4  207  +\n     ```"
    },
    {
        "name": "Select first",
        "description": "lines from a dataset",
        "category": ["Text Manipulation"],
        "help": "This tool helps to extract a specified number of lines from the beginning of a dataset.\n\nWhat it does\n1. **Functionality**\n   - Retrieves a user-defined number of lines from the top of the dataset.\n   - Helps in previewing or sampling large datasets.\n\n2. **Usage**\n   - Input the dataset and specify the number of lines to extract.\n   - The tool will output only the selected number of lines from the beginning.\n\n3. **Example**\n   - Input dataset:\n     ```\n     chr7  56632  56652  D17003_CTCF_R6  310  +\n     chr7  56736  56756  D17003_CTCF_R7  354  +\n     chr7  56761  56781  D17003_CTCF_R4  220  +\n     chr7  56772  56792  D17003_CTCF_R7  372  +\n     chr7  56775  56795  D17003_CTCF_R4  207  +\n     ```\n   - Selecting 2 lines will return:\n     ```\n     chr7  56632  56652  D17003_CTCF_R6  310  +\n     chr7  56736  56756  D17003_CTCF_R7  354  +\n     ```"
    },
    {
        "name": "Select last",
        "description": "lines from a dataset",
        "category": ["Text Manipulation"],
        "help": "This tool helps to extract a specified number of lines from the end of a dataset.\n\nWhat it does\n1. **Functionality**\n   - Retrieves a user-defined number of lines from the bottom of the dataset.\n   - Useful for inspecting recent data or the last few entries in a dataset.\n\n2. **Usage**\n   - Input the dataset and specify the number of lines to extract.\n   - The tool will output only the selected number of lines from the end.\n\n3. **Example**\n   - Input dataset:\n     ```\n     chr7    57134   57154   D17003_CTCF_R7  356     -\n     chr7    57247   57267   D17003_CTCF_R4  207     +\n     chr7    57314   57334   D17003_CTCF_R5  269     +\n     chr7    57341   57361   D17003_CTCF_R7  375     +\n     chr7    57457   57477   D17003_CTCF_R3  188     +\n     ```\n   - Selecting 2 lines will return:\n     ```\n     chr7    57341   57361   D17003_CTCF_R7  375     +\n     chr7    57457   57477   D17003_CTCF_R3  188     +\n     ```"
    },
    {
        "name": "Trim",
        "description": "leading or trailing characters",
        "category": ["Text Manipulation"],
        "help": "This tool trims a specified number of characters from a dataset or a selected column if the dataset is tab-delimited.\n\nWhat it does\n1. **Functionality**\n   - Removes characters from the beginning, end, or both based on user input.\n   - Works on entire lines or a specific column in a tab-delimited dataset.\n\n2. **Usage**\n   - Input the dataset and specify trimming parameters.\n   - Choose:\n     - 'Trim from the beginning up to this position' to remove characters from the start.\n     - 'Remove everything from this position to the end' to trim from the end.\n   - The tool will return the modified dataset accordingly.\n\n3. **Example**\n   - Input dataset:\n     ```\n     1234567890\n     abcdefghijk\n     ```\n   - Trimming settings:\n     - Trim from the beginning up to position **2**\n     - Remove everything from position **6** to the end\n   - Output:\n     ```\n     23456\n     bcdef\n     ```"
    },
    {
        "name": "Line/Word/Character count",
        "description": "of a dataset",
        "category": ["Text Manipulation"],
        "help": "This tool helps to calculate counts of specified attributes (lines, words, characters) in a dataset.\n\nWhat it does\n1. **Functionality**\n   - Outputs counts for the specified attributes: lines, words, or characters.\n   - Useful for quickly analyzing the size and composition of a dataset.\n\n2. **Example Output**\n   - #lines  words  characters\n     7499    41376  624971\n\n3. **Citation**\n   - If you use this tool in Galaxy, please cite Blankenberg D, et al. In preparation."

    },
    {
        "name": "Secure Hash / Message Digest",
        "description": "on a dataset",
        "category": ["Text Manipulation"],
        "help": "This tool helps to generate Secure Hashes / Message Digests for a dataset using user-selected algorithms.\n\nWhat it does\n1. **Functionality**\n   - Computes cryptographic hash values for datasets.\n   - Supports various hashing algorithms like MD5, SHA-1, SHA-256, etc.\n\n2. **Usage**\n   - Select the dataset and choose the desired hashing algorithm.\n   - The tool will output the computed hash values.\n\n3. **Citation**\n   - If you use this tool in Galaxy, please cite Blankenberg D, et al. In preparation."

    },
    {
        "name": "BED-to-GFF",
        "description": "converter",
        "category": ["Convert Formats"],
        "help": "This tool converts data from BED format to GFF format.\n\nExample:\nThe following data in BED format:\n\nchr28\t346187\t388197\tBC114771\t0\t+\t346187\t388197\t0\t9\t144,81,115,63,155,96,134,105,112,\t0,24095,26190,31006,32131,33534,36994,41793,41898,\n\nWill be converted to GFF (note that the start coordinate is incremented by 1):\n\nchr28\tbed2gff\tmRNA\t346188\t388197\t0\t+\t.\tmRNA BC114771;\nchr28\tbed2gff\texon\t346188\t346331\t0\t+\t.\texon BC114771;\nchr28\tbed2gff\texon\t370283\t370363\t0\t+\t.\texon BC114771;\nchr28\tbed2gff\texon\t372378\t372492\t0\t+\t.\texon BC114771;\nchr28\tbed2gff\texon\t377194\t377256\t0\t+\t.\texon BC114771;\nchr28\tbed2gff\texon\t378319\t378473\t0\t+\t.\texon BC114771;\nchr28\tbed2gff\texon\t379722\t379817\t0\t+\t.\texon BC114771;\nchr28\tbed2gff\texon\t383182\t383315\t0\t+\t.\texon BC114771;\nchr28\tbed2gff\texon\t387981\t388085\t0\t+\t.\texon BC114771;\nchr28\tbed2gff\texon\t388086\t388197\t0\t+\t.\texon BC114771;\n\nAbout formats:\n\nBED format (Browser Extensible Data format) was designed at UCSC for displaying data tracks in the Genome Browser. It has three required fields and several additional optional ones:\n\nThe first three BED fields (required) are:\n1. chrom - The name of the chromosome (e.g. chr1, chrY_random).\n2. chromStart - The starting position in the chromosome. (The first base in a chromosome is numbered 0.)\n3. chromEnd - The ending position in the chromosome, plus 1 (i.e., a half-open interval).\n\nThe additional BED fields (optional) are:\n4. name - The name of the BED line.\n5. score - A score between 0 and 1000.\n6. strand - Defines the strand - either '+' or '-'.\n7. thickStart - The starting position where the feature is drawn thickly at the Genome Browser.\n8. thickEnd - The ending position where the feature is drawn thickly at the Genome Browser.\n9. reserved - This should always be set to zero.\n10. blockCount - The number of blocks (exons) in the BED line.\n11. blockSizes - A comma-separated list of the block sizes. The number of items in this list should correspond to blockCount.\n12. blockStarts - A comma-separated list of block starts. All of the blockStart positions should be calculated relative to chromStart. The number of items in this list should correspond to blockCount.\n13. expCount - The number of experiments.\n14. expIds - A comma-separated list of experiment ids. The number of items in this list should correspond to expCount.\n15. expScores - A comma-separated list of experiment scores. All of the expScores should be relative to expIds. The number of items in this list should correspond to expCount.\n\nGFF format (General Feature Format) is a format for describing genes and other features associated with DNA, RNA, and Protein sequences. GFF lines have nine tab-separated fields:\n\n1. seqname - Must be a chromosome or scaffold.\n2. source - The program that generated this feature.\n3. feature - The name of this type of feature. Some examples of standard feature types are 'CDS', 'start_codon', 'stop_codon', and 'exon'.\n4. start - The starting position of the feature in the sequence. The first base is numbered 1.\n5. end - The ending position of the feature (inclusive).\n6. score - A score between 0 and 1000. If there is no score value, enter '.'.\n7. strand - Valid entries include '+', '-', or '.' (for don't know/care).\n8. frame - If the feature is a coding exon, frame should be a number between 0-2 that represents the reading frame of the first base. If the feature is not a coding exon, the value should be '.'.\n9. group - All lines with the same group are linked together into a single item."
    },
    {
        "name": "GFF-to-BED",
        "description": "converter",
        "category": ["Convert Formats"],
         "help": "What it does\n\nThis tool converts data from GFF format to BED format (scroll down for format description).\n\nExample\n\nThe following data in GFF format:\n\nchr22  GeneA  enhancer  10000000  10001000  500      +   .  TGA\nchr22  GeneA  promoter  10010000  10010100  900      +   .  TGA\n\nWill be converted to BED (note that 1 is subtracted from the start coordinate):\n\nchr22   9999999  10001000   enhancer   0   +\nchr22  10009999  10010100   promoter   0   +\n\nAbout formats\n\nBED format\nBrowser Extensible Data format was designed at UCSC for displaying data tracks in the Genome Browser. It has three required fields and several additional optional ones:\n\nThe first three BED fields (required) are:\n\n1. chrom - The name of the chromosome (e.g. chr1, chrY_random).\n2. chromStart - The starting position in the chromosome. (The first base in a chromosome is numbered 0.)\n3. chromEnd - The ending position in the chromosome, plus 1 (i.e., a half-open interval).\n\nThe additional BED fields (optional) are:\n\n4. name - The name of the BED line.\n5. score - A score between 0 and 1000.\n6. strand - Defines the strand - either '+' or '-'.\n7. thickStart - The starting position where the feature is drawn thickly at the Genome Browser.\n8. thickEnd - The ending position where the feature is drawn thickly at the Genome Browser.\n9. reserved - This should always be set to zero.\n10. blockCount - The number of blocks (exons) in the BED line.\n11. blockSizes - A comma-separated list of the block sizes. The number of items in this list should correspond to blockCount.\n12. blockStarts - A comma-separated list of block starts. All of the blockStart positions should be calculated relative to chromStart. The number of items in this list should correspond to blockCount.\n13. expCount - The number of experiments.\n14. expIds - A comma-separated list of experiment ids. The number of items in this list should correspond to expCount.\n15. expScores - A comma-separated list of experiment scores. All of the expScores should be relative to expIds. The number of items in this list should correspond to expCount.\n\nGFF format\nGeneral Feature Format is a format for describing genes and other features associated with DNA, RNA and Protein sequences. GFF lines have nine tab-separated fields:\n\n1. seqname - Must be a chromosome or scaffold.\n2. source - The program that generated this feature.\n3. feature - The name of this type of feature. Some examples of standard feature types are \"CDS\", \"start_codon\", \"stop_codon\", and \"exon\".\n4. start - The starting position of the feature in the sequence. The first base is numbered 1.\n5. end - The ending position of the feature (inclusive).\n6. score - A score between 0 and 1000. If there is no score value, enter \".\".\n7. strand - Valid entries include '+', '-', or '.' (for don't know/care).\n8. frame - If the feature is a coding exon, frame should be a number between 0-2 that represents the reading frame of the first base. If the feature is not a coding exon, the value should be '.'.\n9. group - All lines with the same group are linked together into a single item.\n"
    },
    {
        "name": "MAF to BED",
        "description": "Converts a MAF formatted file to the BED format",
        "category": ["Convert Formats"],
         "help": "This tool converts every MAF block to an interval line (in BED format) describing the position of that alignment block within a corresponding genome. /n Step 1 of 2: Choose multiple alignments from history to be converted to BED format. /n Step 2 of 2: Choose species from the alignment to be included in the output and specify how to deal with alignment blocks that lack one or more species. /n Choose species: The tool reads the alignment provided during Step 1 and generates a list of species contained within that alignment. Using checkboxes, you can specify taxa to be included in the output (only reference genome, shown in bold, is selected by default). If you select more than one species, then more than one history item will be created. /n Include/exclude blocks with missing species: If an alignment block does not contain any one of the species you selected and this option is set to exclude blocks with missing species, then coordinates of such a block will not be included in the output. /n Example 1: Include only reference genome (hg18 in this case) and include blocks with missing species. For the following alignment: s hg18.chr20 56827368 75 + 62435964 GACAGGGTGCATCTGGGAGGG---... The tool will create a single history item: chr20 56827368 56827443 hg18_0 0 + chr20 56827443 56827480 hg18_1 0 + /n Example 2: Include hg18 and mm8 and exclude blocks with missing species. For the following alignment: s hg18.chr20 56827368 75 + 62435964 GACAGGGTGCATCTGGGAGGG---... The tool will create two history items: History item 1 (for hg18): chr20 56827368 56827443 hg18_0 0 + History item 2 (for mm8): chr2 173910832 173910893 mm8_0 0 + /n MAF format: Multiple Alignment Format file. Stores multiple alignments at the DNA level between entire genomes. Each alignment ends with a blank line. Each sequence is on a single line. Lines starting with # are comments. Each alignment block starts with an 'a' line followed by 's' lines for each sequence. /n BED format: Browser Extensible Data format used for displaying data in the UCSC Genome Browser. Required fields: 1. chrom - Chromosome name (e.g., chr1) 2. chromStart - Start position (0-based) 3. chromEnd - End position (exclusive). Optional fields: 4. name - Name of the item 5. score - Score between 0 and 1000 6. strand - Either '+' or '-'"
    },
    {
        "name": "MAF to Interval",
        "description": "Converts a MAF formatted file to the Interval format",
        "category": ["Convert Formats"],
        "help": "This tool converts every MAF block to a set of genomic intervals describing the position of that alignment block within a corresponding genome. Sequences from aligning species are also included in the output. The interface for this tool contains several options: MAF file to convert. Choose multiple alignments from history to be converted to BED format. Choose species. Choose additional species from the alignment to be included in the output. Exclude blocks which have a species missing. if an alignment block does not contain any one of the species found in the alignment set and this option is set to exclude blocks with missing species, then coordinates of such a block will not be included in the output (see Example 2 below). Remove Gap characters from sequences. Gaps can be removed from sequences before they are output. Example 1: Include only reference genome (hg18 in this case) and include blocks with missing species: For the following alignment: a score=68686.000000 s hg18.chr20 56827368 75 + 62435964 GACAGGGTGCATCTGGGAGGG---CCTGCCGGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- s panTro2.chr20 56528685 75 + 62293572 GACAGGGTGCATCTGAGAGGG---CCTGCCAGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- s rheMac2.chr10 89144112 69 - 94855758 GACAGGGTGCATCTGAGAGGG---CCTGCTGGGCCTTTG-TTCAAAACTAGATATGCCCCAACTCCAATTCTA------- s mm8.chr2 173910832 61 + 181976762 AGAAGGATCCACCT------------TGCTGGGCCTCTGCTCCAGCAAGACCCACCTCCCAACTCAAATGCCC------- s canFam2.chr24 46551822 67 + 50763139 CG------GCGTCTGTAAGGGGCCACCGCCCGGCCTGTG-CTCAAAGCTACAAATGACTCAACTCCCAACCGA------C a score=10289.000000 s hg18.chr20 56827443 37 + 62435964 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG s panTro2.chr20 56528760 37 + 62293572 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG s rheMac2.chr10 89144181 37 - 94855758 ATGTGCGGAAAATGTGATACAGAAACCTGCAGAGCAG the tool will create a single history item containing the following (note the name field is numbered iteratively: hg18_0_0, hg18_1_0 etc. where the first number is the block number and the second number is the iteration through the block (if a species appears twice in a block, that interval will be repeated) and sequences for each species are included in the order specified in the header: the field is left empty when no sequence is available for that species): #chrom start end strand score name canFam2 hg18 mm8 panTro2 rheMac2 chr20 56827368 56827443 + 68686.0 hg18_0_0 CG------GCGTCTGTAAGGGGCCACCGCCCGGCCTGTG-CTCAAAGCTACAAATGACTCAACTCCCAACCGA------C GACAGGGTGCATCTGGGAGGG---CCTGCCGGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- AGAAGGATCCACCT------------TGCTGGGCCTCTGCTCCAGCAAGACCCACCTCCCAACTCAAATGCCC------- GACAGGGTGCATCTGAGAGGG---CCTGCCAGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- GACAGGGTGCATCTGAGAGGG---CCTGCTGGGCCTTTG-TTCAAAACTAGATATGCCCCAACTCCAATTCTA------- chr20 56827443 56827480 + 10289.0 hg18_1_0 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG ATGTGCGGAAAATGTGATACAGAAACCTGCAGAGCAG Example 2: Include hg18 and mm8 and exclude blocks with missing species: For the following alignment: a score=68686.000000 s hg18.chr20 56827368 75 + 62435964 GACAGGGTGCATCTGGGAGGG---CCTGCCGGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- s panTro2.chr20 56528685 75 + 62293572 GACAGGGTGCATCTGAGAGGG---CCTGCCAGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- s rheMac2.chr10 89144112 69 - 94855758 GACAGGGTGCATCTGAGAGGG---CCTGCTGGGCCTTTG-TTCAAAACTAGATATGCCCCAACTCCAATTCTA------- s mm8.chr2 173910832 61 + 181976762 AGAAGGATCCACCT------------TGCTGGGCCTCTGCTCCAGCAAGACCCACCTCCCAACTCAAATGCCC------- s canFam2.chr24 46551822 67 + 50763139 CG------GCGTCTGTAAGGGGCCACCGCCCGGCCTGTG-CTCAAAGCTACAAATGACTCAACTCCCAACCGA------C a score=10289.000000 s hg18.chr20 56827443 37 + 62435964 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG s panTro2.chr20 56528760 37 + 62293572 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG s rheMac2.chr10 89144181 37 - 94855758 ATGTGCGGAAAATGTGATACAGAAACCTGCAGAGCAG the tool will create two history items (one for hg18 and one for mm8) containing the following (note that both history items contain only one line describing the first alignment block. The second MAF block is not included in the output because it does not contain mm8): History item 1 (for hg18): #chrom start end strand score name canFam2 hg18 mm8 panTro2 rheMac2 chr20 56827368 56827443 + 68686.0 hg18_0_0 CG------GCGTCTGTAAGGGGCCACCGCCCGGCCTGTG-CTCAAAGCTACAAATGACTCAACTCCCAACCGA------C GACAGGGTGCATCTGGGAGGG---CCTGCCGGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- AGAAGGATCCACCT------------TGCTGGGCCTCTGCTCCAGCAAGACCCACCTCCCAACTCAAATGCCC------- GACAGGGTGCATCTGAGAGGG---CCTGCCAGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- GACAGGGTGCATCTGAGAGGG---CCTGCTGGGCCTTTG-TTCAAAACTAGATATGCCCCAACTCCAATTCTA------- History item 2 (for mm8): #chrom start end strand score name canFam2 hg18 mm8 panTro2 rheMac2 chr2 173910832 173910893 + 68686.0 mm8_0_0 CG------GCGTCTGTAAGGGGCCACCGCCCGGCCTGTG-CTCAAAGCTACAAATGACTCAACTCCCAACCGA------C"
    },
    {
        "name": "MAF to FASTA",
        "description": "Converts a MAF formatted file to FASTA format",
        "category": ["Convert Formats"],
         "help": "This tool converts MAF blocks to FASTA format and concatenates them into a single FASTA block or outputs multiple FASTA blocks separated by empty lines. /n The interface for this tool contains two pages (steps): /n Step 1 of 2. Choose multiple alignments from history to be converted to FASTA format. /n Step 2 of 2. Choose the type of output as well as the species from the alignment to be included in the output. /n Multiple Block output has additional options: /n Choose species - the tool reads the alignment provided during Step 1 and generates a list of species contained within that alignment. Using checkboxes you can specify taxa to be included in the output (all species are selected by default). /n Choose to include/exclude blocks with missing species - if an alignment block does not contain any one of the species you selected within Choose species menu and this option is set to exclude blocks with missing species, then such a block will not be included in the output. /n For example, if you want to extract human, mouse, and rat from a series of alignments and one of the blocks does not contain mouse sequence, then this block will not be converted to FASTA and will not be returned."
    },
    {
        "name": "SFF converter",
        "description": "",
        "category": ["Convert Formats"],
        "help": "This tool extracts data from the 454 Sequencer SFF format and creates three files containing the: Sequences (FASTA), Qualities (QUAL) and Clippings (XML)."
    },
    {
        "name": "BED-to-bigBed",
        "description": "converter",
        "category": ["Convert Formats"],
        "help": "This tool converts a sorted BED file into a bigBed file.\n\nWhat it does\n1. **Functionality**\n   - Transforms a BED file into the binary bigBed format for efficient storage and retrieval.\n   - Requires the input BED file to be sorted.\n\n2. **Usage**\n   - Provide a sorted BED file as input.\n   - The tool will generate a bigBed file as output.\n\n3. **Limitations**\n   - The `bedFields` option to specify non-standard fields is not supported.\n   - An AutoSQL file is required but is currently not supported in Galaxy."

    },
    {
        "name": "Filter",
        "description": "data on any column using simple expressions",
        "category": ["Filter and Sort"],
        "help": "Double equal signs, ==, must be used as 'equal to' (e.g., c1 == 'chr22'). TIP: Attempting to apply a filtering condition may throw exceptions if the data type (e.g., string, integer) in every line of the columns being filtered is not appropriate for the condition. TIP: If your data is not TAB delimited, use Text Manipulation->Convert. Syntax: Columns are referenced with c and a number (e.g., c1 refers to the first column). Make sure multi-character operators contain no white space (e.g., <= is valid while < = is not). Use '==' for equal-to conditions (e.g., c1=='chr1'). Non-numerical values must be in quotes (e.g., c6=='+'). Filtering conditions can include logical operators, all in lowercase (e.g., (c1!='chrX' and c1!='chrY') or not c6=='+'). Example: c1=='chr1' selects lines in which the first column is chr1. c3-c2<100*c4 selects lines where subtracting column 3 from column 2 is less than column 4 times 100. len(c2.split(',')) < 4 selects lines where the second column has less than four comma-separated elements. c2>=1 selects lines where column 2 is greater than or equal to 1. Numbers should not contain commas (e.g., c2<=44554350). Some words in the data can be used but must be quoted (e.g., c3=='exon')."
    },
    {
        "name": "Sort",
        "description": "data in ascending or descending order",
        "category": ["Filter and Sort"],
        "help": "This tool sorts the dataset on any number of columns in ascending or descending order. Numerical sort orders by magnitude, ignoring characters except numbers. General numeric sort handles numbers in scientific notation. Alphabetical sort follows the conventional letter order. Examples: Sorting 4,17,3,5 numerically results in 3,4,5,17, while alphabetically it results in 17,3,4,5. Sorting columns 1 (alphabetical), 3 (numerical), and 6 (numerical) in ascending order yields a specific order of rows based on these columns. Sorting a dataset of chromosomes by column 5 in ascending order results in an order based on their numerical or general numeric values."
    },
    {
        "name": "Select",
        "description": "lines that match an expression",
        "category": ["Filter and Sort"],
        "help": "The select tool searches data for lines matching or not matching a given regular expression pattern. Special characters include ( ) { } [ ] . * ? + ^ $ which have specific meanings, such as matching the beginning (^), end ($), digits (\\d), or whitespace (\\s). Regular expressions allow for pattern matching with conditions like repetition ({n,m}) or alternation (|). Examples: ^chr([0-9A-Za-z])+ matches lines starting with chromosomes, (ACGT){1,5} matches 1 to 5 consecutive 'ACGT', and (abc)|(def) matches either 'abc' or 'def'. Comments can be matched using ^\\W+#."
    },
    {
        "name": "Extract features",
        "description": "from GFF data",
        "category": ["Filter and Sort"],
        "help": "This tool extracts selected features from GFF (General Feature Format) data, which describes genes and other features associated with DNA, RNA, and protein sequences. Each GFF line contains nine tab-separated fields: seqname, source, feature, start, end, score, strand, frame, and group. For example, selecting the 'promoter' feature from a dataset will filter the lines that match this feature type. Example input with the 'promoter' feature: chr22 GeneA promoter 10010000 10010100 900 + . TGA; output will include only the selected promoter lines." 
    },
    {
        "name": "Filter GFF data by attribute",
        "description": "using simple expressions",
        "category": ["Filter and Sort"],
        "help": "The Filter GFF Data by Attribute tool allows you to filter GFF data based on specific conditions applied to an attribute in the data. \n\n### Syntax for Filtering:\n\n1. **Equality Operator (==)**: \n   - Use double equal signs (`==`) to check for equality. \n   - Example: `c1 == 'chr22'` will match rows where the `c1` attribute equals `'chr22'`.\n\n2. **Multi-Character Operators**: \n   - Operators like `<=`, `>=`, `<`, `>`, `!=` must not have spaces between the characters. \n   - Example: `c1 <= 100` is valid, but `c1 < = 100` is **invalid**.\n\n3. **Non-Numerical Values**: \n   - Non-numeric values should be enclosed in single or double quotes. \n   - Example: `attribute_name == 'XX22'`.\n\n4. **Combining Multiple Conditions**: \n   - Use logical operators to combine multiple conditions. \n   - **`and`**: Both conditions must be true. \n   - **`or`**: At least one condition must be true. \n   - Example: `attribute_name == 'XX22' or attribute_name == 'XX21'`.\n\n### Tips: \n1. **Data Type Compatibility**: \n   - Ensure that the data type in each column is appropriate for the condition you're applying. For example, you can't apply numerical operations to strings. \n   - If a data type mismatch occurs, those lines will be skipped and a Condition data issue will be reported.\n\n2. **Handling Non-TAB Delimited Data**: \n   - If your data is not separated by tabs (e.g., comma-separated), use the **Text Manipulation -> Convert** tool to convert it to TAB-delimited format before filtering.\n\n### Examples of Filter Conditions:\n\n- **Equality Check**: `c1 == 'chr22'`: Selects rows where `c1` equals `'chr22'`.\n- **Greater than**: `c2 > 500`: Selects rows where `c2` is greater than 500.\n- **Less than or Equal**: `c3 <= 1000`: Selects rows where `c3` is less than or equal to 1000.\n- **Multiple Conditions (AND/OR)**: \n   - `attribute_name == 'XX22' and value > 100`: Selects rows where `attribute_name` is `'XX22'` and `value` is greater than 100.\n   - `attribute_name == 'XX22' or attribute_name == 'XX21'`: Selects rows where `attribute_name` is either `'XX22'` or `'XX21'."
    },
    {
        "name": "Filter GFF data by feature count",
        "description": "using simple expressions",
        "category": ["Filter and Sort"],
        "help": "The Filter GFF Data by Feature Count tool allows you to restrict your GFF dataset based on the number of occurrences of specific features. This tool is designed to filter out entries based on how frequently a particular feature appears in the dataset. This is especially useful when you need to focus on genes or transcripts with certain characteristics or feature counts.\n\n### Syntax for Filtering by Feature Count:\n\n1. **Equality Operator (==)**: \n   - Use double equal signs (`==`) to check if a feature count is exactly equal to a specified value.\n   - Example: `feature_count == 10` will match rows where the feature count is exactly 10.\n\n2. **Comparison Operators**: \n   - You can use comparison operators like `>`, `<`, `>=`, `<=`, and `!=` to filter features based on their count. \n   - Example: `feature_count > 50` will match rows where the feature count is greater than 50.\n\n3. **Combining Multiple Conditions**: \n   - Use logical operators like **`and`** and **`or`** to combine multiple filtering conditions. \n   - Example: `feature_count >= 100 and feature_count <= 500` will select rows where the feature count is between 100 and 500.\n\n4. **Non-Numeric Feature Counts**: \n   - In most cases, feature counts are numeric. Ensure that the column you are applying the filter to contains numeric data.\n   - Example: `feature_count == 0` would select rows where the feature count is exactly 0.\n\n### Tips: \n1. **Data Type Compatibility**: \n   - Ensure that the feature count column contains numerical values before applying numerical comparisons. Otherwise, you might get data type errors or invalid rows.\n\n2. **Handling Non-TAB Delimited Data**: \n   - If your data is not TAB-delimited, use the **Text Manipulation -> Convert** tool to convert it into a TAB-delimited format before filtering.\n\n### Examples of Filter Conditions:\n\n- **Feature Count Equality**: `feature_count == 10`: Selects rows where the feature count is exactly 10.\n- **Feature Count Greater Than**: `feature_count > 100`: Selects rows where the feature count is greater than 100.\n- **Feature Count Range**: `feature_count >= 50 and feature_count <= 200`: Selects rows where the feature count is between 50 and 200.\n- **Multiple Conditions (AND/OR)**: \n   - `feature_count > 100 and feature_count < 500`: Selects rows where the feature count is between 100 and 500.\n   - `feature_count == 0 or feature_count == 10`: Selects rows where the feature count is either 0 or 10."
    },
    {
        "name": "Filter GTF data by attribute values_list",
        "description": "",
        "category": ["Filter and Sort"],
        "help": "This tool filters a GTF (General Transfer Format) file based on a list of attribute values. The attribute values are taken from the first column of the GTF file, while the additional columns in the file are ignored during the filtering process. The primary use case for this tool is to filter a GTF file using a list of specific `transcript_ids`, `gene_ids`, or other attributes, such as those obtained from tools like Cuffdiff. This allows users to focus on specific genes or transcripts of interest.\n\n### Key Functionality:\n\n1. **Attribute-Based Filtering**: \n   - The filtering condition is applied to the first column in the GTF file, which typically contains attribute values like `transcript_id` or `gene_id`. \n   - Example: You may filter for entries that correspond to certain `transcript_ids` or `gene_ids`.\n\n2. **File Format**: \n   - GTF files are tab-delimited text files used to describe the structure of genes and transcripts. The first column typically represents the sequence name, and subsequent columns describe feature attributes such as `gene_id` and `transcript_id`.\n\n3. **Use Cases**: \n   - **Filter by `transcript_id`**: Filter the GTF file to select only rows that match a specific list of `transcript_ids` (e.g., obtained from Cuffdiff results).\n   - **Filter by `gene_id`**: Select entries with specific `gene_ids` to narrow down the focus to genes of interest.\n\n4. **Handling Multiple Values**: \n   - You can provide a list of attribute values to filter for multiple IDs. The tool will match any row where the value in the first column matches one of the provided attribute values.\n\n### Example Filtering Conditions:\n\n- **Filter by `transcript_id`**: `transcript_id == 'T01'`: Selects rows where the `transcript_id` is exactly `T01`.\n- **Filter by Multiple `gene_ids`**: `gene_id == 'G01' or gene_id == 'G02'`: Selects rows where the `gene_id` is either `G01` or `G02`.\n\n### Tips: \n1. **Attribute Format**: \n   - Make sure that the attribute names you are filtering by (e.g., `transcript_id`, `gene_id`) exactly match the corresponding field names in your GTF file. This ensures accurate filtering.\n\n2. **Non-TAB Delimited Data**: \n   - If your data is not in a TAB-delimited format, consider using the **Text Manipulation -> Convert** tool to reformat it before applying the filter.\n\n3. **Filter Condition Examples**: \n   - `transcript_id == 'T01'`: Selects rows with `transcript_id` equal to `T01`.\n   - `gene_id == 'G01' or gene_id == 'G02'`: Selects rows where `gene_id` is either `G01` or `G02`.\n   - `transcript_id == 'T03'`: Selects rows with `transcript_id` equal to `T03`."
    },
    {
        "name": "Join two Datasets",
        "description": "side by side on a specified field",
        "category": ["Join, Subtract and Group"],
        "help": "This tool allows you to **join** two datasets based on a common field. It enables you to combine the data from both datasets into a single dataset by matching values in a specified column from each dataset. **Empty strings ('')** are not valid identifiers for joining.\n\n### Key Features:\n\n1. **Common Field**: \n   - The tool joins lines from two datasets where the values in the specified columns match.\n   - The columns are referenced by their number (e.g., 1st column, 2nd column, etc.).\n\n2. **Joining Option**: \n   - You can choose to include only the matching lines from both datasets, or you can keep all lines from the first dataset, even if there is no match in the second dataset.\n\n3. **Invalid Identifiers**: \n   - An empty string ('') cannot be used as a valid identifier for joining. Ensure that the fields you are joining on contain meaningful, non-empty values.\n\n### Example Datasets:\n**Dataset1:**\n```\nchr1  10  20  geneA\nchr1  50  80  geneB\nchr5  10  40  geneL\n```\n**Dataset2:**\n```\ngeneA  tumor-suppressor\ngeneB  Foxp2\ngeneC  Gnas1\ngeneE  INK4a\n```\n\n### Example 1: Joining on the 4th column of Dataset1 and the 1st column of Dataset2\n\nJoining the 4th column from Dataset1 (which contains gene identifiers) with the 1st column of Dataset2 (which also contains gene identifiers) results in:\n```\nchr1  10  20  geneA  geneA  tumor-suppressor\nchr1  50  80  geneB  geneB  Foxp2\n```\n\n### Example 2: Including non-matching lines from Dataset1\n\nIf you choose to keep all lines from Dataset1, even those that do not match any line in Dataset2, the result will be:\n```\nchr1  10  20  geneA  geneA  tumor-suppressor\nchr1  50  80  geneB  geneB  Foxp2\nchr5  10  40  geneL\n```\n\n### Key Notes:\n\n- **Joining Columns**: Ensure that the column you are using to join both datasets contains matching values. The column numbers are used to refer to the fields in the datasets (e.g., 4 for the 4th column).\n- **Inclusion of Non-Matching Rows**: You can decide whether to include rows from the first dataset even when no match is found in the second dataset.\n- **Empty Identifiers**: If the field to join on contains an empty string, it will be skipped, as empty strings are not valid identifiers.\n\n### Tips:\n1. **Choosing the Correct Columns**: \n   - Double-check the columns you are joining on. You should choose columns that contain common identifiers or values that can meaningfully match across both datasets.\n\n2. **Handling Non-Matching Data**: \n   - If you expect that not all rows will match, be sure to choose the option to include rows from the first dataset that do not have corresponding matches in the second dataset.\n\n3. **Data Quality**: \n   - Ensure that the data in the columns you are joining on are clean and formatted correctly. This ensures a successful join without issues like missing data or unexpected results."
    },
    {
        "name": "Compare two Datasets",
        "description": "to find common or distinct rows",
        "category": ["Join, Subtract and Group"],
        "help": "This tool compares two datasets based on a specific field, allowing users to find lines in one dataset that **have** or **do not have** a common field with another dataset. The comparison is made between the specified columns of the two datasets. This is useful for identifying matches or differences between datasets that share common attributes.\n\n### Key Functionality:\n\n1. **Matching Rows**: \n   - The tool identifies rows in the first dataset where the value in a specified column (e.g., the 4th column) matches the value in a specific column (e.g., the 1st column) of the second dataset.\n   - Example: Find rows in the first dataset where the `gene` value (from the 4th column) matches a `gene` in the second dataset (from the 1st column).\n\n2. **Non-Matching Rows**: \n   - Alternatively, the tool can find rows in the first dataset where the value in the specified column does **not** match any value in the second dataset's corresponding column.\n   - Example: Find rows in the first dataset where the `gene` value does **not** match any `gene` in the second dataset.\n\n3. **Field Comparison**: \n   - The comparison is based on the field values provided by the user. You can specify which columns of the datasets to compare, enabling flexible matching.\n\n### Example Datasets:\n\n**First dataset**:\n```\nchr1  10  20  geneA\nchr1  50  80  geneB\nchr5  10  40  geneL\n```\n**Second dataset**:\n```\ngeneA  tumor-suppressor\ngeneB  Foxp2\ngeneC  Gnas1\ngeneE  INK4a\n``` \n\n4. **Use Cases**:\n   - **Finding Matching Rows**: If you want to find rows in the first dataset where the gene (4th column) matches a gene in the second dataset (1st column), the result would be:\n   ```\n   chr1  10  20  geneA\n   chr1  50  80  geneB\n   ```\n   - **Finding Non-Matching Rows**: If you use the 'Non-Matching' option, the result would be the rows from the first dataset whose gene values do not match any gene in the second dataset (e.g., `geneL` would not have a match in the second dataset).\n   ```\n   chr5  10  40  geneL\n   ```\n\n### Tips: \n1. **Matching Fields**: \n   - Make sure that the columns you want to compare from both datasets contain comparable data types (e.g., strings or numerical values) for accurate matching.\n\n2. **Custom Column Selection**: \n   - The tool allows you to specify which columns to compare between the two datasets, giving you full control over which fields are used for matching or non-matching filtering.\n\n3. **Handling Multiple Columns**: \n   - In some cases, you may want to compare multiple columns from each dataset. Ensure the column selections are consistent for an effective comparison."
    },
    {
        "name": "Group",
        "description": "data by a column and perform aggregate operation on other columns.",
        "category": ["Join, Subtract and Group"],
         "help": "This tool allows you to **group** the input dataset by a particular column and perform various **aggregate functions** on any specified column(s). The available aggregate functions include **Mean**, **Median**, **Mode**, **Sum**, **Max**, **Min**, **Count**, **Concatenate**, and **Randomly pick**.\n\n### Key Functions:\n\n1. **Grouping**: \n   - The tool groups the dataset by the values in a specified column. Once grouped, you can perform aggregate functions on the columns of interest within each group.\n\n2. **Aggregate Functions**: \n   - **Mean**: Computes the average value of the selected column within each group.\n   - **Median**: Finds the middle value when the values are ordered within each group.\n   - **Mode**: Identifies the most frequent value(s) within each group. If multiple modes exist, all are reported.\n   - **Sum**: Adds up the values in the specified column for each group.\n   - **Max**: Finds the maximum value in the specified column within each group.\n   - **Min**: Finds the minimum value in the specified column within each group.\n   - **Count**: Counts the number of items in the specified column for each group.\n   - **Concatenate**: Builds a comma-delimited list of values in the specified column for each group.\n   - **Concatenate Unique**: Builds a list of unique values, removing duplicates, and joining them with commas.\n   - **Randomly Pick**: Picks a random value from the specified column for each group.\n\n3. **Count and Count Unique**: \n   - **Count** counts the total number of items in a specified column for each group.\n   - **Count Unique** counts only the unique items in the specified column for each group.\n\n4. **Concatenate vs Count**: \n   - **Concatenate** and **Concatenate Unique** will return a comma-separated list of items for each group, whereas **Count** and **Count Unique** return the count of items (either total or unique).\n\n### Example Input:\n```\nchr22  1000  1003  TTT\nchr22  2000  2003  aaa\nchr10  2200  2203  TTT\nchr10  1200  1203  ttt\nchr22  1600  1603  AAA\n```\n\n### Example Grouping and Aggregation:\n\n1. **Grouping on Column 4 while ignoring case** and performing **Count** on Column 1 will return:\n```\nAAA    2\nTTT    3\n```\n   - Here, `AAA` and `TTT` are grouped and counted for occurrences.\n\n2. **Grouping on Column 4 while not ignoring case** and performing **Count** on Column 1 will return:\n```\naaa    1\nAAA    1\nttt    1\nTTT    2\n```\n   - The case-sensitive grouping results in distinct counts for `aaa`, `AAA`, `ttt`, and `TTT`.\n\n### Concatenate and Concatenate Unique: \n   - **Concatenate**: Combines the values in the specified column for each group, separated by commas.\n   - **Concatenate Unique**: Combines only unique values in the specified column for each group.\n   - Example: If Column 1 contains values `1, 1, 2, 3` for a group, the **Concatenate** function will return `1, 1, 2, 3`, while **Concatenate Unique** will return `1, 2, 3`.\n\n### Tips: \n1. **Handling Case Sensitivity**: \n   - The tool allows the option to **ignore case** while performing operations like **Count** and **Concatenate**, which can be helpful for grouping strings regardless of their case.\n\n2. **Multiple Aggregate Functions**: \n   - You can apply multiple aggregate functions to different columns within the same dataset. For example, you can calculate the **Sum** for one column while finding the **Mean** for another.\n\n3. **Handling Non-Numeric Data**: \n   - Functions like **Mean**, **Median**, **Sum**, **Max**, and **Min** are typically used with numeric columns. Ensure your columns contain the appropriate data types for accurate results.\n\n4. **Randomly Picking Values**: \n   - The **Randomly pick** function allows for randomly selecting a value from each group. This can be useful for sampling or testing purposes."
    },
    {
        "name": "Extract Pairwise MAF blocks",
        "description": "given a set of genomic intervals",
        "category": ["Fetch Alignments/Sequences"],
        "help": "### What it does\n\nThis tool extracts pairwise alignment blocks from a MAF (Multiple Alignment Format) file based on a set of genomic coordinates provided by the user. It retrieves alignment blocks that overlap with specified intervals and trims them to match the given coordinates when necessary.\n\n### How it works\n\n- **Input**:\n  1. A list of genomic intervals (typically in BED format or similar): each line specifies a region by its chromosome, start, and end positions.\n  2. A MAF file containing pairwise alignments.\n\n- **Output**:\n  - A new MAF file containing only those alignment blocks that overlap the specified genomic intervals. Any blocks that extend beyond the interval boundaries are trimmed accordingly.\n\n### Key Features\n\n- **Precise trimming**: If an alignment block extends past the START and/or END position of the interval, it is trimmed so that only the relevant region is included.\n- **Multiple block retrieval**: A single genomic interval may correspond to multiple MAF blocks. The tool retrieves and processes all blocks that overlap with the interval.\n- **Support for pairwise alignments**: This tool focuses on extracting blocks from pairwise MAF alignments available on the Galaxy site.\n\n### Example\n\n#### Input genomic interval:\n```\nchr1   1000   2000\n```\n\n#### Superimposed on three MAF blocks:\n- **Block 1** overlaps partially with the interval and is trimmed at the start.\n- **Block 2** lies completely within the interval and is included in full.\n- **Block 3** overlaps partially and is trimmed at the end.\n\n#### Output MAF blocks:\n```\nTrimmed Block 1 (starts at position 1000)\nFull Block 2\nTrimmed Block 3 (ends at position 2000)\n```\n\n### Common use cases\n\n- Extracting alignment data for specific genomic regions of interest.\n- Preparing custom datasets for focused comparative genomics analysis.\n- Reducing the size of MAF files by including only relevant alignment blocks.\n\n### Tips\n\n- Ensure your input intervals are correctly formatted and correspond to the reference genome used in the MAF alignments.\n- This tool works with pairwise alignments and may not be suitable for multi-species MAF files.\n- Trimmed blocks maintain the integrity of alignment coordinates relevant to the input intervals.\n\n### Input Requirements\n\n- Genomic intervals: Provide coordinates that specify the regions you want to extract from the MAF file.\n- Pairwise MAF file: Ensure it matches the genome assembly of your intervals and is formatted correctly."
    },
    {
        "name": "Split MAF blocks",
        "description": "by Species",
        "category": ["Fetch Alignments/Sequences"],
        "help": "### What It Does\n\nThe **Split MAF Block Species** tool analyzes each block in a MAF (Multiple Alignment Format) file to identify cases where a single block contains multiple sequences from the same species. When this occurs, it splits the block into separate blocks, ensuring that each block contains only one sequence per species.\n\nThe goal is to generate all possible combinations of sequences where each species is represented exactly once per block.\n\n### How It Works\n\n1. Scans each MAF block in the input file.\n2. Detects if any species appears more than once in the block.\n3. Splits the block into multiple new blocks.\n   - Each new block includes exactly one sequence per species.\n   - All possible combinations of these sequences are represented.\n4. Optionally, alignment columns that only contain gaps (\"empty columns\") can be removed from the new blocks.\n\n### Example\n\n#### Original MAF Block (containing multiple sequences for `hg18`):\n```\na score=5000\ns hg18.chr1 100 50 + 247249719 ACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTAC\ns hg18.chr2 500 50 + 243199373 TGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATG\ns panTro2.chr1 150 50 + 228333871 ACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTA\n```\n\n#### Split into two new MAF blocks:\n```\na score=5000\ns hg18.chr1 100 50 + 247249719 ACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTAC\ns panTro2.chr1 150 50 + 228333871 ACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTA\n\na score=5000\ns hg18.chr2 500 50 + 243199373 TGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATG\ns panTro2.chr1 150 50 + 228333871 ACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTA\n```\n\n### Inputs\n\n- **MAF file to split**\n  - Select a MAF file containing multiple alignments that you want to split by species.\n\n- **Collapse empty alignment columns** (optional)\n  - If enabled, alignment columns that contain only gaps in the new blocks will be removed, producing cleaner and more compact alignments.\n\n### Output\n\n- A new MAF file where each block contains only one sequence per species.\n- All possible combinations of unique species sequences are represented as separate alignment blocks.\n\n### Use Cases\n\n- Preparing MAF blocks for downstream analyses that require unique species representation per block.\n- Simplifying complex MAF files with duplicate species entries.\n- Removing unnecessary gaps from alignments by collapsing empty columns.\n\n### Notes\n\n- The `collapse empty alignment columns` option is particularly useful for eliminating columns that result from splitting blocks and no longer contain meaningful data.\n- Header lines and scores are preserved in the new blocks.\n- This tool can greatly increase the number of alignment blocks if there are many combinations to represent."
    },
    {
        "name": "Stitch MAF blocks",
        "description": "given a set of genomic intervals",
        "category": ["Fetch Alignments/Sequences"],
        "help": "### What It Does\n\nThe **Stitch MAF Blocks** tool combines multiple MAF alignment blocks that overlap a given genomic region. It is commonly used when a single genomic region is covered by several MAF blocks, and you wish to stitch them together into a continuous alignment. This tool accepts a list of genomic intervals and performs the following:\n\n### How It Works\n\n1. **Input** a list of genomic intervals, specifying the regions you want to stitch.\n2. **Finds** all MAF blocks that overlap each specified interval.\n3. **Sorts** the overlapping MAF blocks by their alignment score (higher scores are preferred).\n4. **Stitches** the blocks together:\n   - Resolves any overlaps between the blocks based on the alignment score.\n   - Fills any gaps between blocks (if applicable) with gaps.\n5. **Outputs** the stitched alignment in **FASTA format**, with continuous sequences for each region.\n\n### Example Workflow\n\n#### Input:\n- **Genomic intervals** (e.g., `chr1:1000-2000`, `chr2:3000-3500`).\n- **MAF alignment blocks** overlapping these intervals.\n\n#### The tool will:\n- Find all MAF blocks that overlap `chr1:1000-2000`.\n- Sort them by alignment score.\n- Stitch them together by resolving overlaps and filling gaps between blocks.\n- Output the stitched block as a single sequence in FASTA format.\n\n### Example\n\nGiven the following three MAF blocks overlapping a genomic region:\n\n```\nBlock 1: chr1 1000-1500 (alignment)\nBlock 2: chr1 1500-2000 (alignment)\nBlock 3: chr1 2000-2500 (alignment)\n```\n\nThe tool stitches these blocks, filling any gaps between blocks 2 and 3 with gaps, producing a continuous alignment:\n```\nFASTA Output: chr1 1000-2500\n>stiched_alignment\nATGCGT...TTAGGAGGGTAA---GAGGCGTGG...GGGAGGAGGA\n```"
    },
    {
        "name": "Stitch Gene blocks",
        "description": "given a set of coding exon intervals",
        "category": ["Fetch Alignments/Sequences"],
         "help": "### What It Does\n\nThe **Stitch Gene Block** tool reconstructs the complete coding sequences of genes by stitching together multiple MAF alignment blocks that overlap the coding exons of those genes.\n\nGenes typically consist of multiple coding exons, each corresponding to separate genomic regions. These regions can be fragmented across different MAF alignment blocks. This tool combines these fragments to produce continuous, high-quality alignments for each gene.\n\n### How It Works\n\n1. **Input** a list of gene intervals in **Gene BED format**, specifying the coding regions (exons) for each gene.\n2. **Search** for all MAF blocks that overlap these coding regions.\n3. **Sort** the overlapping MAF blocks by their alignment score (higher scores first).\n4. **Stitch** the blocks together:\n   - Resolve overlaps between blocks, prioritizing sequences from the block with the highest alignment score.\n   - Concatenate blocks to form a continuous alignment covering the entire coding sequence.\n5. **Output** the stitched alignments in **FASTA format** for downstream analysis.\n\n### Example Workflow\n\n#### Inputs:\n- **Gene intervals** (in Gene BED format):\n  - Example line: `chr1 1000 2000 Gene1`\n- **MAF alignment blocks** covering these regions.\n\n#### The tool will:\n- Find all MAF blocks that overlap `chr1:1000-2000`.\n- Sort them by score.\n- Resolve overlaps by picking the higher-scoring alignment.\n- Output a stitched, continuous alignment of `Gene1` in FASTA format.\n\n### Inputs\n\n- **Gene BED file**\n  - Contains intervals corresponding to coding regions of genes.\n  - Example BED format:\n    ```\n    chrX 1000 1100 GeneA\n    chrX 1500 1600 GeneA\n    ```\n\n- **MAF file**\n  - Multiple alignment format file that contains alignment blocks overlapping the regions defined in the Gene BED file.\n\n### Outputs\n\n- **FASTA alignments**\n  - One stitched alignment per gene, spanning its coding regions.\n  - These alignments are suitable for downstream evolutionary or functional analyses.\n\n### Use Cases\n\n- Constructing complete coding sequence alignments for phylogenetic analysis.\n- Preparing alignments for positive selection tests (e.g., dN/dS analyses).\n- Extracting high-confidence, stitched alignments for gene regions across multiple species.\n\n### Notes\n\n- Overlapping regions between blocks are resolved in favor of the block with the highest alignment score to ensure quality.\n- If no MAF blocks overlap a region, no alignment will be produced for that gene.\n- This tool outputs sequences in FASTA format, not MAF format."
    },
    {
        "name": "MAF Coverage Stats",
        "description": "Alignment coverage information",
        "category": ["Fetch Alignments/Sequences"],
        "help": "### What It Does\n\nThe **MAF Coverage by Stats** tool analyzes a MAF (Multiple Alignment Format) file in conjunction with a user-provided interval file. It generates coverage statistics for each species within the specified intervals. The output details the number of nucleotides aligned and the number of gaps per species, either by interval or as a summary across all intervals.\n\n### How It Works\n\n1. **Input**:\n   - A **MAF file** containing multiple sequence alignments.\n   - An **interval file** listing genomic intervals of interest (format: `chromosome start end label`).\n\n2. The tool scans each specified interval across the MAF alignment blocks.\n3. For each species in the alignment:\n   - It calculates the number of aligned nucleotides.\n   - It calculates the number of gaps.\n4. If requested, it summarizes the total nucleotides and coverage across all intervals.\n\n### Example Workflow\n\n#### Interval File Example:\n```\nchrX 1000 1100 myInterval\n```\n\n#### Alignment Statistics Request for Species (H, M, and R):\n```\nchrX 1000 1100 myInterval H XXX YYY\nchrX 1000 1100 myInterval M XXX YYY\nchrX 1000 1100 myInterval R XXX YYY\n```\n- **XXX**: Number of aligned nucleotides for the species within the interval.\n- **YYY**: Number of gaps for the species within the interval.\n\n#### Summary Output Example (Optional):\n```\n#species   nucleotides   coverage\nhg18       30639         0.2372\nrheMac2    7524          0.0582\npanTro2    30390         0.2353\n```\n- **nucleotides**: Total aligned nucleotides across all intervals.\n- **coverage**: Proportion of nucleotides aligned versus the total length of all provided intervals.\n\n### Input\n- A **MAF file**.\n- An **interval file**, with entries in the format:\n  ```chromosome start end label ```\n- (Optional) Selection of species to report statistics for.\n\n### Output\n- Per-interval statistics for each species: nucleotides and gaps.\n- (Optional) A summary table showing total nucleotides and coverage per species across all intervals.\n\n### Use Cases\n- Evaluating alignment quality and coverage for specific genomic regions.\n- Comparing coverage depth across species.\n- Filtering poorly aligned regions in comparative genomics studies.\n\n### Notes\n- If a column (position) does not exist in the reference genome for an interval, it is not included in the output.\n- Coverage is computed as: `nucleotides / total interval length`."
    },
    {
        "name": "Join MAF blocks",
        "description": "by Species",
        "category": ["Fetch Alignments/Sequences"],
        "help": "### What It Does\n\nThe **Join MAF Blocks by Species** tool allows users to merge adjoining MAF (Multiple Alignment Format) blocks from a MAF file based on specified species. If the alignment blocks are contiguous in the selected species, they are joined into a single block. The tool also removes columns that contain only gaps and excludes any species not specified by the user.\n\n### How It Works\n\n1. The user provides a MAF file as input.\n2. The user specifies one or more species of interest.\n3. The tool identifies adjacent alignment blocks that are contiguous for the specified species.\n4. It merges these blocks into a single alignment block where applicable.\n5. Any columns that consist only of gaps are removed.\n6. Any species not explicitly specified are excluded from the output file.\n\n### Example\n\n#### Input MAF File (simplified example):\n```\na score=60426.000000\ns hg17.chr7    127471195 331 + 158628139 gtttgccatctttt...\ns panTro1.chr6 129885076 331 + 161576975 gtttgccatctttt...\ns mm5.chr6      28904571  357 + 149721531 CTCCACTCTCGTTT...\ns rn3.chr4      56178191  282 + 187371129 CTTCACTCTCATTT...\n\na score=8157.000000\ns hg17.chr7    127471526 58 + 158628139 AATTTGTGGTTTATT...\ns panTro1.chr6 129885407 58 + 161576975 AATTTGTGGTTTATT...\ns mm5.chr6      28904928  54 + 149721531 AA----CGTTTCATT...\n```\n\n#### User Specifies Species:\n```\nhg17, panTro1\n```\n\n#### Output (joined blocks):\n- The tool merges the two blocks for `hg17` and `panTro1`, trims gaps, and excludes `mm5` and `rn3` from the output.\n\n### Input\n- A MAF file with multiple alignment blocks.\n- A list of species (e.g., hg17, panTro1) to be included in the joined output.\n\n### Output\n- A filtered and merged MAF file containing only the specified species, with adjacent blocks joined where applicable.\n\n### Use Case\n- Useful for researchers focusing on specific species comparisons.\n- Helps simplify alignments by merging blocks that are contiguous in target species.\n\n### Notes\n- Contiguity is determined based on the coordinates and strands of the specified species.\n- Columns with only gaps are always removed to optimize the alignment blocks.\n- Non-specified species are excluded from the final output to reduce noise and focus on the species of interest."
    },
    {
        "name": "Filter MAF blocks",
        "description": "by Species",
        "category": ["Fetch Alignments/Sequences"],
        "help": "### What It Does\n\nThis tool allows you to filter and clean a MAF (Multiple Alignment Format) file by removing undesired species and alignment blocks that do not meet specific criteria.\n\nColumns that contain only gaps are removed automatically when a species is specified for filtering.\n\n### Filter Options\n\n- **Exclude blocks which have missing species**:\n  - Example: Suppose you want to restrict an 8-way alignment to human, mouse, and rat.\n  - The tool will first remove all other species from the alignment blocks.\n  - If this option is set to **YES**, the tool WILL NOT return MAF blocks that do not include all three species (human, mouse, and rat). As a result, all alignment blocks returned will have exactly those three sequences.\n\n- **Exclude blocks which have only one species**:\n  - If this option is set to **YES**, any MAF blocks that contain only a single species will NOT be returned.\n  - Useful for excluding uninformative single-species alignments from the output.\n\n### How It Works\n\n1. You provide a MAF file as input.\n2. You can specify a list of species you wish to retain in the alignment.\n3. The tool filters blocks based on your selection and the options chosen:\n   - Removing blocks missing any of your specified species (if enabled).\n   - Excluding blocks with only one species remaining (if enabled).\n4. Columns with gaps only are automatically removed when species filtering is applied.\n\n### Example Use Case\n\n- You are working with an 8-species alignment but only need human, mouse, and rat for further analysis.\n- This tool allows you to clean up the dataset by retaining only these species and removing blocks that don't include them all, resulting in a consistent 3-species alignment dataset.\n\n### Input\n\n- A MAF file (multiple alignments of genomic regions).\n\n### Output\n\n- A filtered MAF file according to the specified species and block inclusion rules."
    },
    {
        "name": "Filter MAF blocks",
        "description": "by Size",
        "category": ["Fetch Alignments/Sequences"],
        "help": "### What It Does\n\nThis tool filters a MAF (Multiple Alignment Format) file by extracting only those alignment blocks whose sizes fall within a specified range.\n\n### How It Works\n\n1. You provide a MAF file as input.\n2. You specify a size range by setting the minimum and maximum block sizes.\n3. The tool examines each alignment block in the MAF file and selects only those blocks whose size is within the defined range.\n4. The output is a filtered MAF file containing only the blocks that match your size criteria.\n\n### Use Case Example\n\n- You have a MAF file containing multiple alignment blocks of varying lengths.\n- You are interested in analyzing blocks that are between 100 and 500 base pairs in size.\n- By setting the minimum size to 100 and the maximum size to 500, the tool will output only the alignment blocks within that size range, discarding all others.\n\n### Input\n\n- A MAF file containing multiple alignment blocks.\n- A minimum size value (inclusive).\n- A maximum size value (inclusive).\n\n### Output\n\n- A filtered MAF file containing only alignment blocks with sizes within the specified range.\n\n### Notes\n\n- Block size is typically determined by the length of the alignment in the reference species or the first species listed in the block.\n- This tool is useful for focusing on alignment blocks of particular biological interest or for eliminating unusually large or small blocks that may skew downstream analyses."
    },
    {
        "name": "Extract MAF by block number",
        "description": "given a set of block numbers and a MAF file",
        "category": ["Fetch Alignments/Sequences"],
        "help": "### What it does\n\nThis tool extracts specific blocks from a Multiple Alignment Format (MAF) file using a provided list of block numbers. You supply a plain text list of block numbers, and the tool retrieves only the matching MAF alignment blocks from the input file.\n\n### How it works\n\n- **Input**:\n  1. A MAF file containing alignment blocks.\n  2. A plain text list of block numbers to extract (one block number per line).\n\n- **Output**:\n  - A MAF file that includes only the blocks corresponding to the provided block numbers.\n\n### Notes\n\n- **Block numbering starts at 0**: The first block in the MAF file is block number `0`, the second is `1`, and so on.\n\n- **List of block numbers**:\n  - The list of block numbers should be provided as plain text, with one block number per line.\n  - Example list:\n```\n0\n2\n5\n```\n  - This would extract the first block, third block, and sixth block from the MAF file.\n\n### Example\n\n#### Input MAF blocks (simplified):\n```\nblock 0: alignment of species A and B\nblock 1: alignment of species C and D\nblock 2: alignment of species E and F\nblock 3: alignment of species G and H\n```\n#### Block number list:\n```\n1\n3\n```\n#### Output MAF:\n```\nblock 1: alignment of species C and D\nblock 3: alignment of species G and H\n```\n\n### Common use cases\n\n- Extracting specific regions of interest from large MAF files.\n- Focusing analysis on particular alignments without handling the entire file.\n\n### Tips\n\n- Ensure the block numbers are correct and exist within the MAF file. If a block number is out of range, it will be ignored.\n- MAF files can be large. Extracting only the necessary blocks can improve efficiency for downstream analysis.\n\n### Input Requirements\n\n- The MAF file should be properly formatted and sorted by block numbers.\n- The block number list should not contain duplicates unless you want the same block to appear multiple times in the output."
    },
    {
        "name": "Reverse Complement",
        "description": "a MAF file",
        "category": ["Fetch Alignments/Sequences"],
        "help": "### What It Does\n\nThe **Reverse Complement** tool processes a MAF (Multiple Alignment Format) file and outputs a new MAF file where each alignment block has been reverse complemented.\n\nFor each alignment block:\n- The aligned sequences are reversed and complemented.\n- The strand information is switched (`+` becomes `-` and vice versa`).\n- Start positions are recalculated relative to the reverse strand.\n\n### Example\n\n#### Original MAF Block:\n```\na score=8157.000000\ns hg17.chr7    127471526 58 + 158628139 AATTTGTGGTTTATTCATTTTTCATTATTTTGTTTAAGGAGGTCTATAGTGGAAGAGG\ns panTro1.chr6 129885407 58 + 161576975 AATTTGTGGTTTATTCGTTTTTCATTATTTTGTTTAAGGAGGTCTATAGTGGAAGAGG\ns mm5.chr6      28904928 54 + 149721531 AA----CGTTTCATTGATTGCTCATCATTTAAAAAAAGAAATTCCTCAGTGGAAGAGG\n```\n\n#### Reverse Complemented MAF Block:\n```\na score=8157.000000\ns hg17.chr7     31156555 58 - 158628139 CCTCTTCCACTATAGACCTCCTTAAACAAAATAATGAAAAATGAATAAACCACAAATT\ns panTro1.chr6  31691510 58 - 161576975 CCTCTTCCACTATAGACCTCCTTAAACAAAATAATGAAAAACGAATAAACCACAAATT\ns mm5.chr6     120816549 54 - 149721531 CCTCTTCCACTGAGGAATTTCTTTTTTTAAATGATGAGCAATCAATGAAACG----TT\n```\n\n### How It Works\n\n1. Reads each block of the input MAF file.\n2. Reverses and complements each aligned sequence (A <-> T, C <-> G).\n3. Updates the strand information:\n   - If a sequence was on the `+` strand, it becomes `-`, and vice versa.\n4. Calculates the new start coordinate for each sequence:\n   - New start = sequence size - (original start + alignment size)\n5. Writes the updated block to the new MAF file.\n\n### Input\n- A **MAF file**.\n\n### Output\n- A **new MAF file** with each block reverse complemented.\n\n### Use Cases\n- Preparing reverse strand alignments for analysis.\n- Generating reverse complement alignments for consistency with reverse-complemented reference assemblies.\n\n### Notes\n- Gaps (`-`) and alignment structure are maintained while reversing the sequences.\n- Header and score lines are preserved in the output."
    },
    {
        "name": "Filter MAF",
        "description": "by specified attributes",
        "category": ["Fetch Alignments/Sequences"],
        "help": "### What it does\n\nThis tool allows you to build complex filters that can be applied to each alignment block of a MAF (Multiple Alignment Format) file. It provides options to filter based on species, chromosomes, strand orientation, and block size, giving you fine-grained control over which alignment blocks are included in the output.\n\n### Key Features\n\n- **Species-based filtering**: Define constraints for specific species based on their chromosome and strand. You can specify comma-separated lists of chromosomes where appropriate.\n- **Homologous chromosome filtering**: Restrict the alignments to only those blocks that contain alignments between chromosomes considered homologous.\n- **Optional species removal**: Remove undesired species from the MAF file. If no species are specified, all species will be kept. If species are specified, columns that contain only gaps are removed.\n\n### Filter Options\n\n- **Exclude blocks with missing species**:\n  - If set to YES, blocks that do not include all specified species will be excluded.\n  - Example: In an 8-way alignment, if you restrict to human, mouse, and rat, and enable this option, the tool will remove any blocks that do not include all three species. Only alignment blocks with exactly three sequences (human, mouse, rat) will be returned.\n\n- **Exclude blocks with only one species**:\n  - If set to YES, alignment blocks that contain only a single species will be excluded from the output.\n\n- **Block size filtering**:\n  - Specify a minimum and maximum size range to limit the output to MAF blocks that fall within the specified range.\n\n### Example Use Cases\n\n- Extracting only blocks that include specific species (e.g., human, mouse, rat) and exclude any blocks that don't contain all of them.\n- Filtering for homologous alignments between specific chromosomes across multiple species.\n- Reducing MAF file complexity by removing undesired species or alignment blocks that do not meet certain criteria.\n\n### How it works\n\n1. **Species constraints**:\n   - You can define which species must be present in each block.\n   - You can specify particular chromosomes and strands for each species.\n2. **Species removal**:\n   - Remove unwanted species from the alignment.\n   - If no species are specified, the tool will retain all species.\n3. **Gaps handling**:\n   - Columns that contain only gaps will be removed for the specified species.\n\n### Tips\n\n- Use comma-separated lists to specify multiple chromosomes.\n- Ensure species names and chromosome labels match those used in the MAF file.\n- This tool is useful for generating smaller, more targeted datasets from large MAF alignment files.\n\n### Input Requirements\n\n- A MAF file containing multiple alignments.\n- (Optional) A list of species with corresponding chromosomes and strands for filtering.\n\n### Output\n\n- A filtered MAF file that matches the user-defined criteria."
    },
    {
        "name": "Wiggle-to-Interval",
        "description": "converter",
        "category": ["Operate on Genomic Intervals"],
        "help": "### What It Does\n\nThe **Wiggle-to-Interval** tool converts **wiggle data** (WIG format) into interval-type data. This transformation is useful when you need to convert data that describes continuous values across a genomic region into discrete intervals.\n\n### Wiggle Format\n\nWiggle files (.wig) are line-oriented files used for representing genomic data. A **track definition line** precedes the data in a WIG file. The data itself can be in one of three formats:\n\n1. **BED Format (without declaration line, 4 columns)**:\n    - `chromA  chromStartA  chromEndA  dataValueA`\n    - `chromB  chromStartB  chromEndB  dataValueB`\n\n2. **Variable Step Format** (with a declaration line and followed by chromosome positions and data values):\n    - `variableStep chrom=chrN [span=windowSize]`\n    - `chromStartA  dataValueA`\n    - `chromStartB  dataValueB`\n\n3. **Fixed Step Format** (with a declaration line, followed by data values at fixed intervals):\n    - `fixedStep chrom=chrN start=position step=stepInterval [span=windowSize]`\n    - `dataValue1`\n    - `dataValue2`\n\n### Output\n\nAfter running this tool, the WIG data will be converted into **interval-type data**, where each entry represents an interval (chromosome, start, end, and associated data value) based on the input WIG format.\n\n### Example\n\nGiven the following **wiggle data** in `variableStep` format:\n\n```\nvariableStep chrom=chr1 span=100\n1000 0.5\n2000 0.6\n3000 0.4\n```\n\nThe output will be intervals like:\n\n```\nchr1 1000 1100 0.5\nchr1 2000 2100 0.6\nchr1 3000 3100 0.4\n```\n\nThis tool helps transform the continuous data from the WIG file into intervals that can be more easily analyzed or processed."
    },
    {
        "name": "Aggregate datapoints",
        "description": "Appends the average, min, max of datapoints per interval",
        "category": ["Operate on Genomic Intervals"],
         "help": "### What It Does\n\nThe **Aggregate Datapoints** tool calculates summary statistics for data points (such as those from UCSC or your own data) that fall within specified genomic intervals. It computes the average, minimum, and maximum values for data within each interval, helping users summarize large datasets associated with specific genomic regions.\n\n### Key Features\n\n1. **Input Data**: The tool assumes that the input dataset is in **interval format** and contains at least three columns: **chromosome** (chrom), **start**, and **end**. Additional columns with data values can be included in the dataset (e.g., gene expression, coverage, etc.).\n\n2. **Supported Genome Builds**: Currently, cached data is available for genome builds **hg16**, **hg17**, and **hg18**. If you're using your own dataset, ensure the genome build of your history items matches the one in use for the tool to recognize it.\n\n3. **Computation**: For each interval, the tool computes summary statistics (average, minimum, and maximum) of the data falling within that interval. The tool handles a variety of quantitative scores related to genomic regions such as:\n   - **Regulatory Potential**\n   - **Neutral rate (Ancestral Repeats)**\n   - **GC fraction**\n   - **Conservation Scores**\n     - **PhastCons**\n     - **binCons**\n     - **GERP**\n\n4. **Error Handling**: If data types in columns are inconsistent (e.g., attempting numerical calculations on strings), the tool will skip invalid lines and report the number of skipped lines in the resulting history item.\n\n### Example Workflow\n\n#### Input:\n- An interval dataset containing columns for **chrom**, **start**, **end**, and additional data columns (e.g., gene expression or GC content).\n- A dataset from UCSC or your own dataset containing genomic data points.\n\n#### The tool will:\n- Match the intervals in the dataset against the genomic data.\n- Compute summary statistics for each interval, including **average**, **minimum**, and **maximum** for the data.\n- Output the aggregated data with the new summary columns appended.\n\n### Notes\n- Ensure that the dataset is properly formatted (with valid numeric values) for the computation to proceed without errors."
    },
    {
        "name": "Gene BED To Exon/Intron/Codon BED",
        "description": "expander",
        "category": ["Operate on Genomic Intervals"],
        "help": "### What It Does\n\nThe **Gene BED To Exon/Intron/Codon BED** tool processes a **BED file** that contains at least 12 fields, representing gene-related information. This tool unpacks a single line describing a gene into multiple lines, each corresponding to individual **exons**, **introns**, **UTRs**, or **coding regions (CDS)**. The output will be empty if the input BED file contains only 3 or 6 fields.\n\n### How It Works\n\nBED files are often used to represent a single gene in one line, which includes information about **exons**, **coding sequence location (CDS)**, and the positions of **untranslated regions (UTRs)**. This tool converts that single line into multiple lines, with each line representing a specific **exon**, **intron**, or **UTR**.\n\n### Example\n\nGiven the following two BED lines:\n\n```\nchr7 127475281 127491632 NM_000230 0 + 127486022 127488767 0 3 29,172,3225,    0,10713,13126\nchr7 127486011 127488900 D49487    0 + 127486022 127488767 0 2 155,490,        0,2399\n```\n\nThe output will be:\n\n```\nchr7 127475281 127475310 NM_000230 0 +\nchr7 127485994 127486166 NM_000230 0 +\nchr7 127488407 127491632 NM_000230 0 +\nchr7 127486011 127486166 D49487    0 +\nchr7 127488410 127488900 D49487    0 +\n```\n\nIn this example, the tool splits the gene's data into separate entries for exons and UTRs, providing each as individual intervals.\n\n### About Formats\n\nThe **BED** (Browser Extensible Data) format is designed to represent genomic data tracks in the UCSC Genome Browser. It has three required fields and several optional ones. In the case of this tool, the following fields must be present in the BED file:\n\n1. **chrom** - Chromosome name (e.g., chr1, chr2, etc.)\n2. **start** - Start position of the feature (zero-based)\n3. **end** - End position of the feature (one-based)\n4. **name** - Name of the feature (e.g., gene or transcript)\n5. **score** - A score value associated with the feature (e.g., transcript abundance)\n6. **strand** - Strand direction (+ or -)\n7. **coding start** - Start position of the coding region (CDS)\n8. **coding end** - End position of the coding region (CDS)\n9. **exon count** - Number of exons in the gene\n10. **exon sizes** - Size of each exon\n11. **exon starts** - Start positions of each exon relative to the gene's start position\n12. **block count** - Number of blocks (exons)\n\n### Notes\n- The input **BED file** must contain at least 12 fields to be processed by this tool.\n- If the input file has only 3 or 6 fields, the output will be empty, as the tool requires more detailed gene structure information."
    },
    {
        "name": "Summary Statistics",
        "description": "for any numerical column",
        "category": ["Statistics"],
        "help": "### What It Does\n\nThe **Summary Statistics** tool computes basic summary statistics on a given column or on a valid expression containing one or more columns from a tab-delimited dataset. This tool is useful for calculating statistics like the mean, median, standard deviation, and other summary measures.\n\nThis tool automatically skips any blank or comment lines (lines starting with the # character) from the input dataset.\n\n### Syntax\n\n- The columns in your dataset are referenced as `c1`, `c2`, `c3`, and so on, where `c1` refers to the first column, `c2` to the second, and so on.\n\n- You can use mathematical expressions involving these columns, such as:\n  - `log(c5)` — computes the summary statistics for the natural log of column 5.\n  - `(c5 + c6 + c7) / 3` — computes the summary statistics on the average of columns 5-7.\n  - `log(c5, 10)` — computes the summary statistics of the base 10 log of column 5.\n  - `sqrt(c5 + c9)` — computes the summary statistics for the square root of the sum of columns 5 and 9.\n\n### Supported R Functions\nThe tool supports a variety of R functions that can be used to transform the data before computing statistics:\n- `abs`, `sign`, `sqrt`, `floor`, `ceiling`, `trunc`, `round`, `signif`, `exp`, `log`, `cos`, `sin`, `tan`, `acos`, `asin`, `atan`, `cosh`, `sinh`, `tanh`, `acosh`, `asinh`, `atanh`, `lgamma`, `gamma`, `gammaCody`, `digamma`, `trigamma`, `cumsum`, `cumprod`, `cummax`, `cummin`.\n\n### Tips\n- If your data is not tab-delimited, use the **Text Manipulation -> Convert delimiters to TAB** tool to convert it to the correct format.\n- If the data in the selected column is not numerical, the tool will skip those lines and not include them in the statistical computation. The number of invalid skipped lines will be documented in the resulting dataset.\n\n### Example\n\n#### Input dataset:\n```\nchr1   10  100  gene1\nchr1   105  200  gene2\nchr1   205  300  gene3\nchr2   10  100  gene4\nchr2   1000 1900  gene5\nchr3   15 1656  gene6\nchr4   10 1765  gene7\nchr4   10 1765  gene8\n```\n\n#### Example Expressions:\n- `log(c5)` — computes the natural logarithm of values in column 5.\n- `(c5 + c6 + c7) / 3` — computes the average of columns 5-7.\n- `sqrt(c5+c9)` — computes the square root of the sum of columns 5 and 9.\n\n### Output\nThe tool will compute the specified summary statistics on the selected column(s) or expression and return the results in the form of a summary table."
    },
    {
        "name": "Count",
        "description": "occurrences of each record",
        "category": ["Statistics"],
        "help": "### What It Does\n\nThe **Called Count** tool counts occurrences of unique values in selected column(s) from a dataset. If multiple columns are selected, the tool performs counting on each unique combination of values in the selected columns.\n\nThe first column of the resulting dataset will contain the count of unique values from the selected column(s), followed by each value found in those columns.\n\n### Syntax\n\n- Select one or more columns from your dataset.\n- The tool will count how many times each unique value or combination of values appears in the selected column(s).\n\n### Example\n\n#### Input file:\n```\nchr1   10  100  gene1\nchr1   105  200  gene2\nchr1   205  300  gene3\nchr2   10  100  gene4\nchr2   1000 1900  gene5\nchr3   15 1656  gene6\nchr4   10 1765  gene7\nchr4   10 1765  gene8\n```\n\n#### Counting unique values in **column 1 (c1)** (chr):\nThe output will be:\n```\n3 chr1\n2 chr2\n1 chr3\n2 chr4\n```\n\n#### Counting unique values in the **grouping of columns 2 (c2) and 3 (c3)** (10, 100, etc.):\nThe output will be:\n```\n2    10    100\n2    10    1765\n1    1000  1900\n1    105   200\n1    15    1656\n1    205   300\n```\n\n### Output\n\nThe resulting dataset will have the unique value count in the first column, followed by the corresponding value(s) from the selected column(s). This tool is useful when you need to summarize the frequency of different values in one or more columns of your dataset."
    },
    {
        "name": "VCF to MAF Custom Track",
        "description": "for display at UCSC",
        "category": ["Graph/Display Data"],
         "help": "### What It Does\n\nThe **VCF to MAF Custom Track** tool converts a Variant Call Format (VCF) file into a Multiple Alignment Format (MAF) custom track file that is suitable for visualization in genome browsers, such as UCSC Custom Track. This tool is useful for displaying variant data but is not intended for use in performing further analysis. For analysis, the original VCF file should be used.\n\nThe conversion represents unknown nucleotides as '*' to ensure proper display, such as for reference bases that appear before a deletion (which are unavailable without querying the original reference sequence).\n\n### Example Input\n\nGiven a VCF file with the following format:\n```\n#CHROM  POS ID  REF ALT QUAL    FILTER  INFO    FORMAT  NA00001 NA00002 NA00003\n20  14370   rs6054257   G   A   29  0   NS=3;DP=14;AF=0.5;DB;H2 GT:GQ:DP:HQ 0|0:48:1:51,51  1|0:48:8:51,51  1/1:43:5:-1,-1\n20  17330   .   T   A   3   q10 NS=3;DP=11;AF=0.017 GT:GQ:DP:HQ 0|0:49:3:58,50  0|1:3:5:65,3    0/0:41:3:-1,-1\n20  1110696 rs6040355   A   G,T 67  0   NS=2;DP=10;AF=0.333,0.667;AA=T;DB   GT:GQ:DP:HQ 1|2:21:6:23,27  2|1:2:0:18,2    2/2:35:4:-1,-1\n20  1230237 .   T   .   47  0   NS=3;DP=13;AA=T GT:GQ:DP:HQ 0|0:54:7:56,60  0|0:48:4:51,51  0/0:61:2:-1,-1\n20  1234567 microsat1   G   D4,IGA  50  0   NS=3;DP=9;AA=G  GT:GQ:DP    0/1:35:4    0/2:17:2    1/1:40:3\n``` \n\n### Example Output (MAF Custom Track):\n```\ntrack name=\"Galaxy Custom Track\" visibility=pack\na score=0\ns hg18.chr20  14369 1 + 14370 G\ns CHB+JPT_1.1     0 1 +     1 A\n\na score=0\ns hg18.chr20  17329 1 + 17330 T\ns CHB+JPT_1.2     0 1 +     1 A\n\na score=0\ns hg18.chr20  1110695 1 + 1110696 A\ns CHB+JPT_1.3       0 1 +       1 G\ns CHB+JPT_2.3       0 1 +       1 T\n\na score=0\ns hg18.chr20  1230236 1 + 1230237 T\ns CHB+JPT_1.4       0 1 +       1 .\n\na score=0\ns hg18.chr20  1234565 5 + 1234572 *G--***\ns CHB+JPT_1.5       0 1 +       1 *------\ns CHB+JPT_2.5       0 7 +       7 *GGA***\n```\n\n### How It Works\n\n- The tool converts VCF data into MAF format by representing each variant as a `s` line, where the position and base information is listed for each chromosome.\n- `a score=0` is used for each variant, which represents the alignment score.\n- Each chromosome and variant information is converted into a multiple alignment format that is compatible with genome browsers.\n- The output is specifically designed for visualization purposes in UCSC and similar genome browsers."
    },
    {
        "name": "SIFT",
        "description": "predictions of functional sites",
        "category": ["Phenotype Association"],
        "help": "### Dataset Formats\n\nThe input and output datasets are in tabular format.\n\n### What It Does\n\nSIFT predicts whether an amino-acid substitution affects protein function, based on sequence homology and the physical properties of amino acids. It can be applied to both naturally occurring non-synonymous polymorphisms and laboratory-induced missense mutations. The tool uses pre-computed SIFT scores and annotations for all possible nucleotide substitutions at each position in the human exome. These annotations are based on **SQLite databases**. The allele frequency data comes from the **HapMap frequency database**, while additional transcript and gene-level data are from **Ensembl BioMart**.\n\nThe input dataset must contain columns for **chromosome**, **position**, and **alleles**. The alleles must be represented by two nucleotides separated by a '/', usually the **reference allele** and the **allele of interest**. The strand should either be specified in a separate column or be consistent across all rows.\n\nThe output contains a standard set of columns, as well as additional columns based on the selected data."
    },
    {
        "name": "g:Profiler",
        "description": "tools for functional profiling of gene lists",
        "category": ["Phenotype Association"],
       "help": "### Dataset Formats\n\nThe input dataset should be tabular with a column containing identifiers such as gene, protein, or microarray probe IDs. The output dataset is an HTML file with a link to the g:Profiler website.\n\n### What It Does\n\nThis tool generates a link to the **g:GOSt** tool (Gene Group Functional Profiling), which is part of the **g:Profiler** site from the University of Tartu, Estonia. g:GOSt retrieves the most significant **Gene Ontology (GO)** terms, **KEGG** and **REACTOME** pathways, and **TRANSFAC** motifs for a user-specified group of genes, proteins, or microarray probes. Additionally, g:GOSt supports the analysis of ranked or ordered gene lists, interactive visualization, and GO graph structure browsing.\n\nMultiple testing corrections are applied to extract only statistically significant results. The form for **g:GOSt** is pre-filled with IDs from the selected column of a tabular Galaxy dataset. Alternatively, genomic coordinates can be used (based on the latest Ensembl build), even if they correspond to SNPs rather than genes. g:GOSt will map SNPs to gene IDs.\n\nOnce the tool finishes running, you can click the eye icon to follow the generated link. On the g:Profiler website, you can explore the g:GOSt results, adjust the form options, or use the provided links to run other g:Profiler tools using the same list of IDs.\n\n### Features\n- Retrieve significant GO terms, KEGG, REACTOME pathways, and TRANSFAC motifs\n- Analyze ranked gene lists and SNPs (mapped to genes)\n- Interactive visualization of results\n- Multiple testing corrections to ensure statistical significance"
    },
    {
        "name": "DAVID",
        "description": "functional annotation for a list of genes",
        "category": ["Phenotype Association"],
         "help": "### Dataset Formats\n\nThe input dataset is expected to be in tabular format, where a list of gene or protein IDs are provided in one of the columns. The output of this tool is an HTML link that directs you to the Database for Annotation, Visualization, and Integrated Discovery (DAVID) website at NIH.\n\n### What It Does\n\nThis tool generates a link to the DAVID website, sending the list of IDs from the selected column of a tabular Galaxy dataset to DAVID. Once the tool has finished running, you can click the eye icon to follow the generated link. The DAVID website provides a comprehensive set of functional annotation tools that help researchers discover the biological meaning behind large gene lists.\n\nDAVID integrates gene annotation data, biological pathways, and other related information to offer in-depth analysis. It is a useful tool for functional genomics research and interpretation of gene lists from experiments.\n\n### References\n\n- Huang DW, Sherman BT, Lempicki RA. (2009) Systematic and integrative analysis of large gene lists using DAVID bioinformatics resources. *Nat Protoc.* 4(1):44-57.\n- Dennis G, Sherman BT, Hosack DA, Yang J, Gao W, Lane HC, Lempicki RA. (2003) DAVID: database for annotation, visualization, and integrated discovery. *Genome Biol.* 4(5):P3. Epub 2003 Apr 3."
    },
    {
        "name": "LD",
        "description": "linkage disequilibrium and tag SNPs",
        "category": ["Phenotype Association"],
        "help": "### Dataset Formats\n\nThe input and output datasets for this tool are tabular.\n\n### What It Does\n\nThis tool is designed to analyze the patterns of **linkage disequilibrium (LD)** between polymorphic sites in a locus. It groups **Single Nucleotide Polymorphisms (SNPs)** based on the threshold level of LD, measured by **r²**, and identifies a representative **tag SNP** for each group. Other SNPs within the group are in LD with the tag SNP, but not necessarily with each other.\n\nThe algorithm used is similar to that of **ldSelect** (Carlson et al. 2004), but this tool is much faster and more efficient than the original implementation.\n\n### Input Format\n\nThe input is a tabular file containing genotype information for each individual at each SNP site. The file must have exactly four columns: **site ID**, **sample ID**, and the two **allele nucleotides**.\n\n### Output\n\nThe output will provide the SNPs grouped by LD patterns, with a tag SNP reported for each group.\n\n### References\nCarlson, C. S., et al. (2004). \" ldSelect: An Efficient Method for Selecting SNPs for Genetic Association Studies.\""
    },
    {
        "name": "MasterVar to pgSnp",
        "description": "Convert from MasterVar to pgSnp format",
        "category": ["Phenotype Association"],
        "help": "### Dataset Formats\n\nThe input dataset must be in the **MasterVar** format provided by the **Complete Genomics** analysis process. Although Galaxy considers this as tabular data, the columns must adhere to the specified MasterVar format. The output dataset will be in **pgSnp** format.\n\n### What It Does\n\nThis tool converts a **Complete Genomics MasterVar** file to **pgSnp** format. The resulting file can be viewed in genome browsers or used with other tools like the **phenotype association** and **interval operations** tools. Positions that are homozygous for the reference allele are excluded from the output.\n\n### Output\n\nThe output is a **pgSnp** formatted file, suitable for further analysis and visualization."
    }, 
    {
        "name": "Generating a Single Cell Matrix Using Alevin",
        "category": ["Single-Cell RNA Sequencing"],
        "description": "A Galaxy workflow to process single-cell RNA-seq data and generate an expression matrix using Alevin.",
        "help": "This workflow is designed for single-cell RNA sequencing analysis within the Galaxy platform. It processes raw sequencing reads (FASTQ files) containing cell barcodes, unique molecular identifiers (UMIs), and transcripts, alongside reference files (FASTA and GTF) from Ensembl (http://www.ensembl.org/info/data/ftp/index.html), to produce an annotated single-cell expression matrix. The workflow includes the following stages: 1. **Raw Reads**: Input FASTQ files include 'Read 1' (cell barcode and UMI) and 'Read 2' (transcript). 2. **Getting Gene Information**: Extracts transcript-gene mappings and gene metadata using the GTF2GeneList tool from a reference FASTA file and GTF file. 3. **Alevin - No Processing**: Quantifies transcript expression with Alevin, producing matrix files (MTX) and metadata without additional preprocessing. 4. **Converting Datatypes and Integrating Gene Information**: Converts Alevin outputs to a 10x-compatible format, integrates gene annotations, and creates a single-cell expression object (SCE). 5. **EmptyDrops, Converting & Reformatting**: Filters empty droplets with EmptyDrops, converts the SCE object to AnnData format, and reformats data for downstream analysis. 6. **Visualizing**: Generates a droplet barcode rank plot to visualize raw barcode frequencies. This workflow is part of the training material at https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/scrna-case_alevin/tutorial.html, licensed under CC-BY-4.0, and created by Julia Jakiela and Wendi Bacon."
    },

    {
        "name": "pAllori Amplicon 16S Pre-Processing and Taxonomy Classification",
        "category": ["Metagenomics"],
        "description": "A Galaxy workflow for pre-processing 16S amplicon data and classifying taxonomy using LotuS2 and ampvis2.",
        "help": "This workflow, designed for amplicon data analysis and visualization within the Galaxy platform, processes 16S paired-end sequencing data to classify taxonomy and visualize microbial communities using LotuS2 and ampvis2. It includes quality control, trimming, human contamination removal, OTU clustering, and result aggregation, with the following stages: 1. **QC Before Trimming**: Performs initial quality assessment of raw 16S paired-end samples using FastQC and MultiQC, generating HTML reports. 2. **Reads Q. Retaining**: Retains high-quality reads using Trimmomatic with a sliding window approach (window size 4, quality 20). 3. **Adapters Trimming**: Trims adapters (e.g., primers AGAGTTTGATCCTGGCTCAG and GCTGCCTCCCGTAGGAGT) using Cutadapt. 4. **Human Contamination Removal**: Removes human reads by aligning to the CHM13_T2T_v2.0 reference genome with Bowtie2, retaining unaligned reads. 5. **QC After Trimming**: Assesses trimmed reads with FastQC and MultiQC for quality validation. 6. **Amplicon Data Analysis and OTUs Building**: Clusters OTUs and assigns taxonomy using LotuS2 with the SILVA database, producing OTU tables and phylogenetic trees. 7. **Input Metadata**: Incorporates sample metadata (e.g., sample file names, positive/negative controls, batch IDs) for downstream analysis and visualization. 8. **OTUs Visualization, Samples Comparison and Results Aggregation**: Visualizes OTUs with ampvis2, generating ordination (PCA) and heatmap plots at the Phylum level, and removes contaminants with Decontam. Licensed under MIT, this workflow is created by Engy Nasr and Paul Zierep, tagged with microGalaxy, 16s, metagenomics, and pallori."
    }, 
    {
        "name": "Version 1 | Combining single cell datasets after pre-processing",
        "category": ["Single-Cell RNA Sequencing"],
        "description": "A Galaxy workflow to combine pre-processed single-cell datasets into a unified AnnData object with cell and gene metadata.",
        "help": "This workflow, designed for single-cell RNA sequencing analysis within the Galaxy platform, combines seven pre-processed AnnData objects into a single annotated dataset, integrating cell and gene metadata such as sex, genotype, and mitochondrial content. It is associated with training material at https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/scrna-case_alevin-combine-datasets/tutorial.html and includes the following stages: 1. **Concatenating Objects**: Inputs seven AnnData objects (Sample 1 to Sample 7, order matters) and concatenates them using Manipulate AnnData with an inner join, adding a 'batch' key for sample identification. 2. **Inspecting Object**: Inspects the concatenated object using Inspect AnnData, outputting general information (txt), observations (obs, tabular), and variables (var, tabular). 3. **Generating Cell Metadata**: Processes the obs output with Replace Text to derive sex (male: 0|1|3|4|5|6, female: 2) and genotype (wildtype: 0|3|4|5, knockout: 1|2|6) metadata, then uses Cut and Paste to create a unified Cell_Metadata file. 4. **Integrating Cell & Batch Metadata**: Adds the Cell_Metadata to the AnnData object and renames batch categories (N701-N707) using Manipulate AnnData, producing a Batched_Object. 5. **Gene Metadata: Mitochondrial Content**: Flags mitochondrial genes (starting with 'NA.') in the Batched_Object and calculates quality control metrics with Scanpy Inspect, annotating mitochondrial content (qc_vars: mito) to create an Annotated_Object. 6. **Inspecting Object**: Inspects the final Annotated_Object, outputting updated obs and var files for validation. Licensed under MIT, this workflow is created by Wendi Bacon and tagged with 'training' and 'single-cell'."
    }, 
    {
        "name": "Workflow constructed from history 'TAPScan_GTN'",
        "category": ["Protein Family Analysis"],
        "description": "A Galaxy workflow to classify protein families with TAPScan and filter sequences by bHLH family identifiers.",
        "help": "This workflow, constructed from the history 'TAPScan_GTN' within the Galaxy platform, processes a collection of protein FASTA sequences to classify transcription-associated proteins (TAPs) using TAPScan and extracts sequences belonging to the bHLH family. It includes the following stages: 1. **Input Dataset Collection**: Takes a list of protein FASTA files (e.g., SELMO.fa) as input. 2. **TAPScan Classify**: Analyzes the protein sequences with TAPScan Classify (version 4.76+galaxy0), producing tabular outputs: taps_detected (detected TAPs), taps_family_counts (family counts), and taps_detected_extra (additional details). 3. **Filter**: Filters the taps_detected output to retain only rows where the second column equals 'bHLH', preserving the header line. 4. **Cut**: Extracts the first column (protein identifiers) from the filtered table using Cut with a tab delimiter. 5. **Remove Beginning**: Removes the header line from the identifier list to prepare it for filtering. 6. **Filter FASTA**: Uses Filter FASTA to extract sequences from the original input collection that match the bHLH identifiers, outputting a filtered FASTA file. This workflow has no additional annotations, tags, or specified license, and its creator is not explicitly defined."
    },
    {
        "name": "Workflow constructed from history 'AquaINFRA's Daugava Workflow'",
        "category": ["Environmental Data Analysis"],
        "description": "A Galaxy workflow for processing and analyzing environmental point data with AquaINFRA OGC API tools, including visualization of trends.",
        "help": "This workflow, constructed from the history 'AquaINFRA's Daugava Workflow' within the Galaxy platform, processes environmental point data (e.g., transparency measurements) and associated regions, performing pre-processing, statistical analysis, and visualization using AquaINFRA OGC API Processes (version 0.3.0). It includes the following stages: 1. **Input Data**: Takes two datasets—'points' (environmental measurements) and 'regions' (spatial boundaries)—as inputs. 2. **Data Pre-processing**: First, assigns polygon attributes to points using 'points-att-polygon' (longitude, latitude), then converts data into seasonal periods with 'peri-conv' (e.g., winter: Dec-01 to Mar-01, spring: Mar-02 to May-30, summer: Jun-01 to Aug-30, autumn: Sep-01 to Nov-30) based on 'visit_date' (format: y/m/d), adjusting years starting December 1. 3. **Data Analysis**: Computes means by group ('mean-by-group') using columns like longitude, latitude, Year_adj_generated, group_labels, and HELCOM_ID for 'transparency_m'; interpolates time series data ('ts-selection-interpolation') with a missing threshold of 80% and minimum 10 data points; and performs Mann-Kendall trend analysis ('trend-analysis-mk') on 'transparency_m' over 'Year_adj_generated'. 4. **Visualization**: Generates a bar plot of trend results ('barplot-trend-results') with Tau_Value, P_Value (threshold 0.05), and HELCOM_ID grouped by period, and creates a shapefile map ('map-shapefile-points') of 'transparency_m' using longitude, latitude, and HELCOM_ID from regions. This workflow has no additional annotations, tags, or specified license, and its creator is not explicitly defined."
    }, 
    {
        "name": "LOCKED | Filter plot and explore single-cell RNA-seq data with Scanpy",
        "category": ["Single-Cell RNA Sequencing"],
        "description": "A Galaxy workflow for filtering, plotting, and analyzing single-cell RNA-seq data using Scanpy tools, including clustering and gene marker identification.",
        "help": "This workflow, titled 'LOCKED | Filter plot and explore single-cell RNA-seq data with Scanpy,' is designed for the Galaxy platform to process single-cell RNA-seq data from an AnnData object with mitochondrial gene annotations, as outlined in the training tutorial at https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/scrna-case_basic-pipeline/tutorial.html. Created by Wendi Bacon (ORCID: 0000-0002-8170-8806) under the MIT license, it includes the following stages: 1. **Inspect Data**: Examines the input AnnData object using Inspect AnnData (v0.10.9+galaxy1) to output general info, observations (obs), and variables (var). 2. **Plotting Before Filtering**: Generates violin plots (e.g., by genotype, sex, batch) and scatter plots (e.g., genes vs. UMI, genes vs. mito) using Scanpy Plot (v1.10.2+galaxy2) to visualize raw data distributions. 3. **Filtering**: Applies sequential filtering with Scanpy Filter (v1.10.2+galaxy3) on genes (log1p_n_genes_by_counts > 5.7 and < 20), UMIs (log1p_total_counts > 6.3 and < 20), mitochondrial content (pct_counts_mito < 4.5), and cells (n_cells > 3), producing filtered AnnData objects with intermediate violin plots. 4. **Processing**: Normalizes data to a target sum of 10,000, logs it (log1p), saves raw data, identifies highly variable genes (Seurat flavor, min_mean=0.0125, max_mean=3, min_disp=0.5), and scales data (max_value=10) using Scanpy Normalize (v1.10.2+galaxy0) and related tools. 5. **Preparing Coordinates**: Performs PCA (50 components), computes neighbors (15 neighbors, 20 PCs), and embeds data with tSNE and UMAP using Scanpy Cluster (v1.10.2+galaxy2), with a PCA variance plot. 6. **Clustering & Gene Markers**: Clusters data using Louvain (resolution=0.5) and identifies differentially expressed genes (DEGs) by genotype and cluster with Scanpy Inspect (v1.10.2+galaxy1), outputting ranked gene lists. 7. **Plotting**: Creates PCA, tSNE, and UMAP plots colored by multiple variables (e.g., louvain, genotype, specific genes like Il2ra, Cd8a). 8. **Annotating Clusters**: Renames Louvain clusters to biologically meaningful labels (e.g., DP-M3, Macrophages) and generates an annotated UMAP plot. The workflow is tagged with 'training' and 'single-cell'."
    },
    {
        "name": "HeLa siRNA screen",
        "category": ["Image Analysis"],
        "description": "A Galaxy workflow for processing 2D images from a HeLa siRNA screen, including filtering, segmentation, and feature extraction.",
        "help": "This workflow, named 'HeLa siRNA screen,' is designed for the Galaxy platform to analyze 2D images from a HeLa siRNA screen, focusing on image processing, segmentation, and feature extraction. It takes an input image and a filter rules dataset as inputs and proceeds through the following stages: 1. **Image Filtering**: Applies a Gaussian filter (size 3.0) to the input image using Filter 2-D image (v1.12.0+galaxy1) to reduce noise. 2. **Thresholding**: Converts the filtered image to a binary image using Threshold image (v0.18.1+galaxy3) with the Otsu method, no inversion, and default parameters (threshold=0, block_size=0, offset=0.0). 3. **Segmentation**: Transforms the binary image into a label map using Convert binary image to label map (v0.5+galaxy0) with the watershed mode (minimum distance=20) to identify distinct regions. 4. **Initial Feature Extraction**: Extracts features (label, area, eccentricity, major_axis_length) from the label map using Extract image features (v0.18.1+galaxy0) without the raw image, outputting a tabular file. 5. **Filtering by Rules**: Filters the label map based on user-defined rules from the 'filter rules' input using Filter label map by rules (v0.0.1-4), producing a refined label map. 6. **Final Feature Extraction**: Extracts additional features (mean_intensity, area, major_axis_length) from the filtered label map using Extract image features (v0.18.1+galaxy0), incorporating the original input image as the raw source, outputting a final tabular file. The workflow outputs the filtered label map (TIFF) and the final feature table (tabular). No creator, license, tags, or annotations are specified."
    }, 
    {
        "name": "MAGs individual workflow v2.0",
        "category": ["Metagenomics"],
        "description": "A Galaxy workflow for assembling, binning, refining, and annotating Metagenome-Assembled Genomes (MAGs) from paired-end reads, with options for MEGAHIT or metaSPAdes assembly and DAS Tool or Binette bin refinement.",
        "help": "This workflow, named 'MAGs individual workflow v2.0,' is designed for the Galaxy platform to process metagenomic paired-end reads into high-quality Metagenome-Assembled Genomes (MAGs). Created by Bérénice Batut (ORCID: 0000-0001-9852-1987), Paul Zierep (ORCID: 0000-0003-2982-388X), Mina Hojat Ansari (ORCID: 0000-0002-3602-7884), Patrick Bühler, and Santino Faack under the MIT license, it includes the following stages: 1. **Inputs**: Accepts trimmed grouped paired reads (list:paired) and sample paired reads (list:paired) for abundance estimation, with a parameter to choose between MEGAHIT or metaSPAdes for assembly. 2. **Assembly**: Uses MEGAHIT (v1.2.9+galaxy2, k-list=21,29,39,59,79,99,119,141, min_contig_len=200) or metaSPAdes (v3.15.5+galaxy3, auto k-mer selection) based on the chosen assembler, with Pick parameter value (v0.2.0) selecting the output contigs. 3. **Quality Assessment**: Runs Quast (v5.3.0+galaxy0) on contigs with paired reads (min_contig=500, metagenome mode) to generate assembly metrics. 4. **Binning Preparation**: Maps reads to contigs with Bowtie2 (v2.5.3+galaxy1), sorts alignments with Samtools sort (v2.0.5), cuts contigs into 10,000 bp chunks with CONCOCT: Cut up contigs (v1.1.0+galaxy2), and generates a coverage table with CONCOCT: Generate the input coverage table (v1.1.0+galaxy2). 5. **Binning**: Applies multiple binning tools—CONCOCT (v1.1.0+galaxy2, 400 clusters), MaxBin2 (v2.2.7+galaxy6, min_contig_length=1000), MetaBAT2 (v2.17+galaxy0), and SemiBin (v2.0.2+galaxy0, single mode, cached DB)—converting bins to contig2bin tables with Converts genome bins in fasta format (v1.1.7+galaxy1). 6. **Bin Refinement**: Offers a choice between DAS Tool (v1.1.7+galaxy1, Diamond search, score_threshold=0.5) or Binette (v1.0.5+galaxy1, min_completeness=40) based on a user parameter, refining bins from all tools, followed by Flatten collection to pool bins and dRep dereplicate (v3.5.0+galaxy1, completeness=75, contamination=25) to produce dereplicated genomes. 7. **Annotation and Quality Control**: Annotates dereplicated bins with Bakta (v1.9.4+galaxy0, V5.1 DB), classifies taxonomy with GTDB-Tk Classify genomes (v2.4.0+galaxy0, release 220), assesses quality with CheckM2 (v1.0.2+galaxy0) and CheckM lineage_wf (v1.2.3+galaxy0), generates coverage with CoverM genome (v0.7.0+galaxy0), and produces a GC plot with CheckM plot (v1.2.3+galaxy0). 8. **Reporting**: Combines results (Quast, CheckM2, CoverM, GTDB-Tk, Bakta, CheckM) into a MultiQC (v1.27+galaxy3) report. Outputs include dereplicated bins, annotations (TSV, GFF3, FFN), assembly metrics, and an HTML report. Annotations suggest adding Bakta, Binette, and merging DAS Tool/Binette outputs. Tagged with 'FAIRyMAGs'."
    }, 
    {
        "name": "Workflow for representing State of the Environment in Antarctic",
        "category": ["Environmental Science"],
        "description": "A Galaxy workflow for visualizing the state of the Antarctic environment through five figures, using Jupyter notebooks to process and plot data from the Copernicus Marine Data Store based on user-defined subareas, years, and months.",
        "help": "This workflow, titled 'Workflow for representing State of the Environment in Antarctic,' is a Galaxy-based pipeline designed to generate visualizations of environmental data in Antarctica, integrating data from the Copernicus Marine Data Store. Authored by Arthur (GitHub: TuturBaba/Antarctic), it requires users to connect their Galaxy account to a Copernicus account via the Copernicus CMEMS API Key (instructions: create an account at https://data.marine.copernicus.eu/login, then link it in Galaxy under 'User > Preferences > Manage Information'). The workflow consists of five main figures (Fig1–Fig5), each driven by Jupyter notebooks, organized into colored frames: 1. **Fig1 (red frame)**: Visualizes data for user-specified Antarctic subareas (e.g., 883, 882, etc.), using a notebook (Fig1_test.ipynb) downloaded via lftp (v4.9.2) and executed in an Interactive JupyterLab Notebook (v1.0.0), outputting 'fig1.png'. 2. **Fig2 (blue frame)**: Plots data for selected years (1978–2025), with optional colors, median (1981–2010), and PNG dimensions, using Fig2.ipynb, producing 'fig2.png'. 3. **Fig3 (green frame)**: Displays concentration and/or anomalies for specified months, years, and subareas, via Fig3.ipynb, outputting 'fig3.png'. 4. **Fig4 (orange frame)**: Generates a plot for a single year and month across subareas, using Fig4.ipynb, yielding 'fig4.png'. 5. **Fig5 (turquoise frame)**: Visualizes Sea Surface Temperature (SST) and/or anomalies for two years and subareas, integrating Copernicus Climate Data Store (v0.3.0) data via Fig5_API_8.ipynb and Fig5_8.ipynb, producing 'fig5.png' and supporting files (API.txt, Fig5_API_8.ipynb netCDF). The workflow uses tools like 'Create text file' (v9.3+galaxy1) to handle notebook URLs and 'downloads' (lftp) to fetch them. Outputs are PNG images and supplementary files, tagged with workflow version 92 and UUID 3ef1c114-2564-4335-a1b9-45aa12e8750c. No additional annotations are provided."
    },
    {
        "name": "RNA-Seq Analysis: Paired-End Read Processing and Quantification",
        "category": ["Transcriptomics"],
        "description": "A Galaxy workflow for processing paired-end RNA-Seq FASTQ files, performing adapter trimming, mapping with STAR, generating count tables, computing FPKM/TPM values with Cufflinks or StringTie, and producing quality control reports and coverage files.",
        "help": "This workflow, titled 'RNA-Seq Analysis: Paired-End Read Processing and Quantification,' is designed for the Galaxy platform to process paired-end RNA-Seq data from FASTQ files to quantified gene expression and quality metrics. It requires the following inputs: 1. **Collection paired FASTQ files**: A list of dataset pairs in fastqsanger format. 2. **GTF file of annotation**: A GTF file with gene annotations for mapping and quantification. 3. **GTF with regions to exclude (optional)**: A GTF file to mask regions (e.g., chrM in mm10) during FPKM normalization with Cufflinks, exemplified by entries like 'chrM chrM_gene exon 0 16299 . + . gene_id \"chrM_gene_plus\"; transcript_id \"chrM_tx_plus\"; exon_id \"chrM_ex_plus\";'. Additional input parameters include: - **Forward and Reverse adapter (optional)**: Adapter sequences (e.g., TruSeq or Nextera) for trimming with fastp, used only if reads don’t overlap. - **Generate additional QC reports**: Boolean to enable FastQC, Picard, read distribution, gene body coverage, and reads per chromosome. - **Reference genome**: Selectable genome for STAR alignment. - **Strandedness**: Options (unstranded, forward, reverse) to match library preparation, affecting alignment and coverage. - **Use featureCounts**: Boolean to use featureCounts for count tables instead of STAR. - **Compute Cufflinks FPKM**: Boolean to compute FPKM with Cufflinks (time-intensive). - **Compute StringTie FPKM**: Boolean to compute FPKM/TPM with StringTie. The processing steps are: 1. **Trimming**: fastp removes adapters and low-quality bases, filtering reads <15 bp. 2. **Mapping**: STAR aligns reads with ENCODE parameters, counts reads per gene, and generates strand-specific normalized coverage (uniquely mapped reads only). 3. **Counting**: Optionally, featureCounts generates count tables if selected. 4. **QC**: Optional tools (FastQC, Picard, read_distribution, geneBody_coverage, samtools idxstats) produce QC metrics, summarized by MultiQC. 5. **Quantification**: Optionally, Cufflinks computes FPKM (multi-map corrected), and StringTie computes FPKM/TPM. 6. **BAM Filtering**: Filters BAM to uniquely mapped reads (NH:i:1). 7. **Coverage**: Bedtools computes unstranded coverage, normalized to million uniquely mapped reads, with all coverages (stranded and unstranded) converted to bigWig. Outputs include BAM files, count tables, FPKM/TPM tables, bigWig coverage files, and a MultiQC report. **Warning**: Stranded coverage depends on library strandedness—useless for unstranded libraries, matches first read for forward-stranded, and second read for reverse-stranded libraries."
    },
    {
        "name": "Generic Variation Analysis on WGS PE Data",
        "category": ["Genomics"],
        "description": "A Galaxy workflow for mapping paired-end whole-genome sequencing (WGS) reads using BWA-MEM, performing sensitive variant calling across a wide range of allele frequencies with LoFreq, and annotating variants with snpEff, utilizing a user-provided GenBank reference genome.",
        "help": "This workflow, titled 'Generic Variation Analysis on WGS PE Data,' is designed for the Galaxy platform to analyze paired-end (PE) whole-genome sequencing (WGS) data for variant detection and annotation. It requires the following inputs: 1. **Collection paired FASTQ files**: A list of dataset pairs in fastqsanger format containing raw WGS reads. 2. **Reference genome**: A GenBank file (e.g., .gbk format) provided by the user, specifying the reference sequence for mapping and variant calling (e.g., a bacterial genome or a eukaryotic chromosome). Optional inputs include: - **Minimum read quality**: A Phred score threshold (default: 20) for filtering low-quality reads prior to mapping. - **Variant calling sensitivity**: Adjustable LoFreq parameters (e.g., minimum coverage, default: 10) to tune sensitivity across allele frequencies (AFs). The processing steps are: 1. **Read Mapping**: BWA-MEM aligns paired-end reads to the reference genome with default parameters optimized for sensitivity and speed, producing a BAM file. 2. **BAM Processing**: The BAM file is sorted and indexed using Samtools (e.g., sort and index commands) to prepare for variant calling. 3. **Variant Calling**: LoFreq performs sensitive variant detection across a wide range of allele frequencies (low to high AFs), leveraging its statistical model to identify single nucleotide variants (SNVs) and small indels, outputting a VCF file. 4. **Variant Annotation**: snpEff annotates the VCF file with functional predictions (e.g., missense, nonsense, synonymous) based on the GenBank reference, generating an annotated VCF and summary statistics (e.g., HTML report). Outputs include: - **BAM file**: Sorted and indexed alignments. - **VCF file**: Raw variants from LoFreq. - **Annotated VCF**: Functionally annotated variants from snpEff. - **Summary report**: snpEff’s HTML summary of variant effects. The workflow is particularly suited for microbial or small eukaryotic genomes where a GenBank file is available, offering flexibility in reference selection. No additional quality control steps (e.g., FastQC) are included by default, but users can extend the workflow as needed."
    }
]
}